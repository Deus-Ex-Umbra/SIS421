{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "Yn4tMSdxogG2",
        "outputId": "5bf88203-5f48-472c-be41-4a09a98a65bf"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": [
              "  function ClickConnect() {\n",
              "    var iconElement = document.getElementById(\"toggle-header-button\");\n",
              "    if (iconElement) {\n",
              "    var clickEvent = new MouseEvent (\"click\", {\n",
              "        bubbles: true, cancelable: true,\n",
              "        view: window\n",
              "      });\n",
              "    iconElement.dispatchEvent (clickEvent);\n",
              "    }\n",
              "  }\n",
              "  setInterval(ClickConnect, 6000);\n"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "%%javascript\n",
        "  function ClickConnect() {\n",
        "    var iconElement = document.getElementById(\"toggle-header-button\");\n",
        "    if (iconElement) {\n",
        "    var clickEvent = new MouseEvent (\"click\", {\n",
        "        bubbles: true, cancelable: true,\n",
        "        view: window\n",
        "      });\n",
        "    iconElement.dispatchEvent (clickEvent);\n",
        "    }\n",
        "  }\n",
        "  setInterval(ClickConnect, 6000);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLJlb2O8ltqm",
        "outputId": "12f69317-edec-4fda-d85e-fc56b6310182"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tflearn in /usr/local/lib/python3.10/dist-packages (0.5.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tflearn) (1.26.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from tflearn) (1.16.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from tflearn) (10.4.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (71.0.4)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.66.0)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.1)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.5.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.8.0)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.12.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.7.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n",
            "Requirement already satisfied: tflearn in /usr/local/lib/python3.10/dist-packages (0.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tflearn) (1.26.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from tflearn) (1.16.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from tflearn) (10.4.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install tflearn tensorflow\n",
        "!pip install --upgrade tflearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "demO8Ff6phFz",
        "outputId": "6f97d064-6a93-43ae-f356-048970b76115"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Name: tflearn\n",
            "Version: 0.5.0\n",
            "Summary: Deep Learning Library featuring a higher-level API for TensorFlow\n",
            "Home-page: https://github.com/tflearn/tflearn\n",
            "Author: TFLearn contributors\n",
            "Author-email: aymeric.damien@gmail.com\n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: numpy, Pillow, six\n",
            "Required-by: \n",
            "/usr/local/lib/python3.10/dist-packages/tflearn/layers\n",
            "from tensorflow.python.util.nest import is_sequence_or_composite as is_sequence_or_composite as is_sequence_or_composite as is_sequence_or_composite as is_sequence_or_composite as is_sequence_or_composite as is_sequence_or_composite as is_sequence\n",
            "    if args is None or (is_sequence(args) and not args):\n",
            "    if not is_sequence(args):\n"
          ]
        }
      ],
      "source": [
        "!pip show tflearn\n",
        "%cd /usr/local/lib/python3.10/dist-packages/tflearn/layers\n",
        "!sed -i 's/from tensorflow.python.util.nest import is_sequence/from tensorflow.python.util.nest import is_sequence_or_composite as is_sequence/' recurrent.py\n",
        "!grep 'is_sequence' recurrent.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4l2MxiBoqF8Q",
        "outputId": "b45c6ce2-3cae-4d9b-b950-93c52114be0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tflearn\n",
            "                 resize_mode=Image.LANCZOS):\n"
          ]
        }
      ],
      "source": [
        "%cd /usr/local/lib/python3.10/dist-packages/tflearn\n",
        "!sed -i 's/Image.ANTIALIAS/Image.LANCZOS/' data_utils.py\n",
        "!grep 'Image.LANCZOS' data_utils.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4NRLzJh6zVSj",
        "outputId": "937f475d-b059-4765-b09a-420624808b07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tflearn\n",
        "from tflearn.data_utils import to_categorical\n",
        "import tensorflow\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ytoZKrF4j0gF"
      },
      "outputs": [],
      "source": [
        "def cargarDataset(ruta_dataset, tamanio_imagen=(300, 300), test_size=0.2):\n",
        "    dataset = []\n",
        "    etiquetas = []\n",
        "    etiquetas_mapping = {}\n",
        "    clases = sorted(os.listdir(ruta_dataset))\n",
        "    etiquetas_mapping = {clase: idx for idx, clase in enumerate(clases)}\n",
        "    for clase in clases:\n",
        "        ruta_clase = os.path.join(ruta_dataset, clase)\n",
        "        etiqueta_num = etiquetas_mapping[clase]\n",
        "        with os.scandir(ruta_clase) as entries:\n",
        "            for entry in entries:\n",
        "                if entry.is_file():\n",
        "                    imagen_ruta = os.path.join(ruta_clase, entry.name)\n",
        "                    imagen = Image.open(imagen_ruta).convert('RGB')\n",
        "                    imagen_resized = imagen.resize(tamanio_imagen)\n",
        "                    imagen_array = np.array(imagen_resized) / 255.0\n",
        "                    dataset.append(imagen_array)\n",
        "                    etiquetas.append(etiqueta_num)\n",
        "    dataset = np.array(dataset, dtype=np.float32)\n",
        "    etiquetas = np.array(etiquetas, dtype=np.int32)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(dataset, etiquetas, test_size=test_size, random_state=42)\n",
        "    return (X_train, y_train), (X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xxEr5B313jEc"
      },
      "outputs": [],
      "source": [
        "tensorflow.compat.v1.reset_default_graph()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0TNcwa-s4Ir"
      },
      "outputs": [],
      "source": [
        "(X_train, y_train), (X_test, y_test) = cargarDataset('/content/drive/MyDrive/SIS421/Laboratorio N°1 - SIS421/Plantas')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jVr8rLAys84i"
      },
      "outputs": [],
      "source": [
        "input_dim = 300 * 300 * 3\n",
        "hidden_units = 256\n",
        "output_classes = 5\n",
        "\n",
        "input_layer = tflearn.input_data(shape=[None, input_dim])\n",
        "hidden_layer = tflearn.fully_connected(input_layer, hidden_units, activation='relu')\n",
        "output_layer = tflearn.fully_connected(hidden_layer, output_classes, activation='softmax')\n",
        "network = tflearn.regression(output_layer, optimizer='sgd', learning_rate=0.001, loss='categorical_crossentropy')\n",
        "model = tflearn.DNN(network)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Yj6ZXBuu-Fk",
        "outputId": "1367ebdf-045e-4851-e0c6-e438f6a304a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Forma de X_train después de la corrección: (8000, 270000)\n",
            "Forma de y_train después de la corrección: (8000, 5)\n"
          ]
        }
      ],
      "source": [
        "X_train = X_train.reshape(-1, input_dim)\n",
        "X_test = X_test.reshape(-1, input_dim)\n",
        "\n",
        "y_train = to_categorical(y_train, nb_classes=output_classes)\n",
        "y_test = to_categorical(y_test, nb_classes=output_classes)\n",
        "\n",
        "print(f\"Forma de X_train después de la corrección: {X_train.shape}\")\n",
        "print(f\"Forma de y_train después de la corrección: {y_train.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSkZgvjikAVR",
        "outputId": "bd150a56-38d9-413d-c0d3-6f78724b2b0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "--\n",
            "Training Step: 8702  | total loss: \u001b[1m\u001b[32m0.62993\u001b[0m\u001b[0m | time: 0.570s\n",
            "| SGD | epoch: 737 | loss: 0.62993 - acc: 0.8109 -- iter: 1000/8000\n",
            "Training Step: 8703  | total loss: \u001b[1m\u001b[32m0.62657\u001b[0m\u001b[0m | time: 1.141s\n",
            "| SGD | epoch: 737 | loss: 0.62657 - acc: 0.8130 -- iter: 2000/8000\n",
            "Training Step: 8704  | total loss: \u001b[1m\u001b[32m0.63055\u001b[0m\u001b[0m | time: 1.700s\n",
            "| SGD | epoch: 737 | loss: 0.63055 - acc: 0.8105 -- iter: 3000/8000\n",
            "Training Step: 8705  | total loss: \u001b[1m\u001b[32m0.62693\u001b[0m\u001b[0m | time: 2.271s\n",
            "| SGD | epoch: 737 | loss: 0.62693 - acc: 0.8116 -- iter: 4000/8000\n",
            "Training Step: 8706  | total loss: \u001b[1m\u001b[32m0.62300\u001b[0m\u001b[0m | time: 2.841s\n",
            "| SGD | epoch: 737 | loss: 0.62300 - acc: 0.8139 -- iter: 5000/8000\n",
            "Training Step: 8707  | total loss: \u001b[1m\u001b[32m0.62015\u001b[0m\u001b[0m | time: 3.420s\n",
            "| SGD | epoch: 737 | loss: 0.62015 - acc: 0.8166 -- iter: 6000/8000\n",
            "Training Step: 8708  | total loss: \u001b[1m\u001b[32m0.62270\u001b[0m\u001b[0m | time: 3.993s\n",
            "| SGD | epoch: 737 | loss: 0.62270 - acc: 0.8147 -- iter: 7000/8000\n",
            "Training Step: 8709  | total loss: \u001b[1m\u001b[32m0.62403\u001b[0m\u001b[0m | time: 4.561s\n",
            "| SGD | epoch: 737 | loss: 0.62403 - acc: 0.8145 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 8710  | total loss: \u001b[1m\u001b[32m0.62478\u001b[0m\u001b[0m | time: 0.580s\n",
            "| SGD | epoch: 738 | loss: 0.62478 - acc: 0.8143 -- iter: 1000/8000\n",
            "Training Step: 8711  | total loss: \u001b[1m\u001b[32m0.62214\u001b[0m\u001b[0m | time: 1.144s\n",
            "| SGD | epoch: 738 | loss: 0.62214 - acc: 0.8154 -- iter: 2000/8000\n",
            "Training Step: 8712  | total loss: \u001b[1m\u001b[32m0.61992\u001b[0m\u001b[0m | time: 1.731s\n",
            "| SGD | epoch: 738 | loss: 0.61992 - acc: 0.8147 -- iter: 3000/8000\n",
            "Training Step: 8713  | total loss: \u001b[1m\u001b[32m0.62150\u001b[0m\u001b[0m | time: 2.302s\n",
            "| SGD | epoch: 738 | loss: 0.62150 - acc: 0.8155 -- iter: 4000/8000\n",
            "Training Step: 8714  | total loss: \u001b[1m\u001b[32m0.62008\u001b[0m\u001b[0m | time: 2.870s\n",
            "| SGD | epoch: 738 | loss: 0.62008 - acc: 0.8159 -- iter: 5000/8000\n",
            "Training Step: 8715  | total loss: \u001b[1m\u001b[32m0.61817\u001b[0m\u001b[0m | time: 3.452s\n",
            "| SGD | epoch: 738 | loss: 0.61817 - acc: 0.8170 -- iter: 6000/8000\n",
            "Training Step: 8716  | total loss: \u001b[1m\u001b[32m0.61906\u001b[0m\u001b[0m | time: 4.014s\n",
            "| SGD | epoch: 738 | loss: 0.61906 - acc: 0.8161 -- iter: 7000/8000\n",
            "Training Step: 8717  | total loss: \u001b[1m\u001b[32m0.62156\u001b[0m\u001b[0m | time: 4.592s\n",
            "| SGD | epoch: 738 | loss: 0.62156 - acc: 0.8163 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 8718  | total loss: \u001b[1m\u001b[32m0.62177\u001b[0m\u001b[0m | time: 0.571s\n",
            "| SGD | epoch: 739 | loss: 0.62177 - acc: 0.8164 -- iter: 1000/8000\n",
            "Training Step: 8719  | total loss: \u001b[1m\u001b[32m0.61962\u001b[0m\u001b[0m | time: 1.150s\n",
            "| SGD | epoch: 739 | loss: 0.61962 - acc: 0.8169 -- iter: 2000/8000\n",
            "Training Step: 8720  | total loss: \u001b[1m\u001b[32m0.62301\u001b[0m\u001b[0m | time: 1.732s\n",
            "| SGD | epoch: 739 | loss: 0.62301 - acc: 0.8162 -- iter: 3000/8000\n",
            "Training Step: 8721  | total loss: \u001b[1m\u001b[32m0.62346\u001b[0m\u001b[0m | time: 2.306s\n",
            "| SGD | epoch: 739 | loss: 0.62346 - acc: 0.8143 -- iter: 4000/8000\n",
            "Training Step: 8722  | total loss: \u001b[1m\u001b[32m0.62262\u001b[0m\u001b[0m | time: 2.882s\n",
            "| SGD | epoch: 739 | loss: 0.62262 - acc: 0.8156 -- iter: 5000/8000\n",
            "Training Step: 8723  | total loss: \u001b[1m\u001b[32m0.62383\u001b[0m\u001b[0m | time: 3.463s\n",
            "| SGD | epoch: 739 | loss: 0.62383 - acc: 0.8141 -- iter: 6000/8000\n",
            "Training Step: 8724  | total loss: \u001b[1m\u001b[32m0.62303\u001b[0m\u001b[0m | time: 4.033s\n",
            "| SGD | epoch: 739 | loss: 0.62303 - acc: 0.8151 -- iter: 7000/8000\n",
            "Training Step: 8725  | total loss: \u001b[1m\u001b[32m0.62267\u001b[0m\u001b[0m | time: 4.615s\n",
            "| SGD | epoch: 739 | loss: 0.62267 - acc: 0.8156 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 8726  | total loss: \u001b[1m\u001b[32m0.61973\u001b[0m\u001b[0m | time: 0.564s\n",
            "| SGD | epoch: 740 | loss: 0.61973 - acc: 0.8169 -- iter: 1000/8000\n",
            "Training Step: 8727  | total loss: \u001b[1m\u001b[32m0.62223\u001b[0m\u001b[0m | time: 1.132s\n",
            "| SGD | epoch: 740 | loss: 0.62223 - acc: 0.8168 -- iter: 2000/8000\n",
            "Training Step: 8728  | total loss: \u001b[1m\u001b[32m0.62051\u001b[0m\u001b[0m | time: 1.700s\n",
            "| SGD | epoch: 740 | loss: 0.62051 - acc: 0.8171 -- iter: 3000/8000\n",
            "Training Step: 8729  | total loss: \u001b[1m\u001b[32m0.62557\u001b[0m\u001b[0m | time: 2.269s\n",
            "| SGD | epoch: 740 | loss: 0.62557 - acc: 0.8159 -- iter: 4000/8000\n",
            "Training Step: 8730  | total loss: \u001b[1m\u001b[32m0.62393\u001b[0m\u001b[0m | time: 2.839s\n",
            "| SGD | epoch: 740 | loss: 0.62393 - acc: 0.8158 -- iter: 5000/8000\n",
            "Training Step: 8731  | total loss: \u001b[1m\u001b[32m0.62241\u001b[0m\u001b[0m | time: 3.417s\n",
            "| SGD | epoch: 740 | loss: 0.62241 - acc: 0.8153 -- iter: 6000/8000\n",
            "Training Step: 8732  | total loss: \u001b[1m\u001b[32m0.62287\u001b[0m\u001b[0m | time: 3.980s\n",
            "| SGD | epoch: 740 | loss: 0.62287 - acc: 0.8143 -- iter: 7000/8000\n",
            "Training Step: 8733  | total loss: \u001b[1m\u001b[32m0.62344\u001b[0m\u001b[0m | time: 4.553s\n",
            "| SGD | epoch: 740 | loss: 0.62344 - acc: 0.8153 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 8734  | total loss: \u001b[1m\u001b[32m0.62340\u001b[0m\u001b[0m | time: 0.575s\n",
            "| SGD | epoch: 741 | loss: 0.62340 - acc: 0.8148 -- iter: 1000/8000\n",
            "Training Step: 8735  | total loss: \u001b[1m\u001b[32m0.61975\u001b[0m\u001b[0m | time: 1.144s\n",
            "| SGD | epoch: 741 | loss: 0.61975 - acc: 0.8156 -- iter: 2000/8000\n",
            "Training Step: 8736  | total loss: \u001b[1m\u001b[32m0.62357\u001b[0m\u001b[0m | time: 1.709s\n",
            "| SGD | epoch: 741 | loss: 0.62357 - acc: 0.8119 -- iter: 3000/8000\n",
            "Training Step: 8737  | total loss: \u001b[1m\u001b[32m0.62307\u001b[0m\u001b[0m | time: 2.276s\n",
            "| SGD | epoch: 741 | loss: 0.62307 - acc: 0.8121 -- iter: 4000/8000\n",
            "Training Step: 8738  | total loss: \u001b[1m\u001b[32m0.62174\u001b[0m\u001b[0m | time: 2.860s\n",
            "| SGD | epoch: 741 | loss: 0.62174 - acc: 0.8123 -- iter: 5000/8000\n",
            "Training Step: 8739  | total loss: \u001b[1m\u001b[32m0.62314\u001b[0m\u001b[0m | time: 3.423s\n",
            "| SGD | epoch: 741 | loss: 0.62314 - acc: 0.8131 -- iter: 6000/8000\n",
            "Training Step: 8740  | total loss: \u001b[1m\u001b[32m0.62059\u001b[0m\u001b[0m | time: 3.997s\n",
            "| SGD | epoch: 741 | loss: 0.62059 - acc: 0.8147 -- iter: 7000/8000\n",
            "Training Step: 8741  | total loss: \u001b[1m\u001b[32m0.62189\u001b[0m\u001b[0m | time: 4.569s\n",
            "| SGD | epoch: 741 | loss: 0.62189 - acc: 0.8144 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 8742  | total loss: \u001b[1m\u001b[32m0.62247\u001b[0m\u001b[0m | time: 0.581s\n",
            "| SGD | epoch: 742 | loss: 0.62247 - acc: 0.8150 -- iter: 1000/8000\n",
            "Training Step: 8743  | total loss: \u001b[1m\u001b[32m0.62520\u001b[0m\u001b[0m | time: 1.160s\n",
            "| SGD | epoch: 742 | loss: 0.62520 - acc: 0.8139 -- iter: 2000/8000\n",
            "Training Step: 8744  | total loss: \u001b[1m\u001b[32m0.61992\u001b[0m\u001b[0m | time: 1.740s\n",
            "| SGD | epoch: 742 | loss: 0.61992 - acc: 0.8166 -- iter: 3000/8000\n",
            "Training Step: 8745  | total loss: \u001b[1m\u001b[32m0.61879\u001b[0m\u001b[0m | time: 2.313s\n",
            "| SGD | epoch: 742 | loss: 0.61879 - acc: 0.8165 -- iter: 4000/8000\n",
            "Training Step: 8746  | total loss: \u001b[1m\u001b[32m0.61941\u001b[0m\u001b[0m | time: 2.888s\n",
            "| SGD | epoch: 742 | loss: 0.61941 - acc: 0.8167 -- iter: 5000/8000\n",
            "Training Step: 8747  | total loss: \u001b[1m\u001b[32m0.62252\u001b[0m\u001b[0m | time: 3.468s\n",
            "| SGD | epoch: 742 | loss: 0.62252 - acc: 0.8155 -- iter: 6000/8000\n",
            "Training Step: 8748  | total loss: \u001b[1m\u001b[32m0.62313\u001b[0m\u001b[0m | time: 4.047s\n",
            "| SGD | epoch: 742 | loss: 0.62313 - acc: 0.8171 -- iter: 7000/8000\n",
            "Training Step: 8749  | total loss: \u001b[1m\u001b[32m0.62287\u001b[0m\u001b[0m | time: 4.621s\n",
            "| SGD | epoch: 742 | loss: 0.62287 - acc: 0.8172 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 8750  | total loss: \u001b[1m\u001b[32m0.62013\u001b[0m\u001b[0m | time: 0.578s\n",
            "| SGD | epoch: 743 | loss: 0.62013 - acc: 0.8178 -- iter: 1000/8000\n",
            "Training Step: 8751  | total loss: \u001b[1m\u001b[32m0.61717\u001b[0m\u001b[0m | time: 1.134s\n",
            "| SGD | epoch: 743 | loss: 0.61717 - acc: 0.8195 -- iter: 2000/8000\n",
            "Training Step: 8752  | total loss: \u001b[1m\u001b[32m0.61713\u001b[0m\u001b[0m | time: 1.710s\n",
            "| SGD | epoch: 743 | loss: 0.61713 - acc: 0.8168 -- iter: 3000/8000\n",
            "Training Step: 8753  | total loss: \u001b[1m\u001b[32m0.61753\u001b[0m\u001b[0m | time: 2.272s\n",
            "| SGD | epoch: 743 | loss: 0.61753 - acc: 0.8167 -- iter: 4000/8000\n",
            "Training Step: 8754  | total loss: \u001b[1m\u001b[32m0.61818\u001b[0m\u001b[0m | time: 2.846s\n",
            "| SGD | epoch: 743 | loss: 0.61818 - acc: 0.8163 -- iter: 5000/8000\n",
            "Training Step: 8755  | total loss: \u001b[1m\u001b[32m0.61450\u001b[0m\u001b[0m | time: 3.420s\n",
            "| SGD | epoch: 743 | loss: 0.61450 - acc: 0.8171 -- iter: 6000/8000\n",
            "Training Step: 8756  | total loss: \u001b[1m\u001b[32m0.61593\u001b[0m\u001b[0m | time: 3.989s\n",
            "| SGD | epoch: 743 | loss: 0.61593 - acc: 0.8161 -- iter: 7000/8000\n",
            "Training Step: 8757  | total loss: \u001b[1m\u001b[32m0.61553\u001b[0m\u001b[0m | time: 4.561s\n",
            "| SGD | epoch: 743 | loss: 0.61553 - acc: 0.8163 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 8758  | total loss: \u001b[1m\u001b[32m0.61704\u001b[0m\u001b[0m | time: 0.568s\n",
            "| SGD | epoch: 744 | loss: 0.61704 - acc: 0.8144 -- iter: 1000/8000\n",
            "Training Step: 8759  | total loss: \u001b[1m\u001b[32m0.61678\u001b[0m\u001b[0m | time: 1.142s\n",
            "| SGD | epoch: 744 | loss: 0.61678 - acc: 0.8158 -- iter: 2000/8000\n",
            "Training Step: 8760  | total loss: \u001b[1m\u001b[32m0.61617\u001b[0m\u001b[0m | time: 1.715s\n",
            "| SGD | epoch: 744 | loss: 0.61617 - acc: 0.8170 -- iter: 3000/8000\n",
            "Training Step: 8761  | total loss: \u001b[1m\u001b[32m0.61851\u001b[0m\u001b[0m | time: 2.292s\n",
            "| SGD | epoch: 744 | loss: 0.61851 - acc: 0.8167 -- iter: 4000/8000\n",
            "Training Step: 8762  | total loss: \u001b[1m\u001b[32m0.61793\u001b[0m\u001b[0m | time: 2.865s\n",
            "| SGD | epoch: 744 | loss: 0.61793 - acc: 0.8176 -- iter: 5000/8000\n",
            "Training Step: 8763  | total loss: \u001b[1m\u001b[32m0.62019\u001b[0m\u001b[0m | time: 3.438s\n",
            "| SGD | epoch: 744 | loss: 0.62019 - acc: 0.8173 -- iter: 6000/8000\n",
            "Training Step: 8764  | total loss: \u001b[1m\u001b[32m0.61614\u001b[0m\u001b[0m | time: 4.007s\n",
            "| SGD | epoch: 744 | loss: 0.61614 - acc: 0.8184 -- iter: 7000/8000\n",
            "Training Step: 8765  | total loss: \u001b[1m\u001b[32m0.61822\u001b[0m\u001b[0m | time: 4.580s\n",
            "| SGD | epoch: 744 | loss: 0.61822 - acc: 0.8168 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 8766  | total loss: \u001b[1m\u001b[32m0.62056\u001b[0m\u001b[0m | time: 0.578s\n",
            "| SGD | epoch: 745 | loss: 0.62056 - acc: 0.8161 -- iter: 1000/8000\n",
            "Training Step: 8767  | total loss: \u001b[1m\u001b[32m0.62155\u001b[0m\u001b[0m | time: 1.154s\n",
            "| SGD | epoch: 745 | loss: 0.62155 - acc: 0.8146 -- iter: 2000/8000\n",
            "Training Step: 8768  | total loss: \u001b[1m\u001b[32m0.62361\u001b[0m\u001b[0m | time: 1.736s\n",
            "| SGD | epoch: 745 | loss: 0.62361 - acc: 0.8140 -- iter: 3000/8000\n",
            "Training Step: 8769  | total loss: \u001b[1m\u001b[32m0.62378\u001b[0m\u001b[0m | time: 2.312s\n",
            "| SGD | epoch: 745 | loss: 0.62378 - acc: 0.8152 -- iter: 4000/8000\n",
            "Training Step: 8770  | total loss: \u001b[1m\u001b[32m0.62126\u001b[0m\u001b[0m | time: 2.886s\n",
            "| SGD | epoch: 745 | loss: 0.62126 - acc: 0.8172 -- iter: 5000/8000\n",
            "Training Step: 8771  | total loss: \u001b[1m\u001b[32m0.61789\u001b[0m\u001b[0m | time: 3.462s\n",
            "| SGD | epoch: 745 | loss: 0.61789 - acc: 0.8178 -- iter: 6000/8000\n",
            "Training Step: 8772  | total loss: \u001b[1m\u001b[32m0.61521\u001b[0m\u001b[0m | time: 4.033s\n",
            "| SGD | epoch: 745 | loss: 0.61521 - acc: 0.8186 -- iter: 7000/8000\n",
            "Training Step: 8773  | total loss: \u001b[1m\u001b[32m0.61685\u001b[0m\u001b[0m | time: 4.610s\n",
            "| SGD | epoch: 745 | loss: 0.61685 - acc: 0.8192 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 8774  | total loss: \u001b[1m\u001b[32m0.61765\u001b[0m\u001b[0m | time: 0.571s\n",
            "| SGD | epoch: 746 | loss: 0.61765 - acc: 0.8181 -- iter: 1000/8000\n",
            "Training Step: 8775  | total loss: \u001b[1m\u001b[32m0.61595\u001b[0m\u001b[0m | time: 1.138s\n",
            "| SGD | epoch: 746 | loss: 0.61595 - acc: 0.8180 -- iter: 2000/8000\n",
            "Training Step: 8776  | total loss: \u001b[1m\u001b[32m0.61572\u001b[0m\u001b[0m | time: 1.708s\n",
            "| SGD | epoch: 746 | loss: 0.61572 - acc: 0.8169 -- iter: 3000/8000\n",
            "Training Step: 8777  | total loss: \u001b[1m\u001b[32m0.61640\u001b[0m\u001b[0m | time: 2.277s\n",
            "| SGD | epoch: 746 | loss: 0.61640 - acc: 0.8167 -- iter: 4000/8000\n",
            "Training Step: 8778  | total loss: \u001b[1m\u001b[32m0.61642\u001b[0m\u001b[0m | time: 2.848s\n",
            "| SGD | epoch: 746 | loss: 0.61642 - acc: 0.8158 -- iter: 5000/8000\n",
            "Training Step: 8779  | total loss: \u001b[1m\u001b[32m0.61707\u001b[0m\u001b[0m | time: 3.419s\n",
            "| SGD | epoch: 746 | loss: 0.61707 - acc: 0.8172 -- iter: 6000/8000\n",
            "Training Step: 8780  | total loss: \u001b[1m\u001b[32m0.61874\u001b[0m\u001b[0m | time: 3.993s\n",
            "| SGD | epoch: 746 | loss: 0.61874 - acc: 0.8178 -- iter: 7000/8000\n",
            "Training Step: 8781  | total loss: \u001b[1m\u001b[32m0.61957\u001b[0m\u001b[0m | time: 4.558s\n",
            "| SGD | epoch: 746 | loss: 0.61957 - acc: 0.8195 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 8782  | total loss: \u001b[1m\u001b[32m0.61789\u001b[0m\u001b[0m | time: 0.568s\n",
            "| SGD | epoch: 747 | loss: 0.61789 - acc: 0.8179 -- iter: 1000/8000\n",
            "Training Step: 8783  | total loss: \u001b[1m\u001b[32m0.61867\u001b[0m\u001b[0m | time: 1.139s\n",
            "| SGD | epoch: 747 | loss: 0.61867 - acc: 0.8164 -- iter: 2000/8000\n",
            "Training Step: 8784  | total loss: \u001b[1m\u001b[32m0.61756\u001b[0m\u001b[0m | time: 1.698s\n",
            "| SGD | epoch: 747 | loss: 0.61756 - acc: 0.8161 -- iter: 3000/8000\n",
            "Training Step: 8785  | total loss: \u001b[1m\u001b[32m0.61967\u001b[0m\u001b[0m | time: 2.267s\n",
            "| SGD | epoch: 747 | loss: 0.61967 - acc: 0.8164 -- iter: 4000/8000\n",
            "Training Step: 8786  | total loss: \u001b[1m\u001b[32m0.61832\u001b[0m\u001b[0m | time: 2.827s\n",
            "| SGD | epoch: 747 | loss: 0.61832 - acc: 0.8152 -- iter: 5000/8000\n",
            "Training Step: 8787  | total loss: \u001b[1m\u001b[32m0.61542\u001b[0m\u001b[0m | time: 3.397s\n",
            "| SGD | epoch: 747 | loss: 0.61542 - acc: 0.8164 -- iter: 6000/8000\n",
            "Training Step: 8788  | total loss: \u001b[1m\u001b[32m0.61389\u001b[0m\u001b[0m | time: 3.961s\n",
            "| SGD | epoch: 747 | loss: 0.61389 - acc: 0.8172 -- iter: 7000/8000\n",
            "Training Step: 8789  | total loss: \u001b[1m\u001b[32m0.61532\u001b[0m\u001b[0m | time: 4.532s\n",
            "| SGD | epoch: 747 | loss: 0.61532 - acc: 0.8177 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 8790  | total loss: \u001b[1m\u001b[32m0.62001\u001b[0m\u001b[0m | time: 0.564s\n",
            "| SGD | epoch: 748 | loss: 0.62001 - acc: 0.8167 -- iter: 1000/8000\n",
            "Training Step: 8791  | total loss: \u001b[1m\u001b[32m0.62078\u001b[0m\u001b[0m | time: 1.140s\n",
            "| SGD | epoch: 748 | loss: 0.62078 - acc: 0.8172 -- iter: 2000/8000\n",
            "Training Step: 8792  | total loss: \u001b[1m\u001b[32m0.62136\u001b[0m\u001b[0m | time: 1.713s\n",
            "| SGD | epoch: 748 | loss: 0.62136 - acc: 0.8154 -- iter: 3000/8000\n",
            "Training Step: 8793  | total loss: \u001b[1m\u001b[32m0.61840\u001b[0m\u001b[0m | time: 2.285s\n",
            "| SGD | epoch: 748 | loss: 0.61840 - acc: 0.8154 -- iter: 4000/8000\n",
            "Training Step: 8794  | total loss: \u001b[1m\u001b[32m0.62056\u001b[0m\u001b[0m | time: 2.859s\n",
            "| SGD | epoch: 748 | loss: 0.62056 - acc: 0.8146 -- iter: 5000/8000\n",
            "Training Step: 8795  | total loss: \u001b[1m\u001b[32m0.61761\u001b[0m\u001b[0m | time: 3.425s\n",
            "| SGD | epoch: 748 | loss: 0.61761 - acc: 0.8162 -- iter: 6000/8000\n",
            "Training Step: 8796  | total loss: \u001b[1m\u001b[32m0.61508\u001b[0m\u001b[0m | time: 3.992s\n",
            "| SGD | epoch: 748 | loss: 0.61508 - acc: 0.8183 -- iter: 7000/8000\n",
            "Training Step: 8797  | total loss: \u001b[1m\u001b[32m0.61701\u001b[0m\u001b[0m | time: 4.561s\n",
            "| SGD | epoch: 748 | loss: 0.61701 - acc: 0.8178 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 8798  | total loss: \u001b[1m\u001b[32m0.61227\u001b[0m\u001b[0m | time: 0.564s\n",
            "| SGD | epoch: 749 | loss: 0.61227 - acc: 0.8199 -- iter: 1000/8000\n",
            "Training Step: 8799  | total loss: \u001b[1m\u001b[32m0.61203\u001b[0m\u001b[0m | time: 1.126s\n",
            "| SGD | epoch: 749 | loss: 0.61203 - acc: 0.8204 -- iter: 2000/8000\n",
            "Training Step: 8800  | total loss: \u001b[1m\u001b[32m0.61508\u001b[0m\u001b[0m | time: 1.706s\n",
            "| SGD | epoch: 749 | loss: 0.61508 - acc: 0.8189 -- iter: 3000/8000\n",
            "Training Step: 8801  | total loss: \u001b[1m\u001b[32m0.61671\u001b[0m\u001b[0m | time: 2.278s\n",
            "| SGD | epoch: 749 | loss: 0.61671 - acc: 0.8179 -- iter: 4000/8000\n",
            "Training Step: 8802  | total loss: \u001b[1m\u001b[32m0.61488\u001b[0m\u001b[0m | time: 2.851s\n",
            "| SGD | epoch: 749 | loss: 0.61488 - acc: 0.8176 -- iter: 5000/8000\n",
            "Training Step: 8803  | total loss: \u001b[1m\u001b[32m0.61375\u001b[0m\u001b[0m | time: 3.427s\n",
            "| SGD | epoch: 749 | loss: 0.61375 - acc: 0.8201 -- iter: 6000/8000\n",
            "Training Step: 8804  | total loss: \u001b[1m\u001b[32m0.61241\u001b[0m\u001b[0m | time: 3.999s\n",
            "| SGD | epoch: 749 | loss: 0.61241 - acc: 0.8205 -- iter: 7000/8000\n",
            "Training Step: 8805  | total loss: \u001b[1m\u001b[32m0.61002\u001b[0m\u001b[0m | time: 4.577s\n",
            "| SGD | epoch: 749 | loss: 0.61002 - acc: 0.8217 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 8806  | total loss: \u001b[1m\u001b[32m0.61140\u001b[0m\u001b[0m | time: 0.570s\n",
            "| SGD | epoch: 750 | loss: 0.61140 - acc: 0.8215 -- iter: 1000/8000\n",
            "Training Step: 8807  | total loss: \u001b[1m\u001b[32m0.61135\u001b[0m\u001b[0m | time: 1.146s\n",
            "| SGD | epoch: 750 | loss: 0.61135 - acc: 0.8220 -- iter: 2000/8000\n",
            "Training Step: 8808  | total loss: \u001b[1m\u001b[32m0.60873\u001b[0m\u001b[0m | time: 1.727s\n",
            "| SGD | epoch: 750 | loss: 0.60873 - acc: 0.8245 -- iter: 3000/8000\n",
            "Training Step: 8809  | total loss: \u001b[1m\u001b[32m0.61240\u001b[0m\u001b[0m | time: 2.302s\n",
            "| SGD | epoch: 750 | loss: 0.61240 - acc: 0.8225 -- iter: 4000/8000\n",
            "Training Step: 8810  | total loss: \u001b[1m\u001b[32m0.61249\u001b[0m\u001b[0m | time: 2.877s\n",
            "| SGD | epoch: 750 | loss: 0.61249 - acc: 0.8219 -- iter: 5000/8000\n",
            "Training Step: 8811  | total loss: \u001b[1m\u001b[32m0.61280\u001b[0m\u001b[0m | time: 3.449s\n",
            "| SGD | epoch: 750 | loss: 0.61280 - acc: 0.8224 -- iter: 6000/8000\n",
            "Training Step: 8812  | total loss: \u001b[1m\u001b[32m0.61187\u001b[0m\u001b[0m | time: 4.027s\n",
            "| SGD | epoch: 750 | loss: 0.61187 - acc: 0.8210 -- iter: 7000/8000\n",
            "Training Step: 8813  | total loss: \u001b[1m\u001b[32m0.61059\u001b[0m\u001b[0m | time: 4.598s\n",
            "| SGD | epoch: 750 | loss: 0.61059 - acc: 0.8213 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 8814  | total loss: \u001b[1m\u001b[32m0.60925\u001b[0m\u001b[0m | time: 0.578s\n",
            "| SGD | epoch: 751 | loss: 0.60925 - acc: 0.8222 -- iter: 1000/8000\n",
            "Training Step: 8815  | total loss: \u001b[1m\u001b[32m0.60974\u001b[0m\u001b[0m | time: 1.166s\n",
            "| SGD | epoch: 751 | loss: 0.60974 - acc: 0.8229 -- iter: 2000/8000\n",
            "Training Step: 8816  | total loss: \u001b[1m\u001b[32m0.61099\u001b[0m\u001b[0m | time: 1.753s\n",
            "| SGD | epoch: 751 | loss: 0.61099 - acc: 0.8208 -- iter: 3000/8000\n",
            "Training Step: 8817  | total loss: \u001b[1m\u001b[32m0.61398\u001b[0m\u001b[0m | time: 2.328s\n",
            "| SGD | epoch: 751 | loss: 0.61398 - acc: 0.8202 -- iter: 4000/8000\n",
            "Training Step: 8818  | total loss: \u001b[1m\u001b[32m0.61377\u001b[0m\u001b[0m | time: 2.901s\n",
            "| SGD | epoch: 751 | loss: 0.61377 - acc: 0.8200 -- iter: 5000/8000\n",
            "Training Step: 8819  | total loss: \u001b[1m\u001b[32m0.61375\u001b[0m\u001b[0m | time: 3.491s\n",
            "| SGD | epoch: 751 | loss: 0.61375 - acc: 0.8198 -- iter: 6000/8000\n",
            "Training Step: 8820  | total loss: \u001b[1m\u001b[32m0.61203\u001b[0m\u001b[0m | time: 4.077s\n",
            "| SGD | epoch: 751 | loss: 0.61203 - acc: 0.8203 -- iter: 7000/8000\n",
            "Training Step: 8821  | total loss: \u001b[1m\u001b[32m0.61339\u001b[0m\u001b[0m | time: 4.631s\n",
            "| SGD | epoch: 751 | loss: 0.61339 - acc: 0.8187 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 8822  | total loss: \u001b[1m\u001b[32m0.61110\u001b[0m\u001b[0m | time: 0.530s\n",
            "| SGD | epoch: 752 | loss: 0.61110 - acc: 0.8183 -- iter: 1000/8000\n",
            "Training Step: 8823  | total loss: \u001b[1m\u001b[32m0.60820\u001b[0m\u001b[0m | time: 1.101s\n",
            "| SGD | epoch: 752 | loss: 0.60820 - acc: 0.8188 -- iter: 2000/8000\n",
            "Training Step: 8824  | total loss: \u001b[1m\u001b[32m0.60912\u001b[0m\u001b[0m | time: 1.670s\n",
            "| SGD | epoch: 752 | loss: 0.60912 - acc: 0.8180 -- iter: 3000/8000\n",
            "Training Step: 8825  | total loss: \u001b[1m\u001b[32m0.60960\u001b[0m\u001b[0m | time: 2.219s\n",
            "| SGD | epoch: 752 | loss: 0.60960 - acc: 0.8189 -- iter: 4000/8000\n",
            "Training Step: 8826  | total loss: \u001b[1m\u001b[32m0.61142\u001b[0m\u001b[0m | time: 2.761s\n",
            "| SGD | epoch: 752 | loss: 0.61142 - acc: 0.8176 -- iter: 5000/8000\n",
            "Training Step: 8827  | total loss: \u001b[1m\u001b[32m0.61540\u001b[0m\u001b[0m | time: 3.307s\n",
            "| SGD | epoch: 752 | loss: 0.61540 - acc: 0.8161 -- iter: 6000/8000\n",
            "Training Step: 8828  | total loss: \u001b[1m\u001b[32m0.61467\u001b[0m\u001b[0m | time: 3.855s\n",
            "| SGD | epoch: 752 | loss: 0.61467 - acc: 0.8174 -- iter: 7000/8000\n",
            "Training Step: 8829  | total loss: \u001b[1m\u001b[32m0.61569\u001b[0m\u001b[0m | time: 4.394s\n",
            "| SGD | epoch: 752 | loss: 0.61569 - acc: 0.8170 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 8830  | total loss: \u001b[1m\u001b[32m0.61326\u001b[0m\u001b[0m | time: 0.688s\n",
            "| SGD | epoch: 753 | loss: 0.61326 - acc: 0.8181 -- iter: 1000/8000\n",
            "Training Step: 8831  | total loss: \u001b[1m\u001b[32m0.61366\u001b[0m\u001b[0m | time: 1.201s\n",
            "| SGD | epoch: 753 | loss: 0.61366 - acc: 0.8179 -- iter: 2000/8000\n",
            "Training Step: 8832  | total loss: \u001b[1m\u001b[32m0.61365\u001b[0m\u001b[0m | time: 1.713s\n",
            "| SGD | epoch: 753 | loss: 0.61365 - acc: 0.8174 -- iter: 3000/8000\n",
            "Training Step: 8833  | total loss: \u001b[1m\u001b[32m0.61293\u001b[0m\u001b[0m | time: 2.228s\n",
            "| SGD | epoch: 753 | loss: 0.61293 - acc: 0.8194 -- iter: 4000/8000\n",
            "Training Step: 8834  | total loss: \u001b[1m\u001b[32m0.61043\u001b[0m\u001b[0m | time: 2.739s\n",
            "| SGD | epoch: 753 | loss: 0.61043 - acc: 0.8194 -- iter: 5000/8000\n",
            "Training Step: 8835  | total loss: \u001b[1m\u001b[32m0.61356\u001b[0m\u001b[0m | time: 3.248s\n",
            "| SGD | epoch: 753 | loss: 0.61356 - acc: 0.8184 -- iter: 6000/8000\n",
            "Training Step: 8836  | total loss: \u001b[1m\u001b[32m0.61556\u001b[0m\u001b[0m | time: 3.758s\n",
            "| SGD | epoch: 753 | loss: 0.61556 - acc: 0.8179 -- iter: 7000/8000\n",
            "Training Step: 8837  | total loss: \u001b[1m\u001b[32m0.61732\u001b[0m\u001b[0m | time: 4.266s\n",
            "| SGD | epoch: 753 | loss: 0.61732 - acc: 0.8178 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 8838  | total loss: \u001b[1m\u001b[32m0.61619\u001b[0m\u001b[0m | time: 0.512s\n",
            "| SGD | epoch: 754 | loss: 0.61619 - acc: 0.8172 -- iter: 1000/8000\n",
            "Training Step: 8839  | total loss: \u001b[1m\u001b[32m0.61389\u001b[0m\u001b[0m | time: 1.031s\n",
            "| SGD | epoch: 754 | loss: 0.61389 - acc: 0.8179 -- iter: 2000/8000\n",
            "Training Step: 8840  | total loss: \u001b[1m\u001b[32m0.61234\u001b[0m\u001b[0m | time: 1.563s\n",
            "| SGD | epoch: 754 | loss: 0.61234 - acc: 0.8183 -- iter: 3000/8000\n",
            "Training Step: 8841  | total loss: \u001b[1m\u001b[32m0.60995\u001b[0m\u001b[0m | time: 2.080s\n",
            "| SGD | epoch: 754 | loss: 0.60995 - acc: 0.8206 -- iter: 4000/8000\n",
            "Training Step: 8842  | total loss: \u001b[1m\u001b[32m0.60930\u001b[0m\u001b[0m | time: 2.609s\n",
            "| SGD | epoch: 754 | loss: 0.60930 - acc: 0.8203 -- iter: 5000/8000\n",
            "Training Step: 8843  | total loss: \u001b[1m\u001b[32m0.61032\u001b[0m\u001b[0m | time: 3.135s\n",
            "| SGD | epoch: 754 | loss: 0.61032 - acc: 0.8200 -- iter: 6000/8000\n",
            "Training Step: 8844  | total loss: \u001b[1m\u001b[32m0.61376\u001b[0m\u001b[0m | time: 3.660s\n",
            "| SGD | epoch: 754 | loss: 0.61376 - acc: 0.8198 -- iter: 7000/8000\n",
            "Training Step: 8845  | total loss: \u001b[1m\u001b[32m0.61369\u001b[0m\u001b[0m | time: 4.185s\n",
            "| SGD | epoch: 754 | loss: 0.61369 - acc: 0.8190 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 8846  | total loss: \u001b[1m\u001b[32m0.61242\u001b[0m\u001b[0m | time: 0.525s\n",
            "| SGD | epoch: 755 | loss: 0.61242 - acc: 0.8196 -- iter: 1000/8000\n",
            "Training Step: 8847  | total loss: \u001b[1m\u001b[32m0.60988\u001b[0m\u001b[0m | time: 1.046s\n",
            "| SGD | epoch: 755 | loss: 0.60988 - acc: 0.8216 -- iter: 2000/8000\n",
            "Training Step: 8848  | total loss: \u001b[1m\u001b[32m0.61359\u001b[0m\u001b[0m | time: 1.557s\n",
            "| SGD | epoch: 755 | loss: 0.61359 - acc: 0.8204 -- iter: 3000/8000\n",
            "Training Step: 8849  | total loss: \u001b[1m\u001b[32m0.61497\u001b[0m\u001b[0m | time: 2.072s\n",
            "| SGD | epoch: 755 | loss: 0.61497 - acc: 0.8203 -- iter: 4000/8000\n",
            "Training Step: 8850  | total loss: \u001b[1m\u001b[32m0.61529\u001b[0m\u001b[0m | time: 2.577s\n",
            "| SGD | epoch: 755 | loss: 0.61529 - acc: 0.8185 -- iter: 5000/8000\n",
            "Training Step: 8851  | total loss: \u001b[1m\u001b[32m0.61311\u001b[0m\u001b[0m | time: 3.092s\n",
            "| SGD | epoch: 755 | loss: 0.61311 - acc: 0.8196 -- iter: 6000/8000\n",
            "Training Step: 8852  | total loss: \u001b[1m\u001b[32m0.61217\u001b[0m\u001b[0m | time: 3.595s\n",
            "| SGD | epoch: 755 | loss: 0.61217 - acc: 0.8181 -- iter: 7000/8000\n",
            "Training Step: 8853  | total loss: \u001b[1m\u001b[32m0.61064\u001b[0m\u001b[0m | time: 4.120s\n",
            "| SGD | epoch: 755 | loss: 0.61064 - acc: 0.8167 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 8854  | total loss: \u001b[1m\u001b[32m0.61045\u001b[0m\u001b[0m | time: 0.508s\n",
            "| SGD | epoch: 756 | loss: 0.61045 - acc: 0.8177 -- iter: 1000/8000\n",
            "Training Step: 8855  | total loss: \u001b[1m\u001b[32m0.60665\u001b[0m\u001b[0m | time: 1.030s\n",
            "| SGD | epoch: 756 | loss: 0.60665 - acc: 0.8186 -- iter: 2000/8000\n",
            "Training Step: 8856  | total loss: \u001b[1m\u001b[32m0.60962\u001b[0m\u001b[0m | time: 1.545s\n",
            "| SGD | epoch: 756 | loss: 0.60962 - acc: 0.8187 -- iter: 3000/8000\n",
            "Training Step: 8857  | total loss: \u001b[1m\u001b[32m0.61060\u001b[0m\u001b[0m | time: 2.058s\n",
            "| SGD | epoch: 756 | loss: 0.61060 - acc: 0.8186 -- iter: 4000/8000\n",
            "Training Step: 8858  | total loss: \u001b[1m\u001b[32m0.60937\u001b[0m\u001b[0m | time: 2.571s\n",
            "| SGD | epoch: 756 | loss: 0.60937 - acc: 0.8199 -- iter: 5000/8000\n",
            "Training Step: 8859  | total loss: \u001b[1m\u001b[32m0.60771\u001b[0m\u001b[0m | time: 3.088s\n",
            "| SGD | epoch: 756 | loss: 0.60771 - acc: 0.8214 -- iter: 6000/8000\n",
            "Training Step: 8860  | total loss: \u001b[1m\u001b[32m0.61118\u001b[0m\u001b[0m | time: 3.596s\n",
            "| SGD | epoch: 756 | loss: 0.61118 - acc: 0.8202 -- iter: 7000/8000\n",
            "Training Step: 8861  | total loss: \u001b[1m\u001b[32m0.61223\u001b[0m\u001b[0m | time: 4.112s\n",
            "| SGD | epoch: 756 | loss: 0.61223 - acc: 0.8205 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 8862  | total loss: \u001b[1m\u001b[32m0.61191\u001b[0m\u001b[0m | time: 0.529s\n",
            "| SGD | epoch: 757 | loss: 0.61191 - acc: 0.8194 -- iter: 1000/8000\n",
            "Training Step: 8863  | total loss: \u001b[1m\u001b[32m0.61503\u001b[0m\u001b[0m | time: 1.042s\n",
            "| SGD | epoch: 757 | loss: 0.61503 - acc: 0.8200 -- iter: 2000/8000\n",
            "Training Step: 8864  | total loss: \u001b[1m\u001b[32m0.61175\u001b[0m\u001b[0m | time: 1.552s\n",
            "| SGD | epoch: 757 | loss: 0.61175 - acc: 0.8208 -- iter: 3000/8000\n",
            "Training Step: 8865  | total loss: \u001b[1m\u001b[32m0.60650\u001b[0m\u001b[0m | time: 2.073s\n",
            "| SGD | epoch: 757 | loss: 0.60650 - acc: 0.8224 -- iter: 4000/8000\n",
            "Training Step: 8866  | total loss: \u001b[1m\u001b[32m0.60789\u001b[0m\u001b[0m | time: 2.586s\n",
            "| SGD | epoch: 757 | loss: 0.60789 - acc: 0.8214 -- iter: 5000/8000\n",
            "Training Step: 8867  | total loss: \u001b[1m\u001b[32m0.61156\u001b[0m\u001b[0m | time: 3.110s\n",
            "| SGD | epoch: 757 | loss: 0.61156 - acc: 0.8202 -- iter: 6000/8000\n",
            "Training Step: 8868  | total loss: \u001b[1m\u001b[32m0.61468\u001b[0m\u001b[0m | time: 3.630s\n",
            "| SGD | epoch: 757 | loss: 0.61468 - acc: 0.8195 -- iter: 7000/8000\n",
            "Training Step: 8869  | total loss: \u001b[1m\u001b[32m0.61353\u001b[0m\u001b[0m | time: 4.151s\n",
            "| SGD | epoch: 757 | loss: 0.61353 - acc: 0.8197 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 8870  | total loss: \u001b[1m\u001b[32m0.61419\u001b[0m\u001b[0m | time: 0.523s\n",
            "| SGD | epoch: 758 | loss: 0.61419 - acc: 0.8181 -- iter: 1000/8000\n",
            "Training Step: 8871  | total loss: \u001b[1m\u001b[32m0.61378\u001b[0m\u001b[0m | time: 1.050s\n",
            "| SGD | epoch: 758 | loss: 0.61378 - acc: 0.8188 -- iter: 2000/8000\n",
            "Training Step: 8872  | total loss: \u001b[1m\u001b[32m0.61352\u001b[0m\u001b[0m | time: 1.578s\n",
            "| SGD | epoch: 758 | loss: 0.61352 - acc: 0.8182 -- iter: 3000/8000\n",
            "Training Step: 8873  | total loss: \u001b[1m\u001b[32m0.61070\u001b[0m\u001b[0m | time: 2.098s\n",
            "| SGD | epoch: 758 | loss: 0.61070 - acc: 0.8201 -- iter: 4000/8000\n",
            "Training Step: 8874  | total loss: \u001b[1m\u001b[32m0.60876\u001b[0m\u001b[0m | time: 2.741s\n",
            "| SGD | epoch: 758 | loss: 0.60876 - acc: 0.8207 -- iter: 5000/8000\n",
            "Training Step: 8875  | total loss: \u001b[1m\u001b[32m0.61393\u001b[0m\u001b[0m | time: 3.384s\n",
            "| SGD | epoch: 758 | loss: 0.61393 - acc: 0.8195 -- iter: 6000/8000\n",
            "Training Step: 8876  | total loss: \u001b[1m\u001b[32m0.61346\u001b[0m\u001b[0m | time: 4.016s\n",
            "| SGD | epoch: 758 | loss: 0.61346 - acc: 0.8204 -- iter: 7000/8000\n",
            "Training Step: 8877  | total loss: \u001b[1m\u001b[32m0.61286\u001b[0m\u001b[0m | time: 4.652s\n",
            "| SGD | epoch: 758 | loss: 0.61286 - acc: 0.8204 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 8878  | total loss: \u001b[1m\u001b[32m0.61185\u001b[0m\u001b[0m | time: 0.631s\n",
            "| SGD | epoch: 759 | loss: 0.61185 - acc: 0.8192 -- iter: 1000/8000\n",
            "Training Step: 8879  | total loss: \u001b[1m\u001b[32m0.61016\u001b[0m\u001b[0m | time: 1.269s\n",
            "| SGD | epoch: 759 | loss: 0.61016 - acc: 0.8213 -- iter: 2000/8000\n",
            "Training Step: 8880  | total loss: \u001b[1m\u001b[32m0.61145\u001b[0m\u001b[0m | time: 1.907s\n",
            "| SGD | epoch: 759 | loss: 0.61145 - acc: 0.8212 -- iter: 3000/8000\n",
            "Training Step: 8881  | total loss: \u001b[1m\u001b[32m0.61557\u001b[0m\u001b[0m | time: 2.545s\n",
            "| SGD | epoch: 759 | loss: 0.61557 - acc: 0.8190 -- iter: 4000/8000\n",
            "Training Step: 8882  | total loss: \u001b[1m\u001b[32m0.61401\u001b[0m\u001b[0m | time: 3.181s\n",
            "| SGD | epoch: 759 | loss: 0.61401 - acc: 0.8206 -- iter: 5000/8000\n",
            "Training Step: 8883  | total loss: \u001b[1m\u001b[32m0.61593\u001b[0m\u001b[0m | time: 3.817s\n",
            "| SGD | epoch: 759 | loss: 0.61593 - acc: 0.8188 -- iter: 6000/8000\n",
            "Training Step: 8884  | total loss: \u001b[1m\u001b[32m0.61564\u001b[0m\u001b[0m | time: 4.450s\n",
            "| SGD | epoch: 759 | loss: 0.61564 - acc: 0.8185 -- iter: 7000/8000\n",
            "Training Step: 8885  | total loss: \u001b[1m\u001b[32m0.61268\u001b[0m\u001b[0m | time: 5.088s\n",
            "| SGD | epoch: 759 | loss: 0.61268 - acc: 0.8200 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 8886  | total loss: \u001b[1m\u001b[32m0.60953\u001b[0m\u001b[0m | time: 0.632s\n",
            "| SGD | epoch: 760 | loss: 0.60953 - acc: 0.8212 -- iter: 1000/8000\n",
            "Training Step: 8887  | total loss: \u001b[1m\u001b[32m0.60666\u001b[0m\u001b[0m | time: 1.270s\n",
            "| SGD | epoch: 760 | loss: 0.60666 - acc: 0.8233 -- iter: 2000/8000\n",
            "Training Step: 8888  | total loss: \u001b[1m\u001b[32m0.60471\u001b[0m\u001b[0m | time: 1.910s\n",
            "| SGD | epoch: 760 | loss: 0.60471 - acc: 0.8246 -- iter: 3000/8000\n",
            "Training Step: 8889  | total loss: \u001b[1m\u001b[32m0.60926\u001b[0m\u001b[0m | time: 2.544s\n",
            "| SGD | epoch: 760 | loss: 0.60926 - acc: 0.8235 -- iter: 4000/8000\n",
            "Training Step: 8890  | total loss: \u001b[1m\u001b[32m0.60673\u001b[0m\u001b[0m | time: 3.195s\n",
            "| SGD | epoch: 760 | loss: 0.60673 - acc: 0.8235 -- iter: 5000/8000\n",
            "Training Step: 8891  | total loss: \u001b[1m\u001b[32m0.60551\u001b[0m\u001b[0m | time: 3.714s\n",
            "| SGD | epoch: 760 | loss: 0.60551 - acc: 0.8234 -- iter: 6000/8000\n",
            "Training Step: 8892  | total loss: \u001b[1m\u001b[32m0.60496\u001b[0m\u001b[0m | time: 4.239s\n",
            "| SGD | epoch: 760 | loss: 0.60496 - acc: 0.8244 -- iter: 7000/8000\n",
            "Training Step: 8893  | total loss: \u001b[1m\u001b[32m0.60680\u001b[0m\u001b[0m | time: 4.885s\n",
            "| SGD | epoch: 760 | loss: 0.60680 - acc: 0.8238 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 8894  | total loss: \u001b[1m\u001b[32m0.60359\u001b[0m\u001b[0m | time: 0.646s\n",
            "| SGD | epoch: 761 | loss: 0.60359 - acc: 0.8255 -- iter: 1000/8000\n",
            "Training Step: 8895  | total loss: \u001b[1m\u001b[32m0.60029\u001b[0m\u001b[0m | time: 1.290s\n",
            "| SGD | epoch: 761 | loss: 0.60029 - acc: 0.8265 -- iter: 2000/8000\n",
            "Training Step: 8896  | total loss: \u001b[1m\u001b[32m0.60137\u001b[0m\u001b[0m | time: 1.912s\n",
            "| SGD | epoch: 761 | loss: 0.60137 - acc: 0.8253 -- iter: 3000/8000\n",
            "Training Step: 8897  | total loss: \u001b[1m\u001b[32m0.60524\u001b[0m\u001b[0m | time: 2.552s\n",
            "| SGD | epoch: 761 | loss: 0.60524 - acc: 0.8234 -- iter: 4000/8000\n",
            "Training Step: 8898  | total loss: \u001b[1m\u001b[32m0.60554\u001b[0m\u001b[0m | time: 3.185s\n",
            "| SGD | epoch: 761 | loss: 0.60554 - acc: 0.8240 -- iter: 5000/8000\n",
            "Training Step: 8899  | total loss: \u001b[1m\u001b[32m0.60444\u001b[0m\u001b[0m | time: 3.815s\n",
            "| SGD | epoch: 761 | loss: 0.60444 - acc: 0.8245 -- iter: 6000/8000\n",
            "Training Step: 8900  | total loss: \u001b[1m\u001b[32m0.60815\u001b[0m\u001b[0m | time: 4.448s\n",
            "| SGD | epoch: 761 | loss: 0.60815 - acc: 0.8225 -- iter: 7000/8000\n",
            "Training Step: 8901  | total loss: \u001b[1m\u001b[32m0.60639\u001b[0m\u001b[0m | time: 5.078s\n",
            "| SGD | epoch: 761 | loss: 0.60639 - acc: 0.8233 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 8902  | total loss: \u001b[1m\u001b[32m0.60093\u001b[0m\u001b[0m | time: 0.632s\n",
            "| SGD | epoch: 762 | loss: 0.60093 - acc: 0.8247 -- iter: 1000/8000\n",
            "Training Step: 8903  | total loss: \u001b[1m\u001b[32m0.60423\u001b[0m\u001b[0m | time: 1.260s\n",
            "| SGD | epoch: 762 | loss: 0.60423 - acc: 0.8232 -- iter: 2000/8000\n",
            "Training Step: 8904  | total loss: \u001b[1m\u001b[32m0.60643\u001b[0m\u001b[0m | time: 1.892s\n",
            "| SGD | epoch: 762 | loss: 0.60643 - acc: 0.8215 -- iter: 3000/8000\n",
            "Training Step: 8905  | total loss: \u001b[1m\u001b[32m0.60995\u001b[0m\u001b[0m | time: 2.528s\n",
            "| SGD | epoch: 762 | loss: 0.60995 - acc: 0.8189 -- iter: 4000/8000\n",
            "Training Step: 8906  | total loss: \u001b[1m\u001b[32m0.60944\u001b[0m\u001b[0m | time: 3.159s\n",
            "| SGD | epoch: 762 | loss: 0.60944 - acc: 0.8196 -- iter: 5000/8000\n",
            "Training Step: 8907  | total loss: \u001b[1m\u001b[32m0.60904\u001b[0m\u001b[0m | time: 3.790s\n",
            "| SGD | epoch: 762 | loss: 0.60904 - acc: 0.8213 -- iter: 6000/8000\n",
            "Training Step: 8908  | total loss: \u001b[1m\u001b[32m0.60858\u001b[0m\u001b[0m | time: 4.419s\n",
            "| SGD | epoch: 762 | loss: 0.60858 - acc: 0.8202 -- iter: 7000/8000\n",
            "Training Step: 8909  | total loss: \u001b[1m\u001b[32m0.60922\u001b[0m\u001b[0m | time: 5.056s\n",
            "| SGD | epoch: 762 | loss: 0.60922 - acc: 0.8219 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 8910  | total loss: \u001b[1m\u001b[32m0.61175\u001b[0m\u001b[0m | time: 0.676s\n",
            "| SGD | epoch: 763 | loss: 0.61175 - acc: 0.8204 -- iter: 1000/8000\n",
            "Training Step: 8911  | total loss: \u001b[1m\u001b[32m0.61133\u001b[0m\u001b[0m | time: 1.292s\n",
            "| SGD | epoch: 763 | loss: 0.61133 - acc: 0.8205 -- iter: 2000/8000\n",
            "Training Step: 8912  | total loss: \u001b[1m\u001b[32m0.61082\u001b[0m\u001b[0m | time: 1.916s\n",
            "| SGD | epoch: 763 | loss: 0.61082 - acc: 0.8199 -- iter: 3000/8000\n",
            "Training Step: 8913  | total loss: \u001b[1m\u001b[32m0.61025\u001b[0m\u001b[0m | time: 2.541s\n",
            "| SGD | epoch: 763 | loss: 0.61025 - acc: 0.8192 -- iter: 4000/8000\n",
            "Training Step: 8914  | total loss: \u001b[1m\u001b[32m0.60784\u001b[0m\u001b[0m | time: 3.161s\n",
            "| SGD | epoch: 763 | loss: 0.60784 - acc: 0.8194 -- iter: 5000/8000\n",
            "Training Step: 8915  | total loss: \u001b[1m\u001b[32m0.60754\u001b[0m\u001b[0m | time: 3.805s\n",
            "| SGD | epoch: 763 | loss: 0.60754 - acc: 0.8179 -- iter: 6000/8000\n",
            "Training Step: 8916  | total loss: \u001b[1m\u001b[32m0.61078\u001b[0m\u001b[0m | time: 4.442s\n",
            "| SGD | epoch: 763 | loss: 0.61078 - acc: 0.8175 -- iter: 7000/8000\n",
            "Training Step: 8917  | total loss: \u001b[1m\u001b[32m0.60797\u001b[0m\u001b[0m | time: 5.062s\n",
            "| SGD | epoch: 763 | loss: 0.60797 - acc: 0.8196 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 8918  | total loss: \u001b[1m\u001b[32m0.60804\u001b[0m\u001b[0m | time: 0.638s\n",
            "| SGD | epoch: 764 | loss: 0.60804 - acc: 0.8188 -- iter: 1000/8000\n",
            "Training Step: 8919  | total loss: \u001b[1m\u001b[32m0.60910\u001b[0m\u001b[0m | time: 1.268s\n",
            "| SGD | epoch: 764 | loss: 0.60910 - acc: 0.8187 -- iter: 2000/8000\n",
            "Training Step: 8920  | total loss: \u001b[1m\u001b[32m0.60919\u001b[0m\u001b[0m | time: 1.897s\n",
            "| SGD | epoch: 764 | loss: 0.60919 - acc: 0.8188 -- iter: 3000/8000\n",
            "Training Step: 8921  | total loss: \u001b[1m\u001b[32m0.60995\u001b[0m\u001b[0m | time: 2.535s\n",
            "| SGD | epoch: 764 | loss: 0.60995 - acc: 0.8182 -- iter: 4000/8000\n",
            "Training Step: 8922  | total loss: \u001b[1m\u001b[32m0.60977\u001b[0m\u001b[0m | time: 3.171s\n",
            "| SGD | epoch: 764 | loss: 0.60977 - acc: 0.8192 -- iter: 5000/8000\n",
            "Training Step: 8923  | total loss: \u001b[1m\u001b[32m0.60651\u001b[0m\u001b[0m | time: 3.802s\n",
            "| SGD | epoch: 764 | loss: 0.60651 - acc: 0.8201 -- iter: 6000/8000\n",
            "Training Step: 8924  | total loss: \u001b[1m\u001b[32m0.60728\u001b[0m\u001b[0m | time: 4.434s\n",
            "| SGD | epoch: 764 | loss: 0.60728 - acc: 0.8196 -- iter: 7000/8000\n",
            "Training Step: 8925  | total loss: \u001b[1m\u001b[32m0.60452\u001b[0m\u001b[0m | time: 5.068s\n",
            "| SGD | epoch: 764 | loss: 0.60452 - acc: 0.8215 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 8926  | total loss: \u001b[1m\u001b[32m0.60531\u001b[0m\u001b[0m | time: 0.631s\n",
            "| SGD | epoch: 765 | loss: 0.60531 - acc: 0.8214 -- iter: 1000/8000\n",
            "Training Step: 8927  | total loss: \u001b[1m\u001b[32m0.60536\u001b[0m\u001b[0m | time: 1.261s\n",
            "| SGD | epoch: 765 | loss: 0.60536 - acc: 0.8204 -- iter: 2000/8000\n",
            "Training Step: 8928  | total loss: \u001b[1m\u001b[32m0.60232\u001b[0m\u001b[0m | time: 1.899s\n",
            "| SGD | epoch: 765 | loss: 0.60232 - acc: 0.8225 -- iter: 3000/8000\n",
            "Training Step: 8929  | total loss: \u001b[1m\u001b[32m0.60321\u001b[0m\u001b[0m | time: 2.532s\n",
            "| SGD | epoch: 765 | loss: 0.60321 - acc: 0.8232 -- iter: 4000/8000\n",
            "Training Step: 8930  | total loss: \u001b[1m\u001b[32m0.60276\u001b[0m\u001b[0m | time: 3.161s\n",
            "| SGD | epoch: 765 | loss: 0.60276 - acc: 0.8221 -- iter: 5000/8000\n",
            "Training Step: 8931  | total loss: \u001b[1m\u001b[32m0.60143\u001b[0m\u001b[0m | time: 3.792s\n",
            "| SGD | epoch: 765 | loss: 0.60143 - acc: 0.8224 -- iter: 6000/8000\n",
            "Training Step: 8932  | total loss: \u001b[1m\u001b[32m0.60512\u001b[0m\u001b[0m | time: 4.424s\n",
            "| SGD | epoch: 765 | loss: 0.60512 - acc: 0.8209 -- iter: 7000/8000\n",
            "Training Step: 8933  | total loss: \u001b[1m\u001b[32m0.60801\u001b[0m\u001b[0m | time: 5.064s\n",
            "| SGD | epoch: 765 | loss: 0.60801 - acc: 0.8195 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 8934  | total loss: \u001b[1m\u001b[32m0.61144\u001b[0m\u001b[0m | time: 0.636s\n",
            "| SGD | epoch: 766 | loss: 0.61144 - acc: 0.8174 -- iter: 1000/8000\n",
            "Training Step: 8935  | total loss: \u001b[1m\u001b[32m0.61057\u001b[0m\u001b[0m | time: 1.281s\n",
            "| SGD | epoch: 766 | loss: 0.61057 - acc: 0.8170 -- iter: 2000/8000\n",
            "Training Step: 8936  | total loss: \u001b[1m\u001b[32m0.60954\u001b[0m\u001b[0m | time: 1.926s\n",
            "| SGD | epoch: 766 | loss: 0.60954 - acc: 0.8160 -- iter: 3000/8000\n",
            "Training Step: 8937  | total loss: \u001b[1m\u001b[32m0.60537\u001b[0m\u001b[0m | time: 2.572s\n",
            "| SGD | epoch: 766 | loss: 0.60537 - acc: 0.8187 -- iter: 4000/8000\n",
            "Training Step: 8938  | total loss: \u001b[1m\u001b[32m0.60605\u001b[0m\u001b[0m | time: 3.214s\n",
            "| SGD | epoch: 766 | loss: 0.60605 - acc: 0.8201 -- iter: 5000/8000\n",
            "Training Step: 8939  | total loss: \u001b[1m\u001b[32m0.60729\u001b[0m\u001b[0m | time: 3.857s\n",
            "| SGD | epoch: 766 | loss: 0.60729 - acc: 0.8207 -- iter: 6000/8000\n",
            "Training Step: 8940  | total loss: \u001b[1m\u001b[32m0.60839\u001b[0m\u001b[0m | time: 4.496s\n",
            "| SGD | epoch: 766 | loss: 0.60839 - acc: 0.8201 -- iter: 7000/8000\n",
            "Training Step: 8941  | total loss: \u001b[1m\u001b[32m0.60860\u001b[0m\u001b[0m | time: 5.134s\n",
            "| SGD | epoch: 766 | loss: 0.60860 - acc: 0.8204 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 8942  | total loss: \u001b[1m\u001b[32m0.60699\u001b[0m\u001b[0m | time: 0.639s\n",
            "| SGD | epoch: 767 | loss: 0.60699 - acc: 0.8219 -- iter: 1000/8000\n",
            "Training Step: 8943  | total loss: \u001b[1m\u001b[32m0.60741\u001b[0m\u001b[0m | time: 1.282s\n",
            "| SGD | epoch: 767 | loss: 0.60741 - acc: 0.8220 -- iter: 2000/8000\n",
            "Training Step: 8944  | total loss: \u001b[1m\u001b[32m0.61115\u001b[0m\u001b[0m | time: 1.927s\n",
            "| SGD | epoch: 767 | loss: 0.61115 - acc: 0.8214 -- iter: 3000/8000\n",
            "Training Step: 8945  | total loss: \u001b[1m\u001b[32m0.61148\u001b[0m\u001b[0m | time: 2.551s\n",
            "| SGD | epoch: 767 | loss: 0.61148 - acc: 0.8217 -- iter: 4000/8000\n",
            "Training Step: 8946  | total loss: \u001b[1m\u001b[32m0.60947\u001b[0m\u001b[0m | time: 3.177s\n",
            "| SGD | epoch: 767 | loss: 0.60947 - acc: 0.8229 -- iter: 5000/8000\n",
            "Training Step: 8947  | total loss: \u001b[1m\u001b[32m0.60830\u001b[0m\u001b[0m | time: 3.807s\n",
            "| SGD | epoch: 767 | loss: 0.60830 - acc: 0.8224 -- iter: 6000/8000\n",
            "Training Step: 8948  | total loss: \u001b[1m\u001b[32m0.60535\u001b[0m\u001b[0m | time: 4.431s\n",
            "| SGD | epoch: 767 | loss: 0.60535 - acc: 0.8234 -- iter: 7000/8000\n",
            "Training Step: 8949  | total loss: \u001b[1m\u001b[32m0.60137\u001b[0m\u001b[0m | time: 5.052s\n",
            "| SGD | epoch: 767 | loss: 0.60137 - acc: 0.8257 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 8950  | total loss: \u001b[1m\u001b[32m0.59909\u001b[0m\u001b[0m | time: 0.620s\n",
            "| SGD | epoch: 768 | loss: 0.59909 - acc: 0.8273 -- iter: 1000/8000\n",
            "Training Step: 8951  | total loss: \u001b[1m\u001b[32m0.60164\u001b[0m\u001b[0m | time: 1.239s\n",
            "| SGD | epoch: 768 | loss: 0.60164 - acc: 0.8264 -- iter: 2000/8000\n",
            "Training Step: 8952  | total loss: \u001b[1m\u001b[32m0.60258\u001b[0m\u001b[0m | time: 1.859s\n",
            "| SGD | epoch: 768 | loss: 0.60258 - acc: 0.8263 -- iter: 3000/8000\n",
            "Training Step: 8953  | total loss: \u001b[1m\u001b[32m0.60067\u001b[0m\u001b[0m | time: 2.483s\n",
            "| SGD | epoch: 768 | loss: 0.60067 - acc: 0.8269 -- iter: 4000/8000\n",
            "Training Step: 8954  | total loss: \u001b[1m\u001b[32m0.60053\u001b[0m\u001b[0m | time: 3.099s\n",
            "| SGD | epoch: 768 | loss: 0.60053 - acc: 0.8262 -- iter: 5000/8000\n",
            "Training Step: 8955  | total loss: \u001b[1m\u001b[32m0.60278\u001b[0m\u001b[0m | time: 3.729s\n",
            "| SGD | epoch: 768 | loss: 0.60278 - acc: 0.8248 -- iter: 6000/8000\n",
            "Training Step: 8956  | total loss: \u001b[1m\u001b[32m0.60316\u001b[0m\u001b[0m | time: 4.357s\n",
            "| SGD | epoch: 768 | loss: 0.60316 - acc: 0.8240 -- iter: 7000/8000\n",
            "Training Step: 8957  | total loss: \u001b[1m\u001b[32m0.60205\u001b[0m\u001b[0m | time: 4.914s\n",
            "| SGD | epoch: 768 | loss: 0.60205 - acc: 0.8249 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 8958  | total loss: \u001b[1m\u001b[32m0.60078\u001b[0m\u001b[0m | time: 0.557s\n",
            "| SGD | epoch: 769 | loss: 0.60078 - acc: 0.8251 -- iter: 1000/8000\n",
            "Training Step: 8959  | total loss: \u001b[1m\u001b[32m0.60169\u001b[0m\u001b[0m | time: 1.114s\n",
            "| SGD | epoch: 769 | loss: 0.60169 - acc: 0.8257 -- iter: 2000/8000\n",
            "Training Step: 8960  | total loss: \u001b[1m\u001b[32m0.60227\u001b[0m\u001b[0m | time: 1.635s\n",
            "| SGD | epoch: 769 | loss: 0.60227 - acc: 0.8254 -- iter: 3000/8000\n",
            "Training Step: 8961  | total loss: \u001b[1m\u001b[32m0.60268\u001b[0m\u001b[0m | time: 2.155s\n",
            "| SGD | epoch: 769 | loss: 0.60268 - acc: 0.8250 -- iter: 4000/8000\n",
            "Training Step: 8962  | total loss: \u001b[1m\u001b[32m0.60169\u001b[0m\u001b[0m | time: 2.670s\n",
            "| SGD | epoch: 769 | loss: 0.60169 - acc: 0.8248 -- iter: 5000/8000\n",
            "Training Step: 8963  | total loss: \u001b[1m\u001b[32m0.60211\u001b[0m\u001b[0m | time: 3.185s\n",
            "| SGD | epoch: 769 | loss: 0.60211 - acc: 0.8262 -- iter: 6000/8000\n",
            "Training Step: 8964  | total loss: \u001b[1m\u001b[32m0.60391\u001b[0m\u001b[0m | time: 3.694s\n",
            "| SGD | epoch: 769 | loss: 0.60391 - acc: 0.8256 -- iter: 7000/8000\n",
            "Training Step: 8965  | total loss: \u001b[1m\u001b[32m0.60514\u001b[0m\u001b[0m | time: 4.234s\n",
            "| SGD | epoch: 769 | loss: 0.60514 - acc: 0.8243 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 8966  | total loss: \u001b[1m\u001b[32m0.60322\u001b[0m\u001b[0m | time: 0.560s\n",
            "| SGD | epoch: 770 | loss: 0.60322 - acc: 0.8251 -- iter: 1000/8000\n",
            "Training Step: 8967  | total loss: \u001b[1m\u001b[32m0.60154\u001b[0m\u001b[0m | time: 1.210s\n",
            "| SGD | epoch: 770 | loss: 0.60154 - acc: 0.8247 -- iter: 2000/8000\n",
            "Training Step: 8968  | total loss: \u001b[1m\u001b[32m0.60221\u001b[0m\u001b[0m | time: 1.754s\n",
            "| SGD | epoch: 770 | loss: 0.60221 - acc: 0.8231 -- iter: 3000/8000\n",
            "Training Step: 8969  | total loss: \u001b[1m\u001b[32m0.60241\u001b[0m\u001b[0m | time: 2.301s\n",
            "| SGD | epoch: 770 | loss: 0.60241 - acc: 0.8222 -- iter: 4000/8000\n",
            "Training Step: 8970  | total loss: \u001b[1m\u001b[32m0.60165\u001b[0m\u001b[0m | time: 2.841s\n",
            "| SGD | epoch: 770 | loss: 0.60165 - acc: 0.8241 -- iter: 5000/8000\n",
            "Training Step: 8971  | total loss: \u001b[1m\u001b[32m0.60454\u001b[0m\u001b[0m | time: 3.400s\n",
            "| SGD | epoch: 770 | loss: 0.60454 - acc: 0.8229 -- iter: 6000/8000\n",
            "Training Step: 8972  | total loss: \u001b[1m\u001b[32m0.60482\u001b[0m\u001b[0m | time: 3.957s\n",
            "| SGD | epoch: 770 | loss: 0.60482 - acc: 0.8216 -- iter: 7000/8000\n",
            "Training Step: 8973  | total loss: \u001b[1m\u001b[32m0.60532\u001b[0m\u001b[0m | time: 4.518s\n",
            "| SGD | epoch: 770 | loss: 0.60532 - acc: 0.8223 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 8974  | total loss: \u001b[1m\u001b[32m0.60217\u001b[0m\u001b[0m | time: 0.565s\n",
            "| SGD | epoch: 771 | loss: 0.60217 - acc: 0.8232 -- iter: 1000/8000\n",
            "Training Step: 8975  | total loss: \u001b[1m\u001b[32m0.60074\u001b[0m\u001b[0m | time: 1.131s\n",
            "| SGD | epoch: 771 | loss: 0.60074 - acc: 0.8244 -- iter: 2000/8000\n",
            "Training Step: 8976  | total loss: \u001b[1m\u001b[32m0.59899\u001b[0m\u001b[0m | time: 1.690s\n",
            "| SGD | epoch: 771 | loss: 0.59899 - acc: 0.8257 -- iter: 3000/8000\n",
            "Training Step: 8977  | total loss: \u001b[1m\u001b[32m0.60070\u001b[0m\u001b[0m | time: 2.248s\n",
            "| SGD | epoch: 771 | loss: 0.60070 - acc: 0.8246 -- iter: 4000/8000\n",
            "Training Step: 8978  | total loss: \u001b[1m\u001b[32m0.59924\u001b[0m\u001b[0m | time: 2.824s\n",
            "| SGD | epoch: 771 | loss: 0.59924 - acc: 0.8259 -- iter: 5000/8000\n",
            "Training Step: 8979  | total loss: \u001b[1m\u001b[32m0.59928\u001b[0m\u001b[0m | time: 3.387s\n",
            "| SGD | epoch: 771 | loss: 0.59928 - acc: 0.8263 -- iter: 6000/8000\n",
            "Training Step: 8980  | total loss: \u001b[1m\u001b[32m0.60035\u001b[0m\u001b[0m | time: 3.959s\n",
            "| SGD | epoch: 771 | loss: 0.60035 - acc: 0.8269 -- iter: 7000/8000\n",
            "Training Step: 8981  | total loss: \u001b[1m\u001b[32m0.59945\u001b[0m\u001b[0m | time: 4.531s\n",
            "| SGD | epoch: 771 | loss: 0.59945 - acc: 0.8268 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 8982  | total loss: \u001b[1m\u001b[32m0.59970\u001b[0m\u001b[0m | time: 0.582s\n",
            "| SGD | epoch: 772 | loss: 0.59970 - acc: 0.8265 -- iter: 1000/8000\n",
            "Training Step: 8983  | total loss: \u001b[1m\u001b[32m0.59985\u001b[0m\u001b[0m | time: 1.149s\n",
            "| SGD | epoch: 772 | loss: 0.59985 - acc: 0.8261 -- iter: 2000/8000\n",
            "Training Step: 8984  | total loss: \u001b[1m\u001b[32m0.60521\u001b[0m\u001b[0m | time: 1.804s\n",
            "| SGD | epoch: 772 | loss: 0.60521 - acc: 0.8246 -- iter: 3000/8000\n",
            "Training Step: 8985  | total loss: \u001b[1m\u001b[32m0.60891\u001b[0m\u001b[0m | time: 2.434s\n",
            "| SGD | epoch: 772 | loss: 0.60891 - acc: 0.8241 -- iter: 4000/8000\n",
            "Training Step: 8986  | total loss: \u001b[1m\u001b[32m0.60988\u001b[0m\u001b[0m | time: 3.060s\n",
            "| SGD | epoch: 772 | loss: 0.60988 - acc: 0.8235 -- iter: 5000/8000\n",
            "Training Step: 8987  | total loss: \u001b[1m\u001b[32m0.60540\u001b[0m\u001b[0m | time: 3.695s\n",
            "| SGD | epoch: 772 | loss: 0.60540 - acc: 0.8252 -- iter: 6000/8000\n",
            "Training Step: 8988  | total loss: \u001b[1m\u001b[32m0.60383\u001b[0m\u001b[0m | time: 4.323s\n",
            "| SGD | epoch: 772 | loss: 0.60383 - acc: 0.8251 -- iter: 7000/8000\n",
            "Training Step: 8989  | total loss: \u001b[1m\u001b[32m0.60306\u001b[0m\u001b[0m | time: 4.935s\n",
            "| SGD | epoch: 772 | loss: 0.60306 - acc: 0.8250 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 8990  | total loss: \u001b[1m\u001b[32m0.60398\u001b[0m\u001b[0m | time: 0.634s\n",
            "| SGD | epoch: 773 | loss: 0.60398 - acc: 0.8247 -- iter: 1000/8000\n",
            "Training Step: 8991  | total loss: \u001b[1m\u001b[32m0.60310\u001b[0m\u001b[0m | time: 1.266s\n",
            "| SGD | epoch: 773 | loss: 0.60310 - acc: 0.8257 -- iter: 2000/8000\n",
            "Training Step: 8992  | total loss: \u001b[1m\u001b[32m0.60577\u001b[0m\u001b[0m | time: 1.899s\n",
            "| SGD | epoch: 773 | loss: 0.60577 - acc: 0.8233 -- iter: 3000/8000\n",
            "Training Step: 8993  | total loss: \u001b[1m\u001b[32m0.60211\u001b[0m\u001b[0m | time: 2.534s\n",
            "| SGD | epoch: 773 | loss: 0.60211 - acc: 0.8239 -- iter: 4000/8000\n",
            "Training Step: 8994  | total loss: \u001b[1m\u001b[32m0.59745\u001b[0m\u001b[0m | time: 3.167s\n",
            "| SGD | epoch: 773 | loss: 0.59745 - acc: 0.8256 -- iter: 5000/8000\n",
            "Training Step: 8995  | total loss: \u001b[1m\u001b[32m0.60016\u001b[0m\u001b[0m | time: 3.802s\n",
            "| SGD | epoch: 773 | loss: 0.60016 - acc: 0.8239 -- iter: 6000/8000\n",
            "Training Step: 8996  | total loss: \u001b[1m\u001b[32m0.60171\u001b[0m\u001b[0m | time: 4.432s\n",
            "| SGD | epoch: 773 | loss: 0.60171 - acc: 0.8237 -- iter: 7000/8000\n",
            "Training Step: 8997  | total loss: \u001b[1m\u001b[32m0.60157\u001b[0m\u001b[0m | time: 5.061s\n",
            "| SGD | epoch: 773 | loss: 0.60157 - acc: 0.8241 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 8998  | total loss: \u001b[1m\u001b[32m0.60202\u001b[0m\u001b[0m | time: 0.633s\n",
            "| SGD | epoch: 774 | loss: 0.60202 - acc: 0.8227 -- iter: 1000/8000\n",
            "Training Step: 8999  | total loss: \u001b[1m\u001b[32m0.60166\u001b[0m\u001b[0m | time: 1.262s\n",
            "| SGD | epoch: 774 | loss: 0.60166 - acc: 0.8226 -- iter: 2000/8000\n",
            "Training Step: 9000  | total loss: \u001b[1m\u001b[32m0.59799\u001b[0m\u001b[0m | time: 1.897s\n",
            "| SGD | epoch: 774 | loss: 0.59799 - acc: 0.8249 -- iter: 3000/8000\n",
            "Training Step: 9001  | total loss: \u001b[1m\u001b[32m0.59727\u001b[0m\u001b[0m | time: 2.530s\n",
            "| SGD | epoch: 774 | loss: 0.59727 - acc: 0.8255 -- iter: 4000/8000\n",
            "Training Step: 9002  | total loss: \u001b[1m\u001b[32m0.60021\u001b[0m\u001b[0m | time: 3.168s\n",
            "| SGD | epoch: 774 | loss: 0.60021 - acc: 0.8267 -- iter: 5000/8000\n",
            "Training Step: 9003  | total loss: \u001b[1m\u001b[32m0.60301\u001b[0m\u001b[0m | time: 3.808s\n",
            "| SGD | epoch: 774 | loss: 0.60301 - acc: 0.8261 -- iter: 6000/8000\n",
            "Training Step: 9004  | total loss: \u001b[1m\u001b[32m0.60122\u001b[0m\u001b[0m | time: 4.451s\n",
            "| SGD | epoch: 774 | loss: 0.60122 - acc: 0.8267 -- iter: 7000/8000\n",
            "Training Step: 9005  | total loss: \u001b[1m\u001b[32m0.60456\u001b[0m\u001b[0m | time: 5.077s\n",
            "| SGD | epoch: 774 | loss: 0.60456 - acc: 0.8238 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9006  | total loss: \u001b[1m\u001b[32m0.60489\u001b[0m\u001b[0m | time: 0.626s\n",
            "| SGD | epoch: 775 | loss: 0.60489 - acc: 0.8246 -- iter: 1000/8000\n",
            "Training Step: 9007  | total loss: \u001b[1m\u001b[32m0.60313\u001b[0m\u001b[0m | time: 1.254s\n",
            "| SGD | epoch: 775 | loss: 0.60313 - acc: 0.8261 -- iter: 2000/8000\n",
            "Training Step: 9008  | total loss: \u001b[1m\u001b[32m0.60311\u001b[0m\u001b[0m | time: 1.876s\n",
            "| SGD | epoch: 775 | loss: 0.60311 - acc: 0.8255 -- iter: 3000/8000\n",
            "Training Step: 9009  | total loss: \u001b[1m\u001b[32m0.59871\u001b[0m\u001b[0m | time: 2.499s\n",
            "| SGD | epoch: 775 | loss: 0.59871 - acc: 0.8270 -- iter: 4000/8000\n",
            "Training Step: 9010  | total loss: \u001b[1m\u001b[32m0.59932\u001b[0m\u001b[0m | time: 3.120s\n",
            "| SGD | epoch: 775 | loss: 0.59932 - acc: 0.8265 -- iter: 5000/8000\n",
            "Training Step: 9011  | total loss: \u001b[1m\u001b[32m0.60109\u001b[0m\u001b[0m | time: 3.736s\n",
            "| SGD | epoch: 775 | loss: 0.60109 - acc: 0.8262 -- iter: 6000/8000\n",
            "Training Step: 9012  | total loss: \u001b[1m\u001b[32m0.60211\u001b[0m\u001b[0m | time: 4.356s\n",
            "| SGD | epoch: 775 | loss: 0.60211 - acc: 0.8265 -- iter: 7000/8000\n",
            "Training Step: 9013  | total loss: \u001b[1m\u001b[32m0.60515\u001b[0m\u001b[0m | time: 4.969s\n",
            "| SGD | epoch: 775 | loss: 0.60515 - acc: 0.8252 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9014  | total loss: \u001b[1m\u001b[32m0.60306\u001b[0m\u001b[0m | time: 0.614s\n",
            "| SGD | epoch: 776 | loss: 0.60306 - acc: 0.8252 -- iter: 1000/8000\n",
            "Training Step: 9015  | total loss: \u001b[1m\u001b[32m0.60196\u001b[0m\u001b[0m | time: 1.230s\n",
            "| SGD | epoch: 776 | loss: 0.60196 - acc: 0.8250 -- iter: 2000/8000\n",
            "Training Step: 9016  | total loss: \u001b[1m\u001b[32m0.60263\u001b[0m\u001b[0m | time: 1.847s\n",
            "| SGD | epoch: 776 | loss: 0.60263 - acc: 0.8238 -- iter: 3000/8000\n",
            "Training Step: 9017  | total loss: \u001b[1m\u001b[32m0.60629\u001b[0m\u001b[0m | time: 2.459s\n",
            "| SGD | epoch: 776 | loss: 0.60629 - acc: 0.8223 -- iter: 4000/8000\n",
            "Training Step: 9018  | total loss: \u001b[1m\u001b[32m0.60393\u001b[0m\u001b[0m | time: 3.072s\n",
            "| SGD | epoch: 776 | loss: 0.60393 - acc: 0.8235 -- iter: 5000/8000\n",
            "Training Step: 9019  | total loss: \u001b[1m\u001b[32m0.60439\u001b[0m\u001b[0m | time: 3.692s\n",
            "| SGD | epoch: 776 | loss: 0.60439 - acc: 0.8242 -- iter: 6000/8000\n",
            "Training Step: 9020  | total loss: \u001b[1m\u001b[32m0.60393\u001b[0m\u001b[0m | time: 4.307s\n",
            "| SGD | epoch: 776 | loss: 0.60393 - acc: 0.8238 -- iter: 7000/8000\n",
            "Training Step: 9021  | total loss: \u001b[1m\u001b[32m0.60234\u001b[0m\u001b[0m | time: 4.923s\n",
            "| SGD | epoch: 776 | loss: 0.60234 - acc: 0.8257 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9022  | total loss: \u001b[1m\u001b[32m0.60054\u001b[0m\u001b[0m | time: 0.615s\n",
            "| SGD | epoch: 777 | loss: 0.60054 - acc: 0.8257 -- iter: 1000/8000\n",
            "Training Step: 9023  | total loss: \u001b[1m\u001b[32m0.60003\u001b[0m\u001b[0m | time: 1.235s\n",
            "| SGD | epoch: 777 | loss: 0.60003 - acc: 0.8276 -- iter: 2000/8000\n",
            "Training Step: 9024  | total loss: \u001b[1m\u001b[32m0.59992\u001b[0m\u001b[0m | time: 1.856s\n",
            "| SGD | epoch: 777 | loss: 0.59992 - acc: 0.8279 -- iter: 3000/8000\n",
            "Training Step: 9025  | total loss: \u001b[1m\u001b[32m0.60127\u001b[0m\u001b[0m | time: 2.482s\n",
            "| SGD | epoch: 777 | loss: 0.60127 - acc: 0.8269 -- iter: 4000/8000\n",
            "Training Step: 9026  | total loss: \u001b[1m\u001b[32m0.60206\u001b[0m\u001b[0m | time: 3.109s\n",
            "| SGD | epoch: 777 | loss: 0.60206 - acc: 0.8260 -- iter: 5000/8000\n",
            "Training Step: 9027  | total loss: \u001b[1m\u001b[32m0.60202\u001b[0m\u001b[0m | time: 3.734s\n",
            "| SGD | epoch: 777 | loss: 0.60202 - acc: 0.8251 -- iter: 6000/8000\n",
            "Training Step: 9028  | total loss: \u001b[1m\u001b[32m0.59929\u001b[0m\u001b[0m | time: 4.361s\n",
            "| SGD | epoch: 777 | loss: 0.59929 - acc: 0.8250 -- iter: 7000/8000\n",
            "Training Step: 9029  | total loss: \u001b[1m\u001b[32m0.59751\u001b[0m\u001b[0m | time: 5.035s\n",
            "| SGD | epoch: 777 | loss: 0.59751 - acc: 0.8274 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9030  | total loss: \u001b[1m\u001b[32m0.59570\u001b[0m\u001b[0m | time: 0.626s\n",
            "| SGD | epoch: 778 | loss: 0.59570 - acc: 0.8294 -- iter: 1000/8000\n",
            "Training Step: 9031  | total loss: \u001b[1m\u001b[32m0.59467\u001b[0m\u001b[0m | time: 1.240s\n",
            "| SGD | epoch: 778 | loss: 0.59467 - acc: 0.8296 -- iter: 2000/8000\n",
            "Training Step: 9032  | total loss: \u001b[1m\u001b[32m0.59667\u001b[0m\u001b[0m | time: 1.872s\n",
            "| SGD | epoch: 778 | loss: 0.59667 - acc: 0.8282 -- iter: 3000/8000\n",
            "Training Step: 9033  | total loss: \u001b[1m\u001b[32m0.59697\u001b[0m\u001b[0m | time: 2.502s\n",
            "| SGD | epoch: 778 | loss: 0.59697 - acc: 0.8270 -- iter: 4000/8000\n",
            "Training Step: 9034  | total loss: \u001b[1m\u001b[32m0.59603\u001b[0m\u001b[0m | time: 3.142s\n",
            "| SGD | epoch: 778 | loss: 0.59603 - acc: 0.8261 -- iter: 5000/8000\n",
            "Training Step: 9035  | total loss: \u001b[1m\u001b[32m0.59521\u001b[0m\u001b[0m | time: 3.772s\n",
            "| SGD | epoch: 778 | loss: 0.59521 - acc: 0.8278 -- iter: 6000/8000\n",
            "Training Step: 9036  | total loss: \u001b[1m\u001b[32m0.59761\u001b[0m\u001b[0m | time: 4.404s\n",
            "| SGD | epoch: 778 | loss: 0.59761 - acc: 0.8258 -- iter: 7000/8000\n",
            "Training Step: 9037  | total loss: \u001b[1m\u001b[32m0.59680\u001b[0m\u001b[0m | time: 5.036s\n",
            "| SGD | epoch: 778 | loss: 0.59680 - acc: 0.8253 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9038  | total loss: \u001b[1m\u001b[32m0.59730\u001b[0m\u001b[0m | time: 0.632s\n",
            "| SGD | epoch: 779 | loss: 0.59730 - acc: 0.8247 -- iter: 1000/8000\n",
            "Training Step: 9039  | total loss: \u001b[1m\u001b[32m0.59609\u001b[0m\u001b[0m | time: 1.265s\n",
            "| SGD | epoch: 779 | loss: 0.59609 - acc: 0.8250 -- iter: 2000/8000\n",
            "Training Step: 9040  | total loss: \u001b[1m\u001b[32m0.59660\u001b[0m\u001b[0m | time: 1.899s\n",
            "| SGD | epoch: 779 | loss: 0.59660 - acc: 0.8263 -- iter: 3000/8000\n",
            "Training Step: 9041  | total loss: \u001b[1m\u001b[32m0.59798\u001b[0m\u001b[0m | time: 2.531s\n",
            "| SGD | epoch: 779 | loss: 0.59798 - acc: 0.8265 -- iter: 4000/8000\n",
            "Training Step: 9042  | total loss: \u001b[1m\u001b[32m0.59750\u001b[0m\u001b[0m | time: 3.147s\n",
            "| SGD | epoch: 779 | loss: 0.59750 - acc: 0.8272 -- iter: 5000/8000\n",
            "Training Step: 9043  | total loss: \u001b[1m\u001b[32m0.60176\u001b[0m\u001b[0m | time: 3.762s\n",
            "| SGD | epoch: 779 | loss: 0.60176 - acc: 0.8262 -- iter: 6000/8000\n",
            "Training Step: 9044  | total loss: \u001b[1m\u001b[32m0.60047\u001b[0m\u001b[0m | time: 4.379s\n",
            "| SGD | epoch: 779 | loss: 0.60047 - acc: 0.8257 -- iter: 7000/8000\n",
            "Training Step: 9045  | total loss: \u001b[1m\u001b[32m0.60034\u001b[0m\u001b[0m | time: 4.997s\n",
            "| SGD | epoch: 779 | loss: 0.60034 - acc: 0.8265 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9046  | total loss: \u001b[1m\u001b[32m0.59597\u001b[0m\u001b[0m | time: 0.617s\n",
            "| SGD | epoch: 780 | loss: 0.59597 - acc: 0.8296 -- iter: 1000/8000\n",
            "Training Step: 9047  | total loss: \u001b[1m\u001b[32m0.59867\u001b[0m\u001b[0m | time: 1.247s\n",
            "| SGD | epoch: 780 | loss: 0.59867 - acc: 0.8274 -- iter: 2000/8000\n",
            "Training Step: 9048  | total loss: \u001b[1m\u001b[32m0.60132\u001b[0m\u001b[0m | time: 1.809s\n",
            "| SGD | epoch: 780 | loss: 0.60132 - acc: 0.8250 -- iter: 3000/8000\n",
            "Training Step: 9049  | total loss: \u001b[1m\u001b[32m0.60106\u001b[0m\u001b[0m | time: 2.371s\n",
            "| SGD | epoch: 780 | loss: 0.60106 - acc: 0.8257 -- iter: 4000/8000\n",
            "Training Step: 9050  | total loss: \u001b[1m\u001b[32m0.59834\u001b[0m\u001b[0m | time: 2.934s\n",
            "| SGD | epoch: 780 | loss: 0.59834 - acc: 0.8248 -- iter: 5000/8000\n",
            "Training Step: 9051  | total loss: \u001b[1m\u001b[32m0.59628\u001b[0m\u001b[0m | time: 3.497s\n",
            "| SGD | epoch: 780 | loss: 0.59628 - acc: 0.8271 -- iter: 6000/8000\n",
            "Training Step: 9052  | total loss: \u001b[1m\u001b[32m0.59850\u001b[0m\u001b[0m | time: 4.053s\n",
            "| SGD | epoch: 780 | loss: 0.59850 - acc: 0.8263 -- iter: 7000/8000\n",
            "Training Step: 9053  | total loss: \u001b[1m\u001b[32m0.59827\u001b[0m\u001b[0m | time: 4.605s\n",
            "| SGD | epoch: 780 | loss: 0.59827 - acc: 0.8256 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9054  | total loss: \u001b[1m\u001b[32m0.59923\u001b[0m\u001b[0m | time: 0.554s\n",
            "| SGD | epoch: 781 | loss: 0.59923 - acc: 0.8253 -- iter: 1000/8000\n",
            "Training Step: 9055  | total loss: \u001b[1m\u001b[32m0.59764\u001b[0m\u001b[0m | time: 1.107s\n",
            "| SGD | epoch: 781 | loss: 0.59764 - acc: 0.8273 -- iter: 2000/8000\n",
            "Training Step: 9056  | total loss: \u001b[1m\u001b[32m0.59780\u001b[0m\u001b[0m | time: 1.650s\n",
            "| SGD | epoch: 781 | loss: 0.59780 - acc: 0.8262 -- iter: 3000/8000\n",
            "Training Step: 9057  | total loss: \u001b[1m\u001b[32m0.59763\u001b[0m\u001b[0m | time: 2.190s\n",
            "| SGD | epoch: 781 | loss: 0.59763 - acc: 0.8258 -- iter: 4000/8000\n",
            "Training Step: 9058  | total loss: \u001b[1m\u001b[32m0.59779\u001b[0m\u001b[0m | time: 2.732s\n",
            "| SGD | epoch: 781 | loss: 0.59779 - acc: 0.8263 -- iter: 5000/8000\n",
            "Training Step: 9059  | total loss: \u001b[1m\u001b[32m0.59805\u001b[0m\u001b[0m | time: 3.279s\n",
            "| SGD | epoch: 781 | loss: 0.59805 - acc: 0.8251 -- iter: 6000/8000\n",
            "Training Step: 9060  | total loss: \u001b[1m\u001b[32m0.60205\u001b[0m\u001b[0m | time: 3.821s\n",
            "| SGD | epoch: 781 | loss: 0.60205 - acc: 0.8240 -- iter: 7000/8000\n",
            "Training Step: 9061  | total loss: \u001b[1m\u001b[32m0.59759\u001b[0m\u001b[0m | time: 4.373s\n",
            "| SGD | epoch: 781 | loss: 0.59759 - acc: 0.8247 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9062  | total loss: \u001b[1m\u001b[32m0.59527\u001b[0m\u001b[0m | time: 0.539s\n",
            "| SGD | epoch: 782 | loss: 0.59527 - acc: 0.8273 -- iter: 1000/8000\n",
            "Training Step: 9063  | total loss: \u001b[1m\u001b[32m0.59801\u001b[0m\u001b[0m | time: 1.093s\n",
            "| SGD | epoch: 782 | loss: 0.59801 - acc: 0.8269 -- iter: 2000/8000\n",
            "Training Step: 9064  | total loss: \u001b[1m\u001b[32m0.59827\u001b[0m\u001b[0m | time: 1.630s\n",
            "| SGD | epoch: 782 | loss: 0.59827 - acc: 0.8272 -- iter: 3000/8000\n",
            "Training Step: 9065  | total loss: \u001b[1m\u001b[32m0.59230\u001b[0m\u001b[0m | time: 2.198s\n",
            "| SGD | epoch: 782 | loss: 0.59230 - acc: 0.8306 -- iter: 4000/8000\n",
            "Training Step: 9066  | total loss: \u001b[1m\u001b[32m0.58689\u001b[0m\u001b[0m | time: 2.767s\n",
            "| SGD | epoch: 782 | loss: 0.58689 - acc: 0.8329 -- iter: 5000/8000\n",
            "Training Step: 9067  | total loss: \u001b[1m\u001b[32m0.58775\u001b[0m\u001b[0m | time: 3.340s\n",
            "| SGD | epoch: 782 | loss: 0.58775 - acc: 0.8312 -- iter: 6000/8000\n",
            "Training Step: 9068  | total loss: \u001b[1m\u001b[32m0.59212\u001b[0m\u001b[0m | time: 3.913s\n",
            "| SGD | epoch: 782 | loss: 0.59212 - acc: 0.8285 -- iter: 7000/8000\n",
            "Training Step: 9069  | total loss: \u001b[1m\u001b[32m0.58976\u001b[0m\u001b[0m | time: 4.486s\n",
            "| SGD | epoch: 782 | loss: 0.58976 - acc: 0.8287 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9070  | total loss: \u001b[1m\u001b[32m0.58863\u001b[0m\u001b[0m | time: 0.570s\n",
            "| SGD | epoch: 783 | loss: 0.58863 - acc: 0.8314 -- iter: 1000/8000\n",
            "Training Step: 9071  | total loss: \u001b[1m\u001b[32m0.59043\u001b[0m\u001b[0m | time: 1.142s\n",
            "| SGD | epoch: 783 | loss: 0.59043 - acc: 0.8300 -- iter: 2000/8000\n",
            "Training Step: 9072  | total loss: \u001b[1m\u001b[32m0.59302\u001b[0m\u001b[0m | time: 1.725s\n",
            "| SGD | epoch: 783 | loss: 0.59302 - acc: 0.8285 -- iter: 3000/8000\n",
            "Training Step: 9073  | total loss: \u001b[1m\u001b[32m0.59004\u001b[0m\u001b[0m | time: 2.299s\n",
            "| SGD | epoch: 783 | loss: 0.59004 - acc: 0.8294 -- iter: 4000/8000\n",
            "Training Step: 9074  | total loss: \u001b[1m\u001b[32m0.59246\u001b[0m\u001b[0m | time: 2.876s\n",
            "| SGD | epoch: 783 | loss: 0.59246 - acc: 0.8291 -- iter: 5000/8000\n",
            "Training Step: 9075  | total loss: \u001b[1m\u001b[32m0.59414\u001b[0m\u001b[0m | time: 3.450s\n",
            "| SGD | epoch: 783 | loss: 0.59414 - acc: 0.8287 -- iter: 6000/8000\n",
            "Training Step: 9076  | total loss: \u001b[1m\u001b[32m0.59585\u001b[0m\u001b[0m | time: 4.025s\n",
            "| SGD | epoch: 783 | loss: 0.59585 - acc: 0.8278 -- iter: 7000/8000\n",
            "Training Step: 9077  | total loss: \u001b[1m\u001b[32m0.59631\u001b[0m\u001b[0m | time: 4.597s\n",
            "| SGD | epoch: 783 | loss: 0.59631 - acc: 0.8274 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9078  | total loss: \u001b[1m\u001b[32m0.59840\u001b[0m\u001b[0m | time: 0.576s\n",
            "| SGD | epoch: 784 | loss: 0.59840 - acc: 0.8267 -- iter: 1000/8000\n",
            "Training Step: 9079  | total loss: \u001b[1m\u001b[32m0.59607\u001b[0m\u001b[0m | time: 1.146s\n",
            "| SGD | epoch: 784 | loss: 0.59607 - acc: 0.8278 -- iter: 2000/8000\n",
            "Training Step: 9080  | total loss: \u001b[1m\u001b[32m0.59958\u001b[0m\u001b[0m | time: 1.715s\n",
            "| SGD | epoch: 784 | loss: 0.59958 - acc: 0.8276 -- iter: 3000/8000\n",
            "Training Step: 9081  | total loss: \u001b[1m\u001b[32m0.59793\u001b[0m\u001b[0m | time: 2.293s\n",
            "| SGD | epoch: 784 | loss: 0.59793 - acc: 0.8289 -- iter: 4000/8000\n",
            "Training Step: 9082  | total loss: \u001b[1m\u001b[32m0.59568\u001b[0m\u001b[0m | time: 2.857s\n",
            "| SGD | epoch: 784 | loss: 0.59568 - acc: 0.8297 -- iter: 5000/8000\n",
            "Training Step: 9083  | total loss: \u001b[1m\u001b[32m0.59929\u001b[0m\u001b[0m | time: 3.430s\n",
            "| SGD | epoch: 784 | loss: 0.59929 - acc: 0.8283 -- iter: 6000/8000\n",
            "Training Step: 9084  | total loss: \u001b[1m\u001b[32m0.60006\u001b[0m\u001b[0m | time: 4.003s\n",
            "| SGD | epoch: 784 | loss: 0.60006 - acc: 0.8286 -- iter: 7000/8000\n",
            "Training Step: 9085  | total loss: \u001b[1m\u001b[32m0.59422\u001b[0m\u001b[0m | time: 4.576s\n",
            "| SGD | epoch: 784 | loss: 0.59422 - acc: 0.8304 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9086  | total loss: \u001b[1m\u001b[32m0.59595\u001b[0m\u001b[0m | time: 0.571s\n",
            "| SGD | epoch: 785 | loss: 0.59595 - acc: 0.8287 -- iter: 1000/8000\n",
            "Training Step: 9087  | total loss: \u001b[1m\u001b[32m0.59307\u001b[0m\u001b[0m | time: 1.141s\n",
            "| SGD | epoch: 785 | loss: 0.59307 - acc: 0.8304 -- iter: 2000/8000\n",
            "Training Step: 9088  | total loss: \u001b[1m\u001b[32m0.59450\u001b[0m\u001b[0m | time: 1.720s\n",
            "| SGD | epoch: 785 | loss: 0.59450 - acc: 0.8300 -- iter: 3000/8000\n",
            "Training Step: 9089  | total loss: \u001b[1m\u001b[32m0.59607\u001b[0m\u001b[0m | time: 2.288s\n",
            "| SGD | epoch: 785 | loss: 0.59607 - acc: 0.8294 -- iter: 4000/8000\n",
            "Training Step: 9090  | total loss: \u001b[1m\u001b[32m0.59538\u001b[0m\u001b[0m | time: 2.862s\n",
            "| SGD | epoch: 785 | loss: 0.59538 - acc: 0.8285 -- iter: 5000/8000\n",
            "Training Step: 9091  | total loss: \u001b[1m\u001b[32m0.59493\u001b[0m\u001b[0m | time: 3.434s\n",
            "| SGD | epoch: 785 | loss: 0.59493 - acc: 0.8288 -- iter: 6000/8000\n",
            "Training Step: 9092  | total loss: \u001b[1m\u001b[32m0.59441\u001b[0m\u001b[0m | time: 4.002s\n",
            "| SGD | epoch: 785 | loss: 0.59441 - acc: 0.8287 -- iter: 7000/8000\n",
            "Training Step: 9093  | total loss: \u001b[1m\u001b[32m0.59415\u001b[0m\u001b[0m | time: 4.574s\n",
            "| SGD | epoch: 785 | loss: 0.59415 - acc: 0.8294 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9094  | total loss: \u001b[1m\u001b[32m0.59690\u001b[0m\u001b[0m | time: 0.572s\n",
            "| SGD | epoch: 786 | loss: 0.59690 - acc: 0.8277 -- iter: 1000/8000\n",
            "Training Step: 9095  | total loss: \u001b[1m\u001b[32m0.59444\u001b[0m\u001b[0m | time: 1.147s\n",
            "| SGD | epoch: 786 | loss: 0.59444 - acc: 0.8286 -- iter: 2000/8000\n",
            "Training Step: 9096  | total loss: \u001b[1m\u001b[32m0.59650\u001b[0m\u001b[0m | time: 1.728s\n",
            "| SGD | epoch: 786 | loss: 0.59650 - acc: 0.8292 -- iter: 3000/8000\n",
            "Training Step: 9097  | total loss: \u001b[1m\u001b[32m0.60002\u001b[0m\u001b[0m | time: 2.302s\n",
            "| SGD | epoch: 786 | loss: 0.60002 - acc: 0.8263 -- iter: 4000/8000\n",
            "Training Step: 9098  | total loss: \u001b[1m\u001b[32m0.59610\u001b[0m\u001b[0m | time: 2.879s\n",
            "| SGD | epoch: 786 | loss: 0.59610 - acc: 0.8269 -- iter: 5000/8000\n",
            "Training Step: 9099  | total loss: \u001b[1m\u001b[32m0.59550\u001b[0m\u001b[0m | time: 3.450s\n",
            "| SGD | epoch: 786 | loss: 0.59550 - acc: 0.8272 -- iter: 6000/8000\n",
            "Training Step: 9100  | total loss: \u001b[1m\u001b[32m0.59136\u001b[0m\u001b[0m | time: 4.031s\n",
            "| SGD | epoch: 786 | loss: 0.59136 - acc: 0.8287 -- iter: 7000/8000\n",
            "Training Step: 9101  | total loss: \u001b[1m\u001b[32m0.59499\u001b[0m\u001b[0m | time: 4.606s\n",
            "| SGD | epoch: 786 | loss: 0.59499 - acc: 0.8270 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9102  | total loss: \u001b[1m\u001b[32m0.59692\u001b[0m\u001b[0m | time: 0.573s\n",
            "| SGD | epoch: 787 | loss: 0.59692 - acc: 0.8261 -- iter: 1000/8000\n",
            "Training Step: 9103  | total loss: \u001b[1m\u001b[32m0.59468\u001b[0m\u001b[0m | time: 1.271s\n",
            "| SGD | epoch: 787 | loss: 0.59468 - acc: 0.8271 -- iter: 2000/8000\n",
            "Training Step: 9104  | total loss: \u001b[1m\u001b[32m0.59564\u001b[0m\u001b[0m | time: 1.818s\n",
            "| SGD | epoch: 787 | loss: 0.59564 - acc: 0.8269 -- iter: 3000/8000\n",
            "Training Step: 9105  | total loss: \u001b[1m\u001b[32m0.59559\u001b[0m\u001b[0m | time: 2.366s\n",
            "| SGD | epoch: 787 | loss: 0.59559 - acc: 0.8270 -- iter: 4000/8000\n",
            "Training Step: 9106  | total loss: \u001b[1m\u001b[32m0.59480\u001b[0m\u001b[0m | time: 2.915s\n",
            "| SGD | epoch: 787 | loss: 0.59480 - acc: 0.8265 -- iter: 5000/8000\n",
            "Training Step: 9107  | total loss: \u001b[1m\u001b[32m0.59374\u001b[0m\u001b[0m | time: 3.460s\n",
            "| SGD | epoch: 787 | loss: 0.59374 - acc: 0.8262 -- iter: 6000/8000\n",
            "Training Step: 9108  | total loss: \u001b[1m\u001b[32m0.59487\u001b[0m\u001b[0m | time: 4.018s\n",
            "| SGD | epoch: 787 | loss: 0.59487 - acc: 0.8244 -- iter: 7000/8000\n",
            "Training Step: 9109  | total loss: \u001b[1m\u001b[32m0.59539\u001b[0m\u001b[0m | time: 4.593s\n",
            "| SGD | epoch: 787 | loss: 0.59539 - acc: 0.8264 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9110  | total loss: \u001b[1m\u001b[32m0.59653\u001b[0m\u001b[0m | time: 0.568s\n",
            "| SGD | epoch: 788 | loss: 0.59653 - acc: 0.8266 -- iter: 1000/8000\n",
            "Training Step: 9111  | total loss: \u001b[1m\u001b[32m0.59689\u001b[0m\u001b[0m | time: 1.141s\n",
            "| SGD | epoch: 788 | loss: 0.59689 - acc: 0.8287 -- iter: 2000/8000\n",
            "Training Step: 9112  | total loss: \u001b[1m\u001b[32m0.59503\u001b[0m\u001b[0m | time: 1.713s\n",
            "| SGD | epoch: 788 | loss: 0.59503 - acc: 0.8304 -- iter: 3000/8000\n",
            "Training Step: 9113  | total loss: \u001b[1m\u001b[32m0.59420\u001b[0m\u001b[0m | time: 2.283s\n",
            "| SGD | epoch: 788 | loss: 0.59420 - acc: 0.8315 -- iter: 4000/8000\n",
            "Training Step: 9114  | total loss: \u001b[1m\u001b[32m0.59207\u001b[0m\u001b[0m | time: 2.851s\n",
            "| SGD | epoch: 788 | loss: 0.59207 - acc: 0.8315 -- iter: 5000/8000\n",
            "Training Step: 9115  | total loss: \u001b[1m\u001b[32m0.59404\u001b[0m\u001b[0m | time: 3.429s\n",
            "| SGD | epoch: 788 | loss: 0.59404 - acc: 0.8309 -- iter: 6000/8000\n",
            "Training Step: 9116  | total loss: \u001b[1m\u001b[32m0.59731\u001b[0m\u001b[0m | time: 3.993s\n",
            "| SGD | epoch: 788 | loss: 0.59731 - acc: 0.8290 -- iter: 7000/8000\n",
            "Training Step: 9117  | total loss: \u001b[1m\u001b[32m0.59576\u001b[0m\u001b[0m | time: 4.566s\n",
            "| SGD | epoch: 788 | loss: 0.59576 - acc: 0.8304 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9118  | total loss: \u001b[1m\u001b[32m0.59584\u001b[0m\u001b[0m | time: 0.571s\n",
            "| SGD | epoch: 789 | loss: 0.59584 - acc: 0.8295 -- iter: 1000/8000\n",
            "Training Step: 9119  | total loss: \u001b[1m\u001b[32m0.59228\u001b[0m\u001b[0m | time: 1.148s\n",
            "| SGD | epoch: 789 | loss: 0.59228 - acc: 0.8314 -- iter: 2000/8000\n",
            "Training Step: 9120  | total loss: \u001b[1m\u001b[32m0.58913\u001b[0m\u001b[0m | time: 1.725s\n",
            "| SGD | epoch: 789 | loss: 0.58913 - acc: 0.8316 -- iter: 3000/8000\n",
            "Training Step: 9121  | total loss: \u001b[1m\u001b[32m0.58727\u001b[0m\u001b[0m | time: 2.304s\n",
            "| SGD | epoch: 789 | loss: 0.58727 - acc: 0.8323 -- iter: 4000/8000\n",
            "Training Step: 9122  | total loss: \u001b[1m\u001b[32m0.59204\u001b[0m\u001b[0m | time: 2.944s\n",
            "| SGD | epoch: 789 | loss: 0.59204 - acc: 0.8299 -- iter: 5000/8000\n",
            "Training Step: 9123  | total loss: \u001b[1m\u001b[32m0.59052\u001b[0m\u001b[0m | time: 3.569s\n",
            "| SGD | epoch: 789 | loss: 0.59052 - acc: 0.8300 -- iter: 6000/8000\n",
            "Training Step: 9124  | total loss: \u001b[1m\u001b[32m0.58656\u001b[0m\u001b[0m | time: 4.205s\n",
            "| SGD | epoch: 789 | loss: 0.58656 - acc: 0.8318 -- iter: 7000/8000\n",
            "Training Step: 9125  | total loss: \u001b[1m\u001b[32m0.58623\u001b[0m\u001b[0m | time: 4.827s\n",
            "| SGD | epoch: 789 | loss: 0.58623 - acc: 0.8307 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9126  | total loss: \u001b[1m\u001b[32m0.58539\u001b[0m\u001b[0m | time: 0.621s\n",
            "| SGD | epoch: 790 | loss: 0.58539 - acc: 0.8326 -- iter: 1000/8000\n",
            "Training Step: 9127  | total loss: \u001b[1m\u001b[32m0.58613\u001b[0m\u001b[0m | time: 1.245s\n",
            "| SGD | epoch: 790 | loss: 0.58613 - acc: 0.8303 -- iter: 2000/8000\n",
            "Training Step: 9128  | total loss: \u001b[1m\u001b[32m0.59001\u001b[0m\u001b[0m | time: 1.871s\n",
            "| SGD | epoch: 790 | loss: 0.59001 - acc: 0.8285 -- iter: 3000/8000\n",
            "Training Step: 9129  | total loss: \u001b[1m\u001b[32m0.59184\u001b[0m\u001b[0m | time: 2.482s\n",
            "| SGD | epoch: 790 | loss: 0.59184 - acc: 0.8282 -- iter: 4000/8000\n",
            "Training Step: 9130  | total loss: \u001b[1m\u001b[32m0.59312\u001b[0m\u001b[0m | time: 3.104s\n",
            "| SGD | epoch: 790 | loss: 0.59312 - acc: 0.8289 -- iter: 5000/8000\n",
            "Training Step: 9131  | total loss: \u001b[1m\u001b[32m0.59574\u001b[0m\u001b[0m | time: 3.736s\n",
            "| SGD | epoch: 790 | loss: 0.59574 - acc: 0.8291 -- iter: 6000/8000\n",
            "Training Step: 9132  | total loss: \u001b[1m\u001b[32m0.59451\u001b[0m\u001b[0m | time: 4.372s\n",
            "| SGD | epoch: 790 | loss: 0.59451 - acc: 0.8305 -- iter: 7000/8000\n",
            "Training Step: 9133  | total loss: \u001b[1m\u001b[32m0.59346\u001b[0m\u001b[0m | time: 4.988s\n",
            "| SGD | epoch: 790 | loss: 0.59346 - acc: 0.8285 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9134  | total loss: \u001b[1m\u001b[32m0.59338\u001b[0m\u001b[0m | time: 0.610s\n",
            "| SGD | epoch: 791 | loss: 0.59338 - acc: 0.8289 -- iter: 1000/8000\n",
            "Training Step: 9135  | total loss: \u001b[1m\u001b[32m0.59566\u001b[0m\u001b[0m | time: 1.226s\n",
            "| SGD | epoch: 791 | loss: 0.59566 - acc: 0.8286 -- iter: 2000/8000\n",
            "Training Step: 9136  | total loss: \u001b[1m\u001b[32m0.59662\u001b[0m\u001b[0m | time: 1.838s\n",
            "| SGD | epoch: 791 | loss: 0.59662 - acc: 0.8280 -- iter: 3000/8000\n",
            "Training Step: 9137  | total loss: \u001b[1m\u001b[32m0.59259\u001b[0m\u001b[0m | time: 2.451s\n",
            "| SGD | epoch: 791 | loss: 0.59259 - acc: 0.8302 -- iter: 4000/8000\n",
            "Training Step: 9138  | total loss: \u001b[1m\u001b[32m0.58873\u001b[0m\u001b[0m | time: 3.065s\n",
            "| SGD | epoch: 791 | loss: 0.58873 - acc: 0.8323 -- iter: 5000/8000\n",
            "Training Step: 9139  | total loss: \u001b[1m\u001b[32m0.58880\u001b[0m\u001b[0m | time: 3.673s\n",
            "| SGD | epoch: 791 | loss: 0.58880 - acc: 0.8313 -- iter: 6000/8000\n",
            "Training Step: 9140  | total loss: \u001b[1m\u001b[32m0.58818\u001b[0m\u001b[0m | time: 4.287s\n",
            "| SGD | epoch: 791 | loss: 0.58818 - acc: 0.8313 -- iter: 7000/8000\n",
            "Training Step: 9141  | total loss: \u001b[1m\u001b[32m0.58876\u001b[0m\u001b[0m | time: 4.925s\n",
            "| SGD | epoch: 791 | loss: 0.58876 - acc: 0.8292 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9142  | total loss: \u001b[1m\u001b[32m0.58644\u001b[0m\u001b[0m | time: 0.635s\n",
            "| SGD | epoch: 792 | loss: 0.58644 - acc: 0.8300 -- iter: 1000/8000\n",
            "Training Step: 9143  | total loss: \u001b[1m\u001b[32m0.59116\u001b[0m\u001b[0m | time: 1.282s\n",
            "| SGD | epoch: 792 | loss: 0.59116 - acc: 0.8278 -- iter: 2000/8000\n",
            "Training Step: 9144  | total loss: \u001b[1m\u001b[32m0.58955\u001b[0m\u001b[0m | time: 1.923s\n",
            "| SGD | epoch: 792 | loss: 0.58955 - acc: 0.8299 -- iter: 3000/8000\n",
            "Training Step: 9145  | total loss: \u001b[1m\u001b[32m0.59012\u001b[0m\u001b[0m | time: 2.448s\n",
            "| SGD | epoch: 792 | loss: 0.59012 - acc: 0.8291 -- iter: 4000/8000\n",
            "Training Step: 9146  | total loss: \u001b[1m\u001b[32m0.58817\u001b[0m\u001b[0m | time: 3.007s\n",
            "| SGD | epoch: 792 | loss: 0.58817 - acc: 0.8305 -- iter: 5000/8000\n",
            "Training Step: 9147  | total loss: \u001b[1m\u001b[32m0.58597\u001b[0m\u001b[0m | time: 3.563s\n",
            "| SGD | epoch: 792 | loss: 0.58597 - acc: 0.8321 -- iter: 6000/8000\n",
            "Training Step: 9148  | total loss: \u001b[1m\u001b[32m0.58697\u001b[0m\u001b[0m | time: 4.112s\n",
            "| SGD | epoch: 792 | loss: 0.58697 - acc: 0.8324 -- iter: 7000/8000\n",
            "Training Step: 9149  | total loss: \u001b[1m\u001b[32m0.58559\u001b[0m\u001b[0m | time: 4.661s\n",
            "| SGD | epoch: 792 | loss: 0.58559 - acc: 0.8334 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9150  | total loss: \u001b[1m\u001b[32m0.58473\u001b[0m\u001b[0m | time: 0.550s\n",
            "| SGD | epoch: 793 | loss: 0.58473 - acc: 0.8332 -- iter: 1000/8000\n",
            "Training Step: 9151  | total loss: \u001b[1m\u001b[32m0.58034\u001b[0m\u001b[0m | time: 1.095s\n",
            "| SGD | epoch: 793 | loss: 0.58034 - acc: 0.8355 -- iter: 2000/8000\n",
            "Training Step: 9152  | total loss: \u001b[1m\u001b[32m0.58214\u001b[0m\u001b[0m | time: 1.639s\n",
            "| SGD | epoch: 793 | loss: 0.58214 - acc: 0.8346 -- iter: 3000/8000\n",
            "Training Step: 9153  | total loss: \u001b[1m\u001b[32m0.58573\u001b[0m\u001b[0m | time: 2.185s\n",
            "| SGD | epoch: 793 | loss: 0.58573 - acc: 0.8336 -- iter: 4000/8000\n",
            "Training Step: 9154  | total loss: \u001b[1m\u001b[32m0.58454\u001b[0m\u001b[0m | time: 2.732s\n",
            "| SGD | epoch: 793 | loss: 0.58454 - acc: 0.8333 -- iter: 5000/8000\n",
            "Training Step: 9155  | total loss: \u001b[1m\u001b[32m0.58569\u001b[0m\u001b[0m | time: 3.281s\n",
            "| SGD | epoch: 793 | loss: 0.58569 - acc: 0.8338 -- iter: 6000/8000\n",
            "Training Step: 9156  | total loss: \u001b[1m\u001b[32m0.58654\u001b[0m\u001b[0m | time: 3.826s\n",
            "| SGD | epoch: 793 | loss: 0.58654 - acc: 0.8348 -- iter: 7000/8000\n",
            "Training Step: 9157  | total loss: \u001b[1m\u001b[32m0.58989\u001b[0m\u001b[0m | time: 4.371s\n",
            "| SGD | epoch: 793 | loss: 0.58989 - acc: 0.8331 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9158  | total loss: \u001b[1m\u001b[32m0.58842\u001b[0m\u001b[0m | time: 0.544s\n",
            "| SGD | epoch: 794 | loss: 0.58842 - acc: 0.8330 -- iter: 1000/8000\n",
            "Training Step: 9159  | total loss: \u001b[1m\u001b[32m0.59050\u001b[0m\u001b[0m | time: 1.084s\n",
            "| SGD | epoch: 794 | loss: 0.59050 - acc: 0.8320 -- iter: 2000/8000\n",
            "Training Step: 9160  | total loss: \u001b[1m\u001b[32m0.59387\u001b[0m\u001b[0m | time: 1.657s\n",
            "| SGD | epoch: 794 | loss: 0.59387 - acc: 0.8309 -- iter: 3000/8000\n",
            "Training Step: 9161  | total loss: \u001b[1m\u001b[32m0.59383\u001b[0m\u001b[0m | time: 2.233s\n",
            "| SGD | epoch: 794 | loss: 0.59383 - acc: 0.8322 -- iter: 4000/8000\n",
            "Training Step: 9162  | total loss: \u001b[1m\u001b[32m0.59077\u001b[0m\u001b[0m | time: 2.805s\n",
            "| SGD | epoch: 794 | loss: 0.59077 - acc: 0.8331 -- iter: 5000/8000\n",
            "Training Step: 9163  | total loss: \u001b[1m\u001b[32m0.59138\u001b[0m\u001b[0m | time: 3.384s\n",
            "| SGD | epoch: 794 | loss: 0.59138 - acc: 0.8324 -- iter: 6000/8000\n",
            "Training Step: 9164  | total loss: \u001b[1m\u001b[32m0.58504\u001b[0m\u001b[0m | time: 3.953s\n",
            "| SGD | epoch: 794 | loss: 0.58504 - acc: 0.8337 -- iter: 7000/8000\n",
            "Training Step: 9165  | total loss: \u001b[1m\u001b[32m0.57933\u001b[0m\u001b[0m | time: 4.531s\n",
            "| SGD | epoch: 794 | loss: 0.57933 - acc: 0.8353 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9166  | total loss: \u001b[1m\u001b[32m0.58422\u001b[0m\u001b[0m | time: 0.585s\n",
            "| SGD | epoch: 795 | loss: 0.58422 - acc: 0.8319 -- iter: 1000/8000\n",
            "Training Step: 9167  | total loss: \u001b[1m\u001b[32m0.58183\u001b[0m\u001b[0m | time: 1.167s\n",
            "| SGD | epoch: 795 | loss: 0.58183 - acc: 0.8331 -- iter: 2000/8000\n",
            "Training Step: 9168  | total loss: \u001b[1m\u001b[32m0.58128\u001b[0m\u001b[0m | time: 1.746s\n",
            "| SGD | epoch: 795 | loss: 0.58128 - acc: 0.8331 -- iter: 3000/8000\n",
            "Training Step: 9169  | total loss: \u001b[1m\u001b[32m0.58371\u001b[0m\u001b[0m | time: 2.326s\n",
            "| SGD | epoch: 795 | loss: 0.58371 - acc: 0.8317 -- iter: 4000/8000\n",
            "Training Step: 9170  | total loss: \u001b[1m\u001b[32m0.58716\u001b[0m\u001b[0m | time: 2.902s\n",
            "| SGD | epoch: 795 | loss: 0.58716 - acc: 0.8312 -- iter: 5000/8000\n",
            "Training Step: 9171  | total loss: \u001b[1m\u001b[32m0.58210\u001b[0m\u001b[0m | time: 3.484s\n",
            "| SGD | epoch: 795 | loss: 0.58210 - acc: 0.8318 -- iter: 6000/8000\n",
            "Training Step: 9172  | total loss: \u001b[1m\u001b[32m0.58305\u001b[0m\u001b[0m | time: 4.063s\n",
            "| SGD | epoch: 795 | loss: 0.58305 - acc: 0.8321 -- iter: 7000/8000\n",
            "Training Step: 9173  | total loss: \u001b[1m\u001b[32m0.58178\u001b[0m\u001b[0m | time: 4.631s\n",
            "| SGD | epoch: 795 | loss: 0.58178 - acc: 0.8326 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9174  | total loss: \u001b[1m\u001b[32m0.58019\u001b[0m\u001b[0m | time: 0.577s\n",
            "| SGD | epoch: 796 | loss: 0.58019 - acc: 0.8335 -- iter: 1000/8000\n",
            "Training Step: 9175  | total loss: \u001b[1m\u001b[32m0.58181\u001b[0m\u001b[0m | time: 1.144s\n",
            "| SGD | epoch: 796 | loss: 0.58181 - acc: 0.8335 -- iter: 2000/8000\n",
            "Training Step: 9176  | total loss: \u001b[1m\u001b[32m0.58314\u001b[0m\u001b[0m | time: 1.715s\n",
            "| SGD | epoch: 796 | loss: 0.58314 - acc: 0.8340 -- iter: 3000/8000\n",
            "Training Step: 9177  | total loss: \u001b[1m\u001b[32m0.58786\u001b[0m\u001b[0m | time: 2.300s\n",
            "| SGD | epoch: 796 | loss: 0.58786 - acc: 0.8326 -- iter: 4000/8000\n",
            "Training Step: 9178  | total loss: \u001b[1m\u001b[32m0.58884\u001b[0m\u001b[0m | time: 2.871s\n",
            "| SGD | epoch: 796 | loss: 0.58884 - acc: 0.8309 -- iter: 5000/8000\n",
            "Training Step: 9179  | total loss: \u001b[1m\u001b[32m0.58765\u001b[0m\u001b[0m | time: 3.440s\n",
            "| SGD | epoch: 796 | loss: 0.58765 - acc: 0.8311 -- iter: 6000/8000\n",
            "Training Step: 9180  | total loss: \u001b[1m\u001b[32m0.58639\u001b[0m\u001b[0m | time: 4.014s\n",
            "| SGD | epoch: 796 | loss: 0.58639 - acc: 0.8314 -- iter: 7000/8000\n",
            "Training Step: 9181  | total loss: \u001b[1m\u001b[32m0.58191\u001b[0m\u001b[0m | time: 4.581s\n",
            "| SGD | epoch: 796 | loss: 0.58191 - acc: 0.8323 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9182  | total loss: \u001b[1m\u001b[32m0.58301\u001b[0m\u001b[0m | time: 0.574s\n",
            "| SGD | epoch: 797 | loss: 0.58301 - acc: 0.8335 -- iter: 1000/8000\n",
            "Training Step: 9183  | total loss: \u001b[1m\u001b[32m0.58354\u001b[0m\u001b[0m | time: 1.147s\n",
            "| SGD | epoch: 797 | loss: 0.58354 - acc: 0.8343 -- iter: 2000/8000\n",
            "Training Step: 9184  | total loss: \u001b[1m\u001b[32m0.58343\u001b[0m\u001b[0m | time: 1.725s\n",
            "| SGD | epoch: 797 | loss: 0.58343 - acc: 0.8343 -- iter: 3000/8000\n",
            "Training Step: 9185  | total loss: \u001b[1m\u001b[32m0.58421\u001b[0m\u001b[0m | time: 2.299s\n",
            "| SGD | epoch: 797 | loss: 0.58421 - acc: 0.8345 -- iter: 4000/8000\n",
            "Training Step: 9186  | total loss: \u001b[1m\u001b[32m0.58674\u001b[0m\u001b[0m | time: 2.876s\n",
            "| SGD | epoch: 797 | loss: 0.58674 - acc: 0.8330 -- iter: 5000/8000\n",
            "Training Step: 9187  | total loss: \u001b[1m\u001b[32m0.58422\u001b[0m\u001b[0m | time: 3.472s\n",
            "| SGD | epoch: 797 | loss: 0.58422 - acc: 0.8341 -- iter: 6000/8000\n",
            "Training Step: 9188  | total loss: \u001b[1m\u001b[32m0.58364\u001b[0m\u001b[0m | time: 4.038s\n",
            "| SGD | epoch: 797 | loss: 0.58364 - acc: 0.8340 -- iter: 7000/8000\n",
            "Training Step: 9189  | total loss: \u001b[1m\u001b[32m0.58370\u001b[0m\u001b[0m | time: 4.612s\n",
            "| SGD | epoch: 797 | loss: 0.58370 - acc: 0.8337 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9190  | total loss: \u001b[1m\u001b[32m0.58583\u001b[0m\u001b[0m | time: 0.580s\n",
            "| SGD | epoch: 798 | loss: 0.58583 - acc: 0.8317 -- iter: 1000/8000\n",
            "Training Step: 9191  | total loss: \u001b[1m\u001b[32m0.58633\u001b[0m\u001b[0m | time: 1.170s\n",
            "| SGD | epoch: 798 | loss: 0.58633 - acc: 0.8325 -- iter: 2000/8000\n",
            "Training Step: 9192  | total loss: \u001b[1m\u001b[32m0.58583\u001b[0m\u001b[0m | time: 1.744s\n",
            "| SGD | epoch: 798 | loss: 0.58583 - acc: 0.8328 -- iter: 3000/8000\n",
            "Training Step: 9193  | total loss: \u001b[1m\u001b[32m0.58679\u001b[0m\u001b[0m | time: 2.323s\n",
            "| SGD | epoch: 798 | loss: 0.58679 - acc: 0.8325 -- iter: 4000/8000\n",
            "Training Step: 9194  | total loss: \u001b[1m\u001b[32m0.58564\u001b[0m\u001b[0m | time: 2.909s\n",
            "| SGD | epoch: 798 | loss: 0.58564 - acc: 0.8337 -- iter: 5000/8000\n",
            "Training Step: 9195  | total loss: \u001b[1m\u001b[32m0.59363\u001b[0m\u001b[0m | time: 3.485s\n",
            "| SGD | epoch: 798 | loss: 0.59363 - acc: 0.8291 -- iter: 6000/8000\n",
            "Training Step: 9196  | total loss: \u001b[1m\u001b[32m0.59109\u001b[0m\u001b[0m | time: 4.070s\n",
            "| SGD | epoch: 798 | loss: 0.59109 - acc: 0.8297 -- iter: 7000/8000\n",
            "Training Step: 9197  | total loss: \u001b[1m\u001b[32m0.59054\u001b[0m\u001b[0m | time: 4.646s\n",
            "| SGD | epoch: 798 | loss: 0.59054 - acc: 0.8292 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9198  | total loss: \u001b[1m\u001b[32m0.58926\u001b[0m\u001b[0m | time: 0.570s\n",
            "| SGD | epoch: 799 | loss: 0.58926 - acc: 0.8302 -- iter: 1000/8000\n",
            "Training Step: 9199  | total loss: \u001b[1m\u001b[32m0.58581\u001b[0m\u001b[0m | time: 1.143s\n",
            "| SGD | epoch: 799 | loss: 0.58581 - acc: 0.8314 -- iter: 2000/8000\n",
            "Training Step: 9200  | total loss: \u001b[1m\u001b[32m0.58491\u001b[0m\u001b[0m | time: 1.717s\n",
            "| SGD | epoch: 799 | loss: 0.58491 - acc: 0.8318 -- iter: 3000/8000\n",
            "Training Step: 9201  | total loss: \u001b[1m\u001b[32m0.58340\u001b[0m\u001b[0m | time: 2.295s\n",
            "| SGD | epoch: 799 | loss: 0.58340 - acc: 0.8330 -- iter: 4000/8000\n",
            "Training Step: 9202  | total loss: \u001b[1m\u001b[32m0.58605\u001b[0m\u001b[0m | time: 2.864s\n",
            "| SGD | epoch: 799 | loss: 0.58605 - acc: 0.8325 -- iter: 5000/8000\n",
            "Training Step: 9203  | total loss: \u001b[1m\u001b[32m0.58319\u001b[0m\u001b[0m | time: 3.444s\n",
            "| SGD | epoch: 799 | loss: 0.58319 - acc: 0.8335 -- iter: 6000/8000\n",
            "Training Step: 9204  | total loss: \u001b[1m\u001b[32m0.58619\u001b[0m\u001b[0m | time: 4.028s\n",
            "| SGD | epoch: 799 | loss: 0.58619 - acc: 0.8335 -- iter: 7000/8000\n",
            "Training Step: 9205  | total loss: \u001b[1m\u001b[32m0.58436\u001b[0m\u001b[0m | time: 4.595s\n",
            "| SGD | epoch: 799 | loss: 0.58436 - acc: 0.8354 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9206  | total loss: \u001b[1m\u001b[32m0.58514\u001b[0m\u001b[0m | time: 0.577s\n",
            "| SGD | epoch: 800 | loss: 0.58514 - acc: 0.8349 -- iter: 1000/8000\n",
            "Training Step: 9207  | total loss: \u001b[1m\u001b[32m0.58631\u001b[0m\u001b[0m | time: 1.153s\n",
            "| SGD | epoch: 800 | loss: 0.58631 - acc: 0.8334 -- iter: 2000/8000\n",
            "Training Step: 9208  | total loss: \u001b[1m\u001b[32m0.58444\u001b[0m\u001b[0m | time: 1.730s\n",
            "| SGD | epoch: 800 | loss: 0.58444 - acc: 0.8347 -- iter: 3000/8000\n",
            "Training Step: 9209  | total loss: \u001b[1m\u001b[32m0.58408\u001b[0m\u001b[0m | time: 2.304s\n",
            "| SGD | epoch: 800 | loss: 0.58408 - acc: 0.8349 -- iter: 4000/8000\n",
            "Training Step: 9210  | total loss: \u001b[1m\u001b[32m0.58297\u001b[0m\u001b[0m | time: 2.881s\n",
            "| SGD | epoch: 800 | loss: 0.58297 - acc: 0.8347 -- iter: 5000/8000\n",
            "Training Step: 9211  | total loss: \u001b[1m\u001b[32m0.58293\u001b[0m\u001b[0m | time: 3.462s\n",
            "| SGD | epoch: 800 | loss: 0.58293 - acc: 0.8350 -- iter: 6000/8000\n",
            "Training Step: 9212  | total loss: \u001b[1m\u001b[32m0.58174\u001b[0m\u001b[0m | time: 4.036s\n",
            "| SGD | epoch: 800 | loss: 0.58174 - acc: 0.8350 -- iter: 7000/8000\n",
            "Training Step: 9213  | total loss: \u001b[1m\u001b[32m0.58522\u001b[0m\u001b[0m | time: 4.608s\n",
            "| SGD | epoch: 800 | loss: 0.58522 - acc: 0.8348 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9214  | total loss: \u001b[1m\u001b[32m0.58597\u001b[0m\u001b[0m | time: 0.588s\n",
            "| SGD | epoch: 801 | loss: 0.58597 - acc: 0.8340 -- iter: 1000/8000\n",
            "Training Step: 9215  | total loss: \u001b[1m\u001b[32m0.58687\u001b[0m\u001b[0m | time: 1.170s\n",
            "| SGD | epoch: 801 | loss: 0.58687 - acc: 0.8346 -- iter: 2000/8000\n",
            "Training Step: 9216  | total loss: \u001b[1m\u001b[32m0.58452\u001b[0m\u001b[0m | time: 1.744s\n",
            "| SGD | epoch: 801 | loss: 0.58452 - acc: 0.8344 -- iter: 3000/8000\n",
            "Training Step: 9217  | total loss: \u001b[1m\u001b[32m0.58469\u001b[0m\u001b[0m | time: 2.322s\n",
            "| SGD | epoch: 801 | loss: 0.58469 - acc: 0.8345 -- iter: 4000/8000\n",
            "Training Step: 9218  | total loss: \u001b[1m\u001b[32m0.58361\u001b[0m\u001b[0m | time: 2.906s\n",
            "| SGD | epoch: 801 | loss: 0.58361 - acc: 0.8346 -- iter: 5000/8000\n",
            "Training Step: 9219  | total loss: \u001b[1m\u001b[32m0.58152\u001b[0m\u001b[0m | time: 3.484s\n",
            "| SGD | epoch: 801 | loss: 0.58152 - acc: 0.8367 -- iter: 6000/8000\n",
            "Training Step: 9220  | total loss: \u001b[1m\u001b[32m0.58465\u001b[0m\u001b[0m | time: 4.059s\n",
            "| SGD | epoch: 801 | loss: 0.58465 - acc: 0.8347 -- iter: 7000/8000\n",
            "Training Step: 9221  | total loss: \u001b[1m\u001b[32m0.58041\u001b[0m\u001b[0m | time: 4.639s\n",
            "| SGD | epoch: 801 | loss: 0.58041 - acc: 0.8350 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9222  | total loss: \u001b[1m\u001b[32m0.58281\u001b[0m\u001b[0m | time: 0.567s\n",
            "| SGD | epoch: 802 | loss: 0.58281 - acc: 0.8339 -- iter: 1000/8000\n",
            "Training Step: 9223  | total loss: \u001b[1m\u001b[32m0.58566\u001b[0m\u001b[0m | time: 1.148s\n",
            "| SGD | epoch: 802 | loss: 0.58566 - acc: 0.8326 -- iter: 2000/8000\n",
            "Training Step: 9224  | total loss: \u001b[1m\u001b[32m0.58539\u001b[0m\u001b[0m | time: 1.728s\n",
            "| SGD | epoch: 802 | loss: 0.58539 - acc: 0.8317 -- iter: 3000/8000\n",
            "Training Step: 9225  | total loss: \u001b[1m\u001b[32m0.58615\u001b[0m\u001b[0m | time: 2.297s\n",
            "| SGD | epoch: 802 | loss: 0.58615 - acc: 0.8303 -- iter: 4000/8000\n",
            "Training Step: 9226  | total loss: \u001b[1m\u001b[32m0.58644\u001b[0m\u001b[0m | time: 2.864s\n",
            "| SGD | epoch: 802 | loss: 0.58644 - acc: 0.8303 -- iter: 5000/8000\n",
            "Training Step: 9227  | total loss: \u001b[1m\u001b[32m0.58383\u001b[0m\u001b[0m | time: 3.446s\n",
            "| SGD | epoch: 802 | loss: 0.58383 - acc: 0.8307 -- iter: 6000/8000\n",
            "Training Step: 9228  | total loss: \u001b[1m\u001b[32m0.58021\u001b[0m\u001b[0m | time: 4.015s\n",
            "| SGD | epoch: 802 | loss: 0.58021 - acc: 0.8326 -- iter: 7000/8000\n",
            "Training Step: 9229  | total loss: \u001b[1m\u001b[32m0.57582\u001b[0m\u001b[0m | time: 4.593s\n",
            "| SGD | epoch: 802 | loss: 0.57582 - acc: 0.8350 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9230  | total loss: \u001b[1m\u001b[32m0.57758\u001b[0m\u001b[0m | time: 0.577s\n",
            "| SGD | epoch: 803 | loss: 0.57758 - acc: 0.8357 -- iter: 1000/8000\n",
            "Training Step: 9231  | total loss: \u001b[1m\u001b[32m0.58193\u001b[0m\u001b[0m | time: 1.144s\n",
            "| SGD | epoch: 803 | loss: 0.58193 - acc: 0.8352 -- iter: 2000/8000\n",
            "Training Step: 9232  | total loss: \u001b[1m\u001b[32m0.58331\u001b[0m\u001b[0m | time: 1.728s\n",
            "| SGD | epoch: 803 | loss: 0.58331 - acc: 0.8344 -- iter: 3000/8000\n",
            "Training Step: 9233  | total loss: \u001b[1m\u001b[32m0.58411\u001b[0m\u001b[0m | time: 2.298s\n",
            "| SGD | epoch: 803 | loss: 0.58411 - acc: 0.8344 -- iter: 4000/8000\n",
            "Training Step: 9234  | total loss: \u001b[1m\u001b[32m0.58113\u001b[0m\u001b[0m | time: 2.874s\n",
            "| SGD | epoch: 803 | loss: 0.58113 - acc: 0.8357 -- iter: 5000/8000\n",
            "Training Step: 9235  | total loss: \u001b[1m\u001b[32m0.58108\u001b[0m\u001b[0m | time: 3.447s\n",
            "| SGD | epoch: 803 | loss: 0.58108 - acc: 0.8375 -- iter: 6000/8000\n",
            "Training Step: 9236  | total loss: \u001b[1m\u001b[32m0.57638\u001b[0m\u001b[0m | time: 4.022s\n",
            "| SGD | epoch: 803 | loss: 0.57638 - acc: 0.8392 -- iter: 7000/8000\n",
            "Training Step: 9237  | total loss: \u001b[1m\u001b[32m0.57242\u001b[0m\u001b[0m | time: 4.601s\n",
            "| SGD | epoch: 803 | loss: 0.57242 - acc: 0.8408 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9238  | total loss: \u001b[1m\u001b[32m0.57415\u001b[0m\u001b[0m | time: 0.588s\n",
            "| SGD | epoch: 804 | loss: 0.57415 - acc: 0.8389 -- iter: 1000/8000\n",
            "Training Step: 9239  | total loss: \u001b[1m\u001b[32m0.57430\u001b[0m\u001b[0m | time: 1.165s\n",
            "| SGD | epoch: 804 | loss: 0.57430 - acc: 0.8380 -- iter: 2000/8000\n",
            "Training Step: 9240  | total loss: \u001b[1m\u001b[32m0.57481\u001b[0m\u001b[0m | time: 1.746s\n",
            "| SGD | epoch: 804 | loss: 0.57481 - acc: 0.8369 -- iter: 3000/8000\n",
            "Training Step: 9241  | total loss: \u001b[1m\u001b[32m0.57302\u001b[0m\u001b[0m | time: 2.355s\n",
            "| SGD | epoch: 804 | loss: 0.57302 - acc: 0.8374 -- iter: 4000/8000\n",
            "Training Step: 9242  | total loss: \u001b[1m\u001b[32m0.57893\u001b[0m\u001b[0m | time: 3.050s\n",
            "| SGD | epoch: 804 | loss: 0.57893 - acc: 0.8364 -- iter: 5000/8000\n",
            "Training Step: 9243  | total loss: \u001b[1m\u001b[32m0.57814\u001b[0m\u001b[0m | time: 3.607s\n",
            "| SGD | epoch: 804 | loss: 0.57814 - acc: 0.8366 -- iter: 6000/8000\n",
            "Training Step: 9244  | total loss: \u001b[1m\u001b[32m0.57907\u001b[0m\u001b[0m | time: 4.159s\n",
            "| SGD | epoch: 804 | loss: 0.57907 - acc: 0.8348 -- iter: 7000/8000\n",
            "Training Step: 9245  | total loss: \u001b[1m\u001b[32m0.57947\u001b[0m\u001b[0m | time: 4.708s\n",
            "| SGD | epoch: 804 | loss: 0.57947 - acc: 0.8349 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9246  | total loss: \u001b[1m\u001b[32m0.57884\u001b[0m\u001b[0m | time: 0.547s\n",
            "| SGD | epoch: 805 | loss: 0.57884 - acc: 0.8353 -- iter: 1000/8000\n",
            "Training Step: 9247  | total loss: \u001b[1m\u001b[32m0.57615\u001b[0m\u001b[0m | time: 1.093s\n",
            "| SGD | epoch: 805 | loss: 0.57615 - acc: 0.8371 -- iter: 2000/8000\n",
            "Training Step: 9248  | total loss: \u001b[1m\u001b[32m0.57793\u001b[0m\u001b[0m | time: 1.651s\n",
            "| SGD | epoch: 805 | loss: 0.57793 - acc: 0.8362 -- iter: 3000/8000\n",
            "Training Step: 9249  | total loss: \u001b[1m\u001b[32m0.58119\u001b[0m\u001b[0m | time: 2.233s\n",
            "| SGD | epoch: 805 | loss: 0.58119 - acc: 0.8348 -- iter: 4000/8000\n",
            "Training Step: 9250  | total loss: \u001b[1m\u001b[32m0.58369\u001b[0m\u001b[0m | time: 2.805s\n",
            "| SGD | epoch: 805 | loss: 0.58369 - acc: 0.8321 -- iter: 5000/8000\n",
            "Training Step: 9251  | total loss: \u001b[1m\u001b[32m0.58203\u001b[0m\u001b[0m | time: 3.375s\n",
            "| SGD | epoch: 805 | loss: 0.58203 - acc: 0.8324 -- iter: 6000/8000\n",
            "Training Step: 9252  | total loss: \u001b[1m\u001b[32m0.57950\u001b[0m\u001b[0m | time: 3.951s\n",
            "| SGD | epoch: 805 | loss: 0.57950 - acc: 0.8340 -- iter: 7000/8000\n",
            "Training Step: 9253  | total loss: \u001b[1m\u001b[32m0.57560\u001b[0m\u001b[0m | time: 4.528s\n",
            "| SGD | epoch: 805 | loss: 0.57560 - acc: 0.8351 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9254  | total loss: \u001b[1m\u001b[32m0.57728\u001b[0m\u001b[0m | time: 0.574s\n",
            "| SGD | epoch: 806 | loss: 0.57728 - acc: 0.8336 -- iter: 1000/8000\n",
            "Training Step: 9255  | total loss: \u001b[1m\u001b[32m0.57876\u001b[0m\u001b[0m | time: 1.147s\n",
            "| SGD | epoch: 806 | loss: 0.57876 - acc: 0.8330 -- iter: 2000/8000\n",
            "Training Step: 9256  | total loss: \u001b[1m\u001b[32m0.58271\u001b[0m\u001b[0m | time: 1.725s\n",
            "| SGD | epoch: 806 | loss: 0.58271 - acc: 0.8306 -- iter: 3000/8000\n",
            "Training Step: 9257  | total loss: \u001b[1m\u001b[32m0.58004\u001b[0m\u001b[0m | time: 2.297s\n",
            "| SGD | epoch: 806 | loss: 0.58004 - acc: 0.8326 -- iter: 4000/8000\n",
            "Training Step: 9258  | total loss: \u001b[1m\u001b[32m0.57987\u001b[0m\u001b[0m | time: 2.875s\n",
            "| SGD | epoch: 806 | loss: 0.57987 - acc: 0.8317 -- iter: 5000/8000\n",
            "Training Step: 9259  | total loss: \u001b[1m\u001b[32m0.58077\u001b[0m\u001b[0m | time: 3.454s\n",
            "| SGD | epoch: 806 | loss: 0.58077 - acc: 0.8304 -- iter: 6000/8000\n",
            "Training Step: 9260  | total loss: \u001b[1m\u001b[32m0.58133\u001b[0m\u001b[0m | time: 4.027s\n",
            "| SGD | epoch: 806 | loss: 0.58133 - acc: 0.8287 -- iter: 7000/8000\n",
            "Training Step: 9261  | total loss: \u001b[1m\u001b[32m0.58282\u001b[0m\u001b[0m | time: 4.599s\n",
            "| SGD | epoch: 806 | loss: 0.58282 - acc: 0.8308 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9262  | total loss: \u001b[1m\u001b[32m0.58185\u001b[0m\u001b[0m | time: 0.579s\n",
            "| SGD | epoch: 807 | loss: 0.58185 - acc: 0.8316 -- iter: 1000/8000\n",
            "Training Step: 9263  | total loss: \u001b[1m\u001b[32m0.58055\u001b[0m\u001b[0m | time: 1.155s\n",
            "| SGD | epoch: 807 | loss: 0.58055 - acc: 0.8332 -- iter: 2000/8000\n",
            "Training Step: 9264  | total loss: \u001b[1m\u001b[32m0.57815\u001b[0m\u001b[0m | time: 1.728s\n",
            "| SGD | epoch: 807 | loss: 0.57815 - acc: 0.8352 -- iter: 3000/8000\n",
            "Training Step: 9265  | total loss: \u001b[1m\u001b[32m0.57442\u001b[0m\u001b[0m | time: 2.313s\n",
            "| SGD | epoch: 807 | loss: 0.57442 - acc: 0.8371 -- iter: 4000/8000\n",
            "Training Step: 9266  | total loss: \u001b[1m\u001b[32m0.57874\u001b[0m\u001b[0m | time: 2.961s\n",
            "| SGD | epoch: 807 | loss: 0.57874 - acc: 0.8352 -- iter: 5000/8000\n",
            "Training Step: 9267  | total loss: \u001b[1m\u001b[32m0.57397\u001b[0m\u001b[0m | time: 3.590s\n",
            "| SGD | epoch: 807 | loss: 0.57397 - acc: 0.8379 -- iter: 6000/8000\n",
            "Training Step: 9268  | total loss: \u001b[1m\u001b[32m0.57594\u001b[0m\u001b[0m | time: 4.221s\n",
            "| SGD | epoch: 807 | loss: 0.57594 - acc: 0.8385 -- iter: 7000/8000\n",
            "Training Step: 9269  | total loss: \u001b[1m\u001b[32m0.57335\u001b[0m\u001b[0m | time: 4.843s\n",
            "| SGD | epoch: 807 | loss: 0.57335 - acc: 0.8390 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9270  | total loss: \u001b[1m\u001b[32m0.57292\u001b[0m\u001b[0m | time: 0.614s\n",
            "| SGD | epoch: 808 | loss: 0.57292 - acc: 0.8386 -- iter: 1000/8000\n",
            "Training Step: 9271  | total loss: \u001b[1m\u001b[32m0.57532\u001b[0m\u001b[0m | time: 1.229s\n",
            "| SGD | epoch: 808 | loss: 0.57532 - acc: 0.8376 -- iter: 2000/8000\n",
            "Training Step: 9272  | total loss: \u001b[1m\u001b[32m0.57600\u001b[0m\u001b[0m | time: 1.864s\n",
            "| SGD | epoch: 808 | loss: 0.57600 - acc: 0.8360 -- iter: 3000/8000\n",
            "Training Step: 9273  | total loss: \u001b[1m\u001b[32m0.57630\u001b[0m\u001b[0m | time: 2.494s\n",
            "| SGD | epoch: 808 | loss: 0.57630 - acc: 0.8355 -- iter: 4000/8000\n",
            "Training Step: 9274  | total loss: \u001b[1m\u001b[32m0.57711\u001b[0m\u001b[0m | time: 3.111s\n",
            "| SGD | epoch: 808 | loss: 0.57711 - acc: 0.8339 -- iter: 5000/8000\n",
            "Training Step: 9275  | total loss: \u001b[1m\u001b[32m0.58192\u001b[0m\u001b[0m | time: 3.723s\n",
            "| SGD | epoch: 808 | loss: 0.58192 - acc: 0.8329 -- iter: 6000/8000\n",
            "Training Step: 9276  | total loss: \u001b[1m\u001b[32m0.58158\u001b[0m\u001b[0m | time: 4.341s\n",
            "| SGD | epoch: 808 | loss: 0.58158 - acc: 0.8331 -- iter: 7000/8000\n",
            "Training Step: 9277  | total loss: \u001b[1m\u001b[32m0.58302\u001b[0m\u001b[0m | time: 4.957s\n",
            "| SGD | epoch: 808 | loss: 0.58302 - acc: 0.8325 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9278  | total loss: \u001b[1m\u001b[32m0.58075\u001b[0m\u001b[0m | time: 0.617s\n",
            "| SGD | epoch: 809 | loss: 0.58075 - acc: 0.8332 -- iter: 1000/8000\n",
            "Training Step: 9279  | total loss: \u001b[1m\u001b[32m0.58271\u001b[0m\u001b[0m | time: 1.236s\n",
            "| SGD | epoch: 809 | loss: 0.58271 - acc: 0.8322 -- iter: 2000/8000\n",
            "Training Step: 9280  | total loss: \u001b[1m\u001b[32m0.57970\u001b[0m\u001b[0m | time: 1.853s\n",
            "| SGD | epoch: 809 | loss: 0.57970 - acc: 0.8318 -- iter: 3000/8000\n",
            "Training Step: 9281  | total loss: \u001b[1m\u001b[32m0.58118\u001b[0m\u001b[0m | time: 2.468s\n",
            "| SGD | epoch: 809 | loss: 0.58118 - acc: 0.8327 -- iter: 4000/8000\n",
            "Training Step: 9282  | total loss: \u001b[1m\u001b[32m0.58220\u001b[0m\u001b[0m | time: 3.103s\n",
            "| SGD | epoch: 809 | loss: 0.58220 - acc: 0.8330 -- iter: 5000/8000\n",
            "Training Step: 9283  | total loss: \u001b[1m\u001b[32m0.58034\u001b[0m\u001b[0m | time: 3.737s\n",
            "| SGD | epoch: 809 | loss: 0.58034 - acc: 0.8351 -- iter: 6000/8000\n",
            "Training Step: 9284  | total loss: \u001b[1m\u001b[32m0.58101\u001b[0m\u001b[0m | time: 4.376s\n",
            "| SGD | epoch: 809 | loss: 0.58101 - acc: 0.8349 -- iter: 7000/8000\n",
            "Training Step: 9285  | total loss: \u001b[1m\u001b[32m0.57760\u001b[0m\u001b[0m | time: 5.005s\n",
            "| SGD | epoch: 809 | loss: 0.57760 - acc: 0.8354 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9286  | total loss: \u001b[1m\u001b[32m0.57868\u001b[0m\u001b[0m | time: 0.626s\n",
            "| SGD | epoch: 810 | loss: 0.57868 - acc: 0.8354 -- iter: 1000/8000\n",
            "Training Step: 9287  | total loss: \u001b[1m\u001b[32m0.57787\u001b[0m\u001b[0m | time: 1.252s\n",
            "| SGD | epoch: 810 | loss: 0.57787 - acc: 0.8349 -- iter: 2000/8000\n",
            "Training Step: 9288  | total loss: \u001b[1m\u001b[32m0.57836\u001b[0m\u001b[0m | time: 1.879s\n",
            "| SGD | epoch: 810 | loss: 0.57836 - acc: 0.8347 -- iter: 3000/8000\n",
            "Training Step: 9289  | total loss: \u001b[1m\u001b[32m0.57916\u001b[0m\u001b[0m | time: 2.503s\n",
            "| SGD | epoch: 810 | loss: 0.57916 - acc: 0.8351 -- iter: 4000/8000\n",
            "Training Step: 9290  | total loss: \u001b[1m\u001b[32m0.57752\u001b[0m\u001b[0m | time: 3.129s\n",
            "| SGD | epoch: 810 | loss: 0.57752 - acc: 0.8363 -- iter: 5000/8000\n",
            "Training Step: 9291  | total loss: \u001b[1m\u001b[32m0.57429\u001b[0m\u001b[0m | time: 3.752s\n",
            "| SGD | epoch: 810 | loss: 0.57429 - acc: 0.8382 -- iter: 6000/8000\n",
            "Training Step: 9292  | total loss: \u001b[1m\u001b[32m0.57415\u001b[0m\u001b[0m | time: 4.368s\n",
            "| SGD | epoch: 810 | loss: 0.57415 - acc: 0.8378 -- iter: 7000/8000\n",
            "Training Step: 9293  | total loss: \u001b[1m\u001b[32m0.57874\u001b[0m\u001b[0m | time: 4.986s\n",
            "| SGD | epoch: 810 | loss: 0.57874 - acc: 0.8351 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9294  | total loss: \u001b[1m\u001b[32m0.58069\u001b[0m\u001b[0m | time: 0.619s\n",
            "| SGD | epoch: 811 | loss: 0.58069 - acc: 0.8353 -- iter: 1000/8000\n",
            "Training Step: 9295  | total loss: \u001b[1m\u001b[32m0.58012\u001b[0m\u001b[0m | time: 1.234s\n",
            "| SGD | epoch: 811 | loss: 0.58012 - acc: 0.8347 -- iter: 2000/8000\n",
            "Training Step: 9296  | total loss: \u001b[1m\u001b[32m0.57919\u001b[0m\u001b[0m | time: 1.852s\n",
            "| SGD | epoch: 811 | loss: 0.57919 - acc: 0.8350 -- iter: 3000/8000\n",
            "Training Step: 9297  | total loss: \u001b[1m\u001b[32m0.57984\u001b[0m\u001b[0m | time: 2.469s\n",
            "| SGD | epoch: 811 | loss: 0.57984 - acc: 0.8343 -- iter: 4000/8000\n",
            "Training Step: 9298  | total loss: \u001b[1m\u001b[32m0.58038\u001b[0m\u001b[0m | time: 3.084s\n",
            "| SGD | epoch: 811 | loss: 0.58038 - acc: 0.8348 -- iter: 5000/8000\n",
            "Training Step: 9299  | total loss: \u001b[1m\u001b[32m0.57865\u001b[0m\u001b[0m | time: 3.702s\n",
            "| SGD | epoch: 811 | loss: 0.57865 - acc: 0.8353 -- iter: 6000/8000\n",
            "Training Step: 9300  | total loss: \u001b[1m\u001b[32m0.57614\u001b[0m\u001b[0m | time: 4.320s\n",
            "| SGD | epoch: 811 | loss: 0.57614 - acc: 0.8376 -- iter: 7000/8000\n",
            "Training Step: 9301  | total loss: \u001b[1m\u001b[32m0.57643\u001b[0m\u001b[0m | time: 4.940s\n",
            "| SGD | epoch: 811 | loss: 0.57643 - acc: 0.8357 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9302  | total loss: \u001b[1m\u001b[32m0.57735\u001b[0m\u001b[0m | time: 0.614s\n",
            "| SGD | epoch: 812 | loss: 0.57735 - acc: 0.8351 -- iter: 1000/8000\n",
            "Training Step: 9303  | total loss: \u001b[1m\u001b[32m0.57805\u001b[0m\u001b[0m | time: 1.233s\n",
            "| SGD | epoch: 812 | loss: 0.57805 - acc: 0.8354 -- iter: 2000/8000\n",
            "Training Step: 9304  | total loss: \u001b[1m\u001b[32m0.57753\u001b[0m\u001b[0m | time: 1.857s\n",
            "| SGD | epoch: 812 | loss: 0.57753 - acc: 0.8356 -- iter: 3000/8000\n",
            "Training Step: 9305  | total loss: \u001b[1m\u001b[32m0.57821\u001b[0m\u001b[0m | time: 2.472s\n",
            "| SGD | epoch: 812 | loss: 0.57821 - acc: 0.8361 -- iter: 4000/8000\n",
            "Training Step: 9306  | total loss: \u001b[1m\u001b[32m0.57713\u001b[0m\u001b[0m | time: 3.098s\n",
            "| SGD | epoch: 812 | loss: 0.57713 - acc: 0.8357 -- iter: 5000/8000\n",
            "Training Step: 9307  | total loss: \u001b[1m\u001b[32m0.57435\u001b[0m\u001b[0m | time: 3.726s\n",
            "| SGD | epoch: 812 | loss: 0.57435 - acc: 0.8366 -- iter: 6000/8000\n",
            "Training Step: 9308  | total loss: \u001b[1m\u001b[32m0.57643\u001b[0m\u001b[0m | time: 4.353s\n",
            "| SGD | epoch: 812 | loss: 0.57643 - acc: 0.8352 -- iter: 7000/8000\n",
            "Training Step: 9309  | total loss: \u001b[1m\u001b[32m0.57774\u001b[0m\u001b[0m | time: 4.986s\n",
            "| SGD | epoch: 812 | loss: 0.57774 - acc: 0.8350 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9310  | total loss: \u001b[1m\u001b[32m0.57978\u001b[0m\u001b[0m | time: 0.628s\n",
            "| SGD | epoch: 813 | loss: 0.57978 - acc: 0.8339 -- iter: 1000/8000\n",
            "Training Step: 9311  | total loss: \u001b[1m\u001b[32m0.57962\u001b[0m\u001b[0m | time: 1.258s\n",
            "| SGD | epoch: 813 | loss: 0.57962 - acc: 0.8344 -- iter: 2000/8000\n",
            "Training Step: 9312  | total loss: \u001b[1m\u001b[32m0.57720\u001b[0m\u001b[0m | time: 1.810s\n",
            "| SGD | epoch: 813 | loss: 0.57720 - acc: 0.8345 -- iter: 3000/8000\n",
            "Training Step: 9313  | total loss: \u001b[1m\u001b[32m0.57345\u001b[0m\u001b[0m | time: 2.361s\n",
            "| SGD | epoch: 813 | loss: 0.57345 - acc: 0.8350 -- iter: 4000/8000\n",
            "Training Step: 9314  | total loss: \u001b[1m\u001b[32m0.57551\u001b[0m\u001b[0m | time: 2.909s\n",
            "| SGD | epoch: 813 | loss: 0.57551 - acc: 0.8343 -- iter: 5000/8000\n",
            "Training Step: 9315  | total loss: \u001b[1m\u001b[32m0.57766\u001b[0m\u001b[0m | time: 3.451s\n",
            "| SGD | epoch: 813 | loss: 0.57766 - acc: 0.8341 -- iter: 6000/8000\n",
            "Training Step: 9316  | total loss: \u001b[1m\u001b[32m0.58050\u001b[0m\u001b[0m | time: 3.994s\n",
            "| SGD | epoch: 813 | loss: 0.58050 - acc: 0.8335 -- iter: 7000/8000\n",
            "Training Step: 9317  | total loss: \u001b[1m\u001b[32m0.57898\u001b[0m\u001b[0m | time: 4.536s\n",
            "| SGD | epoch: 813 | loss: 0.57898 - acc: 0.8340 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9318  | total loss: \u001b[1m\u001b[32m0.57653\u001b[0m\u001b[0m | time: 0.551s\n",
            "| SGD | epoch: 814 | loss: 0.57653 - acc: 0.8344 -- iter: 1000/8000\n",
            "Training Step: 9319  | total loss: \u001b[1m\u001b[32m0.57513\u001b[0m\u001b[0m | time: 1.096s\n",
            "| SGD | epoch: 814 | loss: 0.57513 - acc: 0.8361 -- iter: 2000/8000\n",
            "Training Step: 9320  | total loss: \u001b[1m\u001b[32m0.57808\u001b[0m\u001b[0m | time: 1.640s\n",
            "| SGD | epoch: 814 | loss: 0.57808 - acc: 0.8347 -- iter: 3000/8000\n",
            "Training Step: 9321  | total loss: \u001b[1m\u001b[32m0.57960\u001b[0m\u001b[0m | time: 2.184s\n",
            "| SGD | epoch: 814 | loss: 0.57960 - acc: 0.8329 -- iter: 4000/8000\n",
            "Training Step: 9322  | total loss: \u001b[1m\u001b[32m0.57829\u001b[0m\u001b[0m | time: 2.736s\n",
            "| SGD | epoch: 814 | loss: 0.57829 - acc: 0.8322 -- iter: 5000/8000\n",
            "Training Step: 9323  | total loss: \u001b[1m\u001b[32m0.57816\u001b[0m\u001b[0m | time: 3.281s\n",
            "| SGD | epoch: 814 | loss: 0.57816 - acc: 0.8335 -- iter: 6000/8000\n",
            "Training Step: 9324  | total loss: \u001b[1m\u001b[32m0.57954\u001b[0m\u001b[0m | time: 3.823s\n",
            "| SGD | epoch: 814 | loss: 0.57954 - acc: 0.8326 -- iter: 7000/8000\n",
            "Training Step: 9325  | total loss: \u001b[1m\u001b[32m0.58242\u001b[0m\u001b[0m | time: 4.375s\n",
            "| SGD | epoch: 814 | loss: 0.58242 - acc: 0.8324 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9326  | total loss: \u001b[1m\u001b[32m0.58105\u001b[0m\u001b[0m | time: 0.551s\n",
            "| SGD | epoch: 815 | loss: 0.58105 - acc: 0.8333 -- iter: 1000/8000\n",
            "Training Step: 9327  | total loss: \u001b[1m\u001b[32m0.57966\u001b[0m\u001b[0m | time: 1.091s\n",
            "| SGD | epoch: 815 | loss: 0.57966 - acc: 0.8354 -- iter: 2000/8000\n",
            "Training Step: 9328  | total loss: \u001b[1m\u001b[32m0.57882\u001b[0m\u001b[0m | time: 1.651s\n",
            "| SGD | epoch: 815 | loss: 0.57882 - acc: 0.8350 -- iter: 3000/8000\n",
            "Training Step: 9329  | total loss: \u001b[1m\u001b[32m0.57500\u001b[0m\u001b[0m | time: 2.221s\n",
            "| SGD | epoch: 815 | loss: 0.57500 - acc: 0.8375 -- iter: 4000/8000\n",
            "Training Step: 9330  | total loss: \u001b[1m\u001b[32m0.57250\u001b[0m\u001b[0m | time: 2.788s\n",
            "| SGD | epoch: 815 | loss: 0.57250 - acc: 0.8386 -- iter: 5000/8000\n",
            "Training Step: 9331  | total loss: \u001b[1m\u001b[32m0.57223\u001b[0m\u001b[0m | time: 3.364s\n",
            "| SGD | epoch: 815 | loss: 0.57223 - acc: 0.8391 -- iter: 6000/8000\n",
            "Training Step: 9332  | total loss: \u001b[1m\u001b[32m0.57432\u001b[0m\u001b[0m | time: 3.941s\n",
            "| SGD | epoch: 815 | loss: 0.57432 - acc: 0.8386 -- iter: 7000/8000\n",
            "Training Step: 9333  | total loss: \u001b[1m\u001b[32m0.57265\u001b[0m\u001b[0m | time: 4.519s\n",
            "| SGD | epoch: 815 | loss: 0.57265 - acc: 0.8391 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9334  | total loss: \u001b[1m\u001b[32m0.57386\u001b[0m\u001b[0m | time: 0.579s\n",
            "| SGD | epoch: 816 | loss: 0.57386 - acc: 0.8392 -- iter: 1000/8000\n",
            "Training Step: 9335  | total loss: \u001b[1m\u001b[32m0.57404\u001b[0m\u001b[0m | time: 1.151s\n",
            "| SGD | epoch: 816 | loss: 0.57404 - acc: 0.8392 -- iter: 2000/8000\n",
            "Training Step: 9336  | total loss: \u001b[1m\u001b[32m0.57268\u001b[0m\u001b[0m | time: 1.722s\n",
            "| SGD | epoch: 816 | loss: 0.57268 - acc: 0.8395 -- iter: 3000/8000\n",
            "Training Step: 9337  | total loss: \u001b[1m\u001b[32m0.57274\u001b[0m\u001b[0m | time: 2.295s\n",
            "| SGD | epoch: 816 | loss: 0.57274 - acc: 0.8395 -- iter: 4000/8000\n",
            "Training Step: 9338  | total loss: \u001b[1m\u001b[32m0.57643\u001b[0m\u001b[0m | time: 2.868s\n",
            "| SGD | epoch: 816 | loss: 0.57643 - acc: 0.8377 -- iter: 5000/8000\n",
            "Training Step: 9339  | total loss: \u001b[1m\u001b[32m0.58012\u001b[0m\u001b[0m | time: 3.426s\n",
            "| SGD | epoch: 816 | loss: 0.58012 - acc: 0.8368 -- iter: 6000/8000\n",
            "Training Step: 9340  | total loss: \u001b[1m\u001b[32m0.57666\u001b[0m\u001b[0m | time: 3.997s\n",
            "| SGD | epoch: 816 | loss: 0.57666 - acc: 0.8375 -- iter: 7000/8000\n",
            "Training Step: 9341  | total loss: \u001b[1m\u001b[32m0.57506\u001b[0m\u001b[0m | time: 4.562s\n",
            "| SGD | epoch: 816 | loss: 0.57506 - acc: 0.8386 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9342  | total loss: \u001b[1m\u001b[32m0.57543\u001b[0m\u001b[0m | time: 0.567s\n",
            "| SGD | epoch: 817 | loss: 0.57543 - acc: 0.8377 -- iter: 1000/8000\n",
            "Training Step: 9343  | total loss: \u001b[1m\u001b[32m0.57339\u001b[0m\u001b[0m | time: 1.140s\n",
            "| SGD | epoch: 817 | loss: 0.57339 - acc: 0.8383 -- iter: 2000/8000\n",
            "Training Step: 9344  | total loss: \u001b[1m\u001b[32m0.57532\u001b[0m\u001b[0m | time: 1.708s\n",
            "| SGD | epoch: 817 | loss: 0.57532 - acc: 0.8379 -- iter: 3000/8000\n",
            "Training Step: 9345  | total loss: \u001b[1m\u001b[32m0.57663\u001b[0m\u001b[0m | time: 2.267s\n",
            "| SGD | epoch: 817 | loss: 0.57663 - acc: 0.8384 -- iter: 4000/8000\n",
            "Training Step: 9346  | total loss: \u001b[1m\u001b[32m0.57678\u001b[0m\u001b[0m | time: 2.838s\n",
            "| SGD | epoch: 817 | loss: 0.57678 - acc: 0.8378 -- iter: 5000/8000\n",
            "Training Step: 9347  | total loss: \u001b[1m\u001b[32m0.57774\u001b[0m\u001b[0m | time: 3.405s\n",
            "| SGD | epoch: 817 | loss: 0.57774 - acc: 0.8366 -- iter: 6000/8000\n",
            "Training Step: 9348  | total loss: \u001b[1m\u001b[32m0.57884\u001b[0m\u001b[0m | time: 3.972s\n",
            "| SGD | epoch: 817 | loss: 0.57884 - acc: 0.8355 -- iter: 7000/8000\n",
            "Training Step: 9349  | total loss: \u001b[1m\u001b[32m0.57952\u001b[0m\u001b[0m | time: 4.536s\n",
            "| SGD | epoch: 817 | loss: 0.57952 - acc: 0.8361 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9350  | total loss: \u001b[1m\u001b[32m0.58105\u001b[0m\u001b[0m | time: 0.567s\n",
            "| SGD | epoch: 818 | loss: 0.58105 - acc: 0.8338 -- iter: 1000/8000\n",
            "Training Step: 9351  | total loss: \u001b[1m\u001b[32m0.58081\u001b[0m\u001b[0m | time: 1.137s\n",
            "| SGD | epoch: 818 | loss: 0.58081 - acc: 0.8342 -- iter: 2000/8000\n",
            "Training Step: 9352  | total loss: \u001b[1m\u001b[32m0.57999\u001b[0m\u001b[0m | time: 1.694s\n",
            "| SGD | epoch: 818 | loss: 0.57999 - acc: 0.8341 -- iter: 3000/8000\n",
            "Training Step: 9353  | total loss: \u001b[1m\u001b[32m0.57731\u001b[0m\u001b[0m | time: 2.255s\n",
            "| SGD | epoch: 818 | loss: 0.57731 - acc: 0.8350 -- iter: 4000/8000\n",
            "Training Step: 9354  | total loss: \u001b[1m\u001b[32m0.57399\u001b[0m\u001b[0m | time: 2.825s\n",
            "| SGD | epoch: 818 | loss: 0.57399 - acc: 0.8368 -- iter: 5000/8000\n",
            "Training Step: 9355  | total loss: \u001b[1m\u001b[32m0.57247\u001b[0m\u001b[0m | time: 3.393s\n",
            "| SGD | epoch: 818 | loss: 0.57247 - acc: 0.8372 -- iter: 6000/8000\n",
            "Training Step: 9356  | total loss: \u001b[1m\u001b[32m0.57127\u001b[0m\u001b[0m | time: 3.960s\n",
            "| SGD | epoch: 818 | loss: 0.57127 - acc: 0.8373 -- iter: 7000/8000\n",
            "Training Step: 9357  | total loss: \u001b[1m\u001b[32m0.56826\u001b[0m\u001b[0m | time: 4.530s\n",
            "| SGD | epoch: 818 | loss: 0.56826 - acc: 0.8386 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9358  | total loss: \u001b[1m\u001b[32m0.56647\u001b[0m\u001b[0m | time: 0.568s\n",
            "| SGD | epoch: 819 | loss: 0.56647 - acc: 0.8401 -- iter: 1000/8000\n",
            "Training Step: 9359  | total loss: \u001b[1m\u001b[32m0.56481\u001b[0m\u001b[0m | time: 1.139s\n",
            "| SGD | epoch: 819 | loss: 0.56481 - acc: 0.8409 -- iter: 2000/8000\n",
            "Training Step: 9360  | total loss: \u001b[1m\u001b[32m0.56946\u001b[0m\u001b[0m | time: 1.703s\n",
            "| SGD | epoch: 819 | loss: 0.56946 - acc: 0.8393 -- iter: 3000/8000\n",
            "Training Step: 9361  | total loss: \u001b[1m\u001b[32m0.57340\u001b[0m\u001b[0m | time: 2.275s\n",
            "| SGD | epoch: 819 | loss: 0.57340 - acc: 0.8368 -- iter: 4000/8000\n",
            "Training Step: 9362  | total loss: \u001b[1m\u001b[32m0.57515\u001b[0m\u001b[0m | time: 2.838s\n",
            "| SGD | epoch: 819 | loss: 0.57515 - acc: 0.8357 -- iter: 5000/8000\n",
            "Training Step: 9363  | total loss: \u001b[1m\u001b[32m0.57671\u001b[0m\u001b[0m | time: 3.407s\n",
            "| SGD | epoch: 819 | loss: 0.57671 - acc: 0.8346 -- iter: 6000/8000\n",
            "Training Step: 9364  | total loss: \u001b[1m\u001b[32m0.57695\u001b[0m\u001b[0m | time: 3.963s\n",
            "| SGD | epoch: 819 | loss: 0.57695 - acc: 0.8337 -- iter: 7000/8000\n",
            "Training Step: 9365  | total loss: \u001b[1m\u001b[32m0.57600\u001b[0m\u001b[0m | time: 4.535s\n",
            "| SGD | epoch: 819 | loss: 0.57600 - acc: 0.8349 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9366  | total loss: \u001b[1m\u001b[32m0.57372\u001b[0m\u001b[0m | time: 0.563s\n",
            "| SGD | epoch: 820 | loss: 0.57372 - acc: 0.8365 -- iter: 1000/8000\n",
            "Training Step: 9367  | total loss: \u001b[1m\u001b[32m0.57150\u001b[0m\u001b[0m | time: 1.120s\n",
            "| SGD | epoch: 820 | loss: 0.57150 - acc: 0.8357 -- iter: 2000/8000\n",
            "Training Step: 9368  | total loss: \u001b[1m\u001b[32m0.57381\u001b[0m\u001b[0m | time: 1.687s\n",
            "| SGD | epoch: 820 | loss: 0.57381 - acc: 0.8367 -- iter: 3000/8000\n",
            "Training Step: 9369  | total loss: \u001b[1m\u001b[32m0.57388\u001b[0m\u001b[0m | time: 2.255s\n",
            "| SGD | epoch: 820 | loss: 0.57388 - acc: 0.8369 -- iter: 4000/8000\n",
            "Training Step: 9370  | total loss: \u001b[1m\u001b[32m0.57277\u001b[0m\u001b[0m | time: 2.812s\n",
            "| SGD | epoch: 820 | loss: 0.57277 - acc: 0.8369 -- iter: 5000/8000\n",
            "Training Step: 9371  | total loss: \u001b[1m\u001b[32m0.57659\u001b[0m\u001b[0m | time: 3.379s\n",
            "| SGD | epoch: 820 | loss: 0.57659 - acc: 0.8367 -- iter: 6000/8000\n",
            "Training Step: 9372  | total loss: \u001b[1m\u001b[32m0.57978\u001b[0m\u001b[0m | time: 3.950s\n",
            "| SGD | epoch: 820 | loss: 0.57978 - acc: 0.8364 -- iter: 7000/8000\n",
            "Training Step: 9373  | total loss: \u001b[1m\u001b[32m0.57991\u001b[0m\u001b[0m | time: 4.520s\n",
            "| SGD | epoch: 820 | loss: 0.57991 - acc: 0.8350 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9374  | total loss: \u001b[1m\u001b[32m0.57939\u001b[0m\u001b[0m | time: 0.566s\n",
            "| SGD | epoch: 821 | loss: 0.57939 - acc: 0.8350 -- iter: 1000/8000\n",
            "Training Step: 9375  | total loss: \u001b[1m\u001b[32m0.57685\u001b[0m\u001b[0m | time: 1.134s\n",
            "| SGD | epoch: 821 | loss: 0.57685 - acc: 0.8365 -- iter: 2000/8000\n",
            "Training Step: 9376  | total loss: \u001b[1m\u001b[32m0.57466\u001b[0m\u001b[0m | time: 1.703s\n",
            "| SGD | epoch: 821 | loss: 0.57466 - acc: 0.8372 -- iter: 3000/8000\n",
            "Training Step: 9377  | total loss: \u001b[1m\u001b[32m0.57354\u001b[0m\u001b[0m | time: 2.269s\n",
            "| SGD | epoch: 821 | loss: 0.57354 - acc: 0.8375 -- iter: 4000/8000\n",
            "Training Step: 9378  | total loss: \u001b[1m\u001b[32m0.57133\u001b[0m\u001b[0m | time: 2.839s\n",
            "| SGD | epoch: 821 | loss: 0.57133 - acc: 0.8378 -- iter: 5000/8000\n",
            "Training Step: 9379  | total loss: \u001b[1m\u001b[32m0.57779\u001b[0m\u001b[0m | time: 3.414s\n",
            "| SGD | epoch: 821 | loss: 0.57779 - acc: 0.8346 -- iter: 6000/8000\n",
            "Training Step: 9380  | total loss: \u001b[1m\u001b[32m0.57839\u001b[0m\u001b[0m | time: 4.011s\n",
            "| SGD | epoch: 821 | loss: 0.57839 - acc: 0.8340 -- iter: 7000/8000\n",
            "Training Step: 9381  | total loss: \u001b[1m\u001b[32m0.57852\u001b[0m\u001b[0m | time: 4.569s\n",
            "| SGD | epoch: 821 | loss: 0.57852 - acc: 0.8344 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9382  | total loss: \u001b[1m\u001b[32m0.57699\u001b[0m\u001b[0m | time: 0.564s\n",
            "| SGD | epoch: 822 | loss: 0.57699 - acc: 0.8346 -- iter: 1000/8000\n",
            "Training Step: 9383  | total loss: \u001b[1m\u001b[32m0.57521\u001b[0m\u001b[0m | time: 1.136s\n",
            "| SGD | epoch: 822 | loss: 0.57521 - acc: 0.8375 -- iter: 2000/8000\n",
            "Training Step: 9384  | total loss: \u001b[1m\u001b[32m0.57438\u001b[0m\u001b[0m | time: 1.704s\n",
            "| SGD | epoch: 822 | loss: 0.57438 - acc: 0.8384 -- iter: 3000/8000\n",
            "Training Step: 9385  | total loss: \u001b[1m\u001b[32m0.57418\u001b[0m\u001b[0m | time: 2.273s\n",
            "| SGD | epoch: 822 | loss: 0.57418 - acc: 0.8390 -- iter: 4000/8000\n",
            "Training Step: 9386  | total loss: \u001b[1m\u001b[32m0.57294\u001b[0m\u001b[0m | time: 2.845s\n",
            "| SGD | epoch: 822 | loss: 0.57294 - acc: 0.8392 -- iter: 5000/8000\n",
            "Training Step: 9387  | total loss: \u001b[1m\u001b[32m0.57333\u001b[0m\u001b[0m | time: 3.417s\n",
            "| SGD | epoch: 822 | loss: 0.57333 - acc: 0.8383 -- iter: 6000/8000\n",
            "Training Step: 9388  | total loss: \u001b[1m\u001b[32m0.57548\u001b[0m\u001b[0m | time: 3.985s\n",
            "| SGD | epoch: 822 | loss: 0.57548 - acc: 0.8374 -- iter: 7000/8000\n",
            "Training Step: 9389  | total loss: \u001b[1m\u001b[32m0.57336\u001b[0m\u001b[0m | time: 4.546s\n",
            "| SGD | epoch: 822 | loss: 0.57336 - acc: 0.8374 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9390  | total loss: \u001b[1m\u001b[32m0.57119\u001b[0m\u001b[0m | time: 0.567s\n",
            "| SGD | epoch: 823 | loss: 0.57119 - acc: 0.8367 -- iter: 1000/8000\n",
            "Training Step: 9391  | total loss: \u001b[1m\u001b[32m0.57137\u001b[0m\u001b[0m | time: 1.143s\n",
            "| SGD | epoch: 823 | loss: 0.57137 - acc: 0.8368 -- iter: 2000/8000\n",
            "Training Step: 9392  | total loss: \u001b[1m\u001b[32m0.57075\u001b[0m\u001b[0m | time: 1.711s\n",
            "| SGD | epoch: 823 | loss: 0.57075 - acc: 0.8373 -- iter: 3000/8000\n",
            "Training Step: 9393  | total loss: \u001b[1m\u001b[32m0.57278\u001b[0m\u001b[0m | time: 2.284s\n",
            "| SGD | epoch: 823 | loss: 0.57278 - acc: 0.8366 -- iter: 4000/8000\n",
            "Training Step: 9394  | total loss: \u001b[1m\u001b[32m0.57243\u001b[0m\u001b[0m | time: 2.855s\n",
            "| SGD | epoch: 823 | loss: 0.57243 - acc: 0.8361 -- iter: 5000/8000\n",
            "Training Step: 9395  | total loss: \u001b[1m\u001b[32m0.56880\u001b[0m\u001b[0m | time: 3.419s\n",
            "| SGD | epoch: 823 | loss: 0.56880 - acc: 0.8380 -- iter: 6000/8000\n",
            "Training Step: 9396  | total loss: \u001b[1m\u001b[32m0.56807\u001b[0m\u001b[0m | time: 4.002s\n",
            "| SGD | epoch: 823 | loss: 0.56807 - acc: 0.8373 -- iter: 7000/8000\n",
            "Training Step: 9397  | total loss: \u001b[1m\u001b[32m0.56685\u001b[0m\u001b[0m | time: 4.567s\n",
            "| SGD | epoch: 823 | loss: 0.56685 - acc: 0.8390 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9398  | total loss: \u001b[1m\u001b[32m0.56662\u001b[0m\u001b[0m | time: 0.577s\n",
            "| SGD | epoch: 824 | loss: 0.56662 - acc: 0.8400 -- iter: 1000/8000\n",
            "Training Step: 9399  | total loss: \u001b[1m\u001b[32m0.56675\u001b[0m\u001b[0m | time: 1.141s\n",
            "| SGD | epoch: 824 | loss: 0.56675 - acc: 0.8403 -- iter: 2000/8000\n",
            "Training Step: 9400  | total loss: \u001b[1m\u001b[32m0.56930\u001b[0m\u001b[0m | time: 1.704s\n",
            "| SGD | epoch: 824 | loss: 0.56930 - acc: 0.8412 -- iter: 3000/8000\n",
            "Training Step: 9401  | total loss: \u001b[1m\u001b[32m0.57169\u001b[0m\u001b[0m | time: 2.283s\n",
            "| SGD | epoch: 824 | loss: 0.57169 - acc: 0.8401 -- iter: 4000/8000\n",
            "Training Step: 9402  | total loss: \u001b[1m\u001b[32m0.57480\u001b[0m\u001b[0m | time: 2.842s\n",
            "| SGD | epoch: 824 | loss: 0.57480 - acc: 0.8387 -- iter: 5000/8000\n",
            "Training Step: 9403  | total loss: \u001b[1m\u001b[32m0.57560\u001b[0m\u001b[0m | time: 3.421s\n",
            "| SGD | epoch: 824 | loss: 0.57560 - acc: 0.8393 -- iter: 6000/8000\n",
            "Training Step: 9404  | total loss: \u001b[1m\u001b[32m0.57479\u001b[0m\u001b[0m | time: 3.987s\n",
            "| SGD | epoch: 824 | loss: 0.57479 - acc: 0.8383 -- iter: 7000/8000\n",
            "Training Step: 9405  | total loss: \u001b[1m\u001b[32m0.57778\u001b[0m\u001b[0m | time: 4.565s\n",
            "| SGD | epoch: 824 | loss: 0.57778 - acc: 0.8363 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9406  | total loss: \u001b[1m\u001b[32m0.57270\u001b[0m\u001b[0m | time: 0.572s\n",
            "| SGD | epoch: 825 | loss: 0.57270 - acc: 0.8392 -- iter: 1000/8000\n",
            "Training Step: 9407  | total loss: \u001b[1m\u001b[32m0.57438\u001b[0m\u001b[0m | time: 1.150s\n",
            "| SGD | epoch: 825 | loss: 0.57438 - acc: 0.8388 -- iter: 2000/8000\n",
            "Training Step: 9408  | total loss: \u001b[1m\u001b[32m0.57525\u001b[0m\u001b[0m | time: 1.731s\n",
            "| SGD | epoch: 825 | loss: 0.57525 - acc: 0.8378 -- iter: 3000/8000\n",
            "Training Step: 9409  | total loss: \u001b[1m\u001b[32m0.57133\u001b[0m\u001b[0m | time: 2.367s\n",
            "| SGD | epoch: 825 | loss: 0.57133 - acc: 0.8393 -- iter: 4000/8000\n",
            "Training Step: 9410  | total loss: \u001b[1m\u001b[32m0.57087\u001b[0m\u001b[0m | time: 2.987s\n",
            "| SGD | epoch: 825 | loss: 0.57087 - acc: 0.8393 -- iter: 5000/8000\n",
            "Training Step: 9411  | total loss: \u001b[1m\u001b[32m0.56937\u001b[0m\u001b[0m | time: 3.606s\n",
            "| SGD | epoch: 825 | loss: 0.56937 - acc: 0.8392 -- iter: 6000/8000\n",
            "Training Step: 9412  | total loss: \u001b[1m\u001b[32m0.56996\u001b[0m\u001b[0m | time: 4.222s\n",
            "| SGD | epoch: 825 | loss: 0.56996 - acc: 0.8390 -- iter: 7000/8000\n",
            "Training Step: 9413  | total loss: \u001b[1m\u001b[32m0.56560\u001b[0m\u001b[0m | time: 4.833s\n",
            "| SGD | epoch: 825 | loss: 0.56560 - acc: 0.8409 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9414  | total loss: \u001b[1m\u001b[32m0.56807\u001b[0m\u001b[0m | time: 0.620s\n",
            "| SGD | epoch: 826 | loss: 0.56807 - acc: 0.8414 -- iter: 1000/8000\n",
            "Training Step: 9415  | total loss: \u001b[1m\u001b[32m0.56771\u001b[0m\u001b[0m | time: 1.233s\n",
            "| SGD | epoch: 826 | loss: 0.56771 - acc: 0.8421 -- iter: 2000/8000\n",
            "Training Step: 9416  | total loss: \u001b[1m\u001b[32m0.56715\u001b[0m\u001b[0m | time: 1.848s\n",
            "| SGD | epoch: 826 | loss: 0.56715 - acc: 0.8424 -- iter: 3000/8000\n",
            "Training Step: 9417  | total loss: \u001b[1m\u001b[32m0.56658\u001b[0m\u001b[0m | time: 2.464s\n",
            "| SGD | epoch: 826 | loss: 0.56658 - acc: 0.8425 -- iter: 4000/8000\n",
            "Training Step: 9418  | total loss: \u001b[1m\u001b[32m0.57046\u001b[0m\u001b[0m | time: 3.077s\n",
            "| SGD | epoch: 826 | loss: 0.57046 - acc: 0.8391 -- iter: 5000/8000\n",
            "Training Step: 9419  | total loss: \u001b[1m\u001b[32m0.57174\u001b[0m\u001b[0m | time: 3.696s\n",
            "| SGD | epoch: 826 | loss: 0.57174 - acc: 0.8387 -- iter: 6000/8000\n",
            "Training Step: 9420  | total loss: \u001b[1m\u001b[32m0.57250\u001b[0m\u001b[0m | time: 4.309s\n",
            "| SGD | epoch: 826 | loss: 0.57250 - acc: 0.8392 -- iter: 7000/8000\n",
            "Training Step: 9421  | total loss: \u001b[1m\u001b[32m0.56886\u001b[0m\u001b[0m | time: 4.918s\n",
            "| SGD | epoch: 826 | loss: 0.56886 - acc: 0.8405 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9422  | total loss: \u001b[1m\u001b[32m0.56757\u001b[0m\u001b[0m | time: 0.630s\n",
            "| SGD | epoch: 827 | loss: 0.56757 - acc: 0.8419 -- iter: 1000/8000\n",
            "Training Step: 9423  | total loss: \u001b[1m\u001b[32m0.57230\u001b[0m\u001b[0m | time: 1.263s\n",
            "| SGD | epoch: 827 | loss: 0.57230 - acc: 0.8393 -- iter: 2000/8000\n",
            "Training Step: 9424  | total loss: \u001b[1m\u001b[32m0.57212\u001b[0m\u001b[0m | time: 1.893s\n",
            "| SGD | epoch: 827 | loss: 0.57212 - acc: 0.8386 -- iter: 3000/8000\n",
            "Training Step: 9425  | total loss: \u001b[1m\u001b[32m0.57029\u001b[0m\u001b[0m | time: 2.524s\n",
            "| SGD | epoch: 827 | loss: 0.57029 - acc: 0.8382 -- iter: 4000/8000\n",
            "Training Step: 9426  | total loss: \u001b[1m\u001b[32m0.56850\u001b[0m\u001b[0m | time: 3.162s\n",
            "| SGD | epoch: 827 | loss: 0.56850 - acc: 0.8380 -- iter: 5000/8000\n",
            "Training Step: 9427  | total loss: \u001b[1m\u001b[32m0.56887\u001b[0m\u001b[0m | time: 3.801s\n",
            "| SGD | epoch: 827 | loss: 0.56887 - acc: 0.8359 -- iter: 6000/8000\n",
            "Training Step: 9428  | total loss: \u001b[1m\u001b[32m0.56769\u001b[0m\u001b[0m | time: 4.447s\n",
            "| SGD | epoch: 827 | loss: 0.56769 - acc: 0.8374 -- iter: 7000/8000\n",
            "Training Step: 9429  | total loss: \u001b[1m\u001b[32m0.56803\u001b[0m\u001b[0m | time: 5.090s\n",
            "| SGD | epoch: 827 | loss: 0.56803 - acc: 0.8372 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9430  | total loss: \u001b[1m\u001b[32m0.56752\u001b[0m\u001b[0m | time: 0.647s\n",
            "| SGD | epoch: 828 | loss: 0.56752 - acc: 0.8372 -- iter: 1000/8000\n",
            "Training Step: 9431  | total loss: \u001b[1m\u001b[32m0.56688\u001b[0m\u001b[0m | time: 1.294s\n",
            "| SGD | epoch: 828 | loss: 0.56688 - acc: 0.8377 -- iter: 2000/8000\n",
            "Training Step: 9432  | total loss: \u001b[1m\u001b[32m0.56573\u001b[0m\u001b[0m | time: 1.939s\n",
            "| SGD | epoch: 828 | loss: 0.56573 - acc: 0.8381 -- iter: 3000/8000\n",
            "Training Step: 9433  | total loss: \u001b[1m\u001b[32m0.56648\u001b[0m\u001b[0m | time: 2.578s\n",
            "| SGD | epoch: 828 | loss: 0.56648 - acc: 0.8400 -- iter: 4000/8000\n",
            "Training Step: 9434  | total loss: \u001b[1m\u001b[32m0.56713\u001b[0m\u001b[0m | time: 3.216s\n",
            "| SGD | epoch: 828 | loss: 0.56713 - acc: 0.8396 -- iter: 5000/8000\n",
            "Training Step: 9435  | total loss: \u001b[1m\u001b[32m0.56740\u001b[0m\u001b[0m | time: 3.862s\n",
            "| SGD | epoch: 828 | loss: 0.56740 - acc: 0.8397 -- iter: 6000/8000\n",
            "Training Step: 9436  | total loss: \u001b[1m\u001b[32m0.56725\u001b[0m\u001b[0m | time: 4.501s\n",
            "| SGD | epoch: 828 | loss: 0.56725 - acc: 0.8379 -- iter: 7000/8000\n",
            "Training Step: 9437  | total loss: \u001b[1m\u001b[32m0.56937\u001b[0m\u001b[0m | time: 5.141s\n",
            "| SGD | epoch: 828 | loss: 0.56937 - acc: 0.8381 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9438  | total loss: \u001b[1m\u001b[32m0.56906\u001b[0m\u001b[0m | time: 0.639s\n",
            "| SGD | epoch: 829 | loss: 0.56906 - acc: 0.8386 -- iter: 1000/8000\n",
            "Training Step: 9439  | total loss: \u001b[1m\u001b[32m0.56855\u001b[0m\u001b[0m | time: 1.277s\n",
            "| SGD | epoch: 829 | loss: 0.56855 - acc: 0.8392 -- iter: 2000/8000\n",
            "Training Step: 9440  | total loss: \u001b[1m\u001b[32m0.56688\u001b[0m\u001b[0m | time: 1.918s\n",
            "| SGD | epoch: 829 | loss: 0.56688 - acc: 0.8394 -- iter: 3000/8000\n",
            "Training Step: 9441  | total loss: \u001b[1m\u001b[32m0.56616\u001b[0m\u001b[0m | time: 2.561s\n",
            "| SGD | epoch: 829 | loss: 0.56616 - acc: 0.8395 -- iter: 4000/8000\n",
            "Training Step: 9442  | total loss: \u001b[1m\u001b[32m0.56664\u001b[0m\u001b[0m | time: 3.195s\n",
            "| SGD | epoch: 829 | loss: 0.56664 - acc: 0.8398 -- iter: 5000/8000\n",
            "Training Step: 9443  | total loss: \u001b[1m\u001b[32m0.56816\u001b[0m\u001b[0m | time: 3.823s\n",
            "| SGD | epoch: 829 | loss: 0.56816 - acc: 0.8397 -- iter: 6000/8000\n",
            "Training Step: 9444  | total loss: \u001b[1m\u001b[32m0.56952\u001b[0m\u001b[0m | time: 4.455s\n",
            "| SGD | epoch: 829 | loss: 0.56952 - acc: 0.8388 -- iter: 7000/8000\n",
            "Training Step: 9445  | total loss: \u001b[1m\u001b[32m0.56791\u001b[0m\u001b[0m | time: 5.087s\n",
            "| SGD | epoch: 829 | loss: 0.56791 - acc: 0.8398 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9446  | total loss: \u001b[1m\u001b[32m0.57129\u001b[0m\u001b[0m | time: 0.630s\n",
            "| SGD | epoch: 830 | loss: 0.57129 - acc: 0.8380 -- iter: 1000/8000\n",
            "Training Step: 9447  | total loss: \u001b[1m\u001b[32m0.57306\u001b[0m\u001b[0m | time: 1.267s\n",
            "| SGD | epoch: 830 | loss: 0.57306 - acc: 0.8372 -- iter: 2000/8000\n",
            "Training Step: 9448  | total loss: \u001b[1m\u001b[32m0.57304\u001b[0m\u001b[0m | time: 1.905s\n",
            "| SGD | epoch: 830 | loss: 0.57304 - acc: 0.8376 -- iter: 3000/8000\n",
            "Training Step: 9449  | total loss: \u001b[1m\u001b[32m0.57286\u001b[0m\u001b[0m | time: 2.528s\n",
            "| SGD | epoch: 830 | loss: 0.57286 - acc: 0.8379 -- iter: 4000/8000\n",
            "Training Step: 9450  | total loss: \u001b[1m\u001b[32m0.56830\u001b[0m\u001b[0m | time: 3.148s\n",
            "| SGD | epoch: 830 | loss: 0.56830 - acc: 0.8405 -- iter: 5000/8000\n",
            "Training Step: 9451  | total loss: \u001b[1m\u001b[32m0.56723\u001b[0m\u001b[0m | time: 3.773s\n",
            "| SGD | epoch: 830 | loss: 0.56723 - acc: 0.8401 -- iter: 6000/8000\n",
            "Training Step: 9452  | total loss: \u001b[1m\u001b[32m0.56843\u001b[0m\u001b[0m | time: 4.398s\n",
            "| SGD | epoch: 830 | loss: 0.56843 - acc: 0.8403 -- iter: 7000/8000\n",
            "Training Step: 9453  | total loss: \u001b[1m\u001b[32m0.56891\u001b[0m\u001b[0m | time: 5.021s\n",
            "| SGD | epoch: 830 | loss: 0.56891 - acc: 0.8423 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9454  | total loss: \u001b[1m\u001b[32m0.56947\u001b[0m\u001b[0m | time: 0.624s\n",
            "| SGD | epoch: 831 | loss: 0.56947 - acc: 0.8398 -- iter: 1000/8000\n",
            "Training Step: 9455  | total loss: \u001b[1m\u001b[32m0.57027\u001b[0m\u001b[0m | time: 1.241s\n",
            "| SGD | epoch: 831 | loss: 0.57027 - acc: 0.8394 -- iter: 2000/8000\n",
            "Training Step: 9456  | total loss: \u001b[1m\u001b[32m0.56870\u001b[0m\u001b[0m | time: 1.860s\n",
            "| SGD | epoch: 831 | loss: 0.56870 - acc: 0.8401 -- iter: 3000/8000\n",
            "Training Step: 9457  | total loss: \u001b[1m\u001b[32m0.56834\u001b[0m\u001b[0m | time: 2.491s\n",
            "| SGD | epoch: 831 | loss: 0.56834 - acc: 0.8418 -- iter: 4000/8000\n",
            "Training Step: 9458  | total loss: \u001b[1m\u001b[32m0.56778\u001b[0m\u001b[0m | time: 3.120s\n",
            "| SGD | epoch: 831 | loss: 0.56778 - acc: 0.8403 -- iter: 5000/8000\n",
            "Training Step: 9459  | total loss: \u001b[1m\u001b[32m0.56925\u001b[0m\u001b[0m | time: 3.751s\n",
            "| SGD | epoch: 831 | loss: 0.56925 - acc: 0.8396 -- iter: 6000/8000\n",
            "Training Step: 9460  | total loss: \u001b[1m\u001b[32m0.56742\u001b[0m\u001b[0m | time: 4.384s\n",
            "| SGD | epoch: 831 | loss: 0.56742 - acc: 0.8392 -- iter: 7000/8000\n",
            "Training Step: 9461  | total loss: \u001b[1m\u001b[32m0.56770\u001b[0m\u001b[0m | time: 4.993s\n",
            "| SGD | epoch: 831 | loss: 0.56770 - acc: 0.8407 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9462  | total loss: \u001b[1m\u001b[32m0.56808\u001b[0m\u001b[0m | time: 0.627s\n",
            "| SGD | epoch: 832 | loss: 0.56808 - acc: 0.8409 -- iter: 1000/8000\n",
            "Training Step: 9463  | total loss: \u001b[1m\u001b[32m0.56762\u001b[0m\u001b[0m | time: 1.262s\n",
            "| SGD | epoch: 832 | loss: 0.56762 - acc: 0.8415 -- iter: 2000/8000\n",
            "Training Step: 9464  | total loss: \u001b[1m\u001b[32m0.56860\u001b[0m\u001b[0m | time: 1.891s\n",
            "| SGD | epoch: 832 | loss: 0.56860 - acc: 0.8400 -- iter: 3000/8000\n",
            "Training Step: 9465  | total loss: \u001b[1m\u001b[32m0.56993\u001b[0m\u001b[0m | time: 2.523s\n",
            "| SGD | epoch: 832 | loss: 0.56993 - acc: 0.8411 -- iter: 4000/8000\n",
            "Training Step: 9466  | total loss: \u001b[1m\u001b[32m0.56970\u001b[0m\u001b[0m | time: 3.153s\n",
            "| SGD | epoch: 832 | loss: 0.56970 - acc: 0.8410 -- iter: 5000/8000\n",
            "Training Step: 9467  | total loss: \u001b[1m\u001b[32m0.56727\u001b[0m\u001b[0m | time: 3.783s\n",
            "| SGD | epoch: 832 | loss: 0.56727 - acc: 0.8416 -- iter: 6000/8000\n",
            "Training Step: 9468  | total loss: \u001b[1m\u001b[32m0.56895\u001b[0m\u001b[0m | time: 4.414s\n",
            "| SGD | epoch: 832 | loss: 0.56895 - acc: 0.8417 -- iter: 7000/8000\n",
            "Training Step: 9469  | total loss: \u001b[1m\u001b[32m0.56733\u001b[0m\u001b[0m | time: 5.046s\n",
            "| SGD | epoch: 832 | loss: 0.56733 - acc: 0.8436 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9470  | total loss: \u001b[1m\u001b[32m0.56618\u001b[0m\u001b[0m | time: 0.640s\n",
            "| SGD | epoch: 833 | loss: 0.56618 - acc: 0.8437 -- iter: 1000/8000\n",
            "Training Step: 9471  | total loss: \u001b[1m\u001b[32m0.56494\u001b[0m\u001b[0m | time: 1.281s\n",
            "| SGD | epoch: 833 | loss: 0.56494 - acc: 0.8441 -- iter: 2000/8000\n",
            "Training Step: 9472  | total loss: \u001b[1m\u001b[32m0.56502\u001b[0m\u001b[0m | time: 1.919s\n",
            "| SGD | epoch: 833 | loss: 0.56502 - acc: 0.8434 -- iter: 3000/8000\n",
            "Training Step: 9473  | total loss: \u001b[1m\u001b[32m0.56148\u001b[0m\u001b[0m | time: 2.560s\n",
            "| SGD | epoch: 833 | loss: 0.56148 - acc: 0.8450 -- iter: 4000/8000\n",
            "Training Step: 9474  | total loss: \u001b[1m\u001b[32m0.56031\u001b[0m\u001b[0m | time: 3.204s\n",
            "| SGD | epoch: 833 | loss: 0.56031 - acc: 0.8458 -- iter: 5000/8000\n",
            "Training Step: 9475  | total loss: \u001b[1m\u001b[32m0.55957\u001b[0m\u001b[0m | time: 3.845s\n",
            "| SGD | epoch: 833 | loss: 0.55957 - acc: 0.8461 -- iter: 6000/8000\n",
            "Training Step: 9476  | total loss: \u001b[1m\u001b[32m0.55965\u001b[0m\u001b[0m | time: 4.482s\n",
            "| SGD | epoch: 833 | loss: 0.55965 - acc: 0.8451 -- iter: 7000/8000\n",
            "Training Step: 9477  | total loss: \u001b[1m\u001b[32m0.55955\u001b[0m\u001b[0m | time: 5.109s\n",
            "| SGD | epoch: 833 | loss: 0.55955 - acc: 0.8455 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9478  | total loss: \u001b[1m\u001b[32m0.55934\u001b[0m\u001b[0m | time: 0.630s\n",
            "| SGD | epoch: 834 | loss: 0.55934 - acc: 0.8441 -- iter: 1000/8000\n",
            "Training Step: 9479  | total loss: \u001b[1m\u001b[32m0.56182\u001b[0m\u001b[0m | time: 1.257s\n",
            "| SGD | epoch: 834 | loss: 0.56182 - acc: 0.8418 -- iter: 2000/8000\n",
            "Training Step: 9480  | total loss: \u001b[1m\u001b[32m0.56368\u001b[0m\u001b[0m | time: 1.895s\n",
            "| SGD | epoch: 834 | loss: 0.56368 - acc: 0.8411 -- iter: 3000/8000\n",
            "Training Step: 9481  | total loss: \u001b[1m\u001b[32m0.56538\u001b[0m\u001b[0m | time: 2.531s\n",
            "| SGD | epoch: 834 | loss: 0.56538 - acc: 0.8406 -- iter: 4000/8000\n",
            "Training Step: 9482  | total loss: \u001b[1m\u001b[32m0.56464\u001b[0m\u001b[0m | time: 3.148s\n",
            "| SGD | epoch: 834 | loss: 0.56464 - acc: 0.8427 -- iter: 5000/8000\n",
            "Training Step: 9483  | total loss: \u001b[1m\u001b[32m0.56578\u001b[0m\u001b[0m | time: 3.763s\n",
            "| SGD | epoch: 834 | loss: 0.56578 - acc: 0.8410 -- iter: 6000/8000\n",
            "Training Step: 9484  | total loss: \u001b[1m\u001b[32m0.56813\u001b[0m\u001b[0m | time: 4.379s\n",
            "| SGD | epoch: 834 | loss: 0.56813 - acc: 0.8403 -- iter: 7000/8000\n",
            "Training Step: 9485  | total loss: \u001b[1m\u001b[32m0.57317\u001b[0m\u001b[0m | time: 4.991s\n",
            "| SGD | epoch: 834 | loss: 0.57317 - acc: 0.8379 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9486  | total loss: \u001b[1m\u001b[32m0.57230\u001b[0m\u001b[0m | time: 0.615s\n",
            "| SGD | epoch: 835 | loss: 0.57230 - acc: 0.8380 -- iter: 1000/8000\n",
            "Training Step: 9487  | total loss: \u001b[1m\u001b[32m0.56680\u001b[0m\u001b[0m | time: 1.230s\n",
            "| SGD | epoch: 835 | loss: 0.56680 - acc: 0.8386 -- iter: 2000/8000\n",
            "Training Step: 9488  | total loss: \u001b[1m\u001b[32m0.56675\u001b[0m\u001b[0m | time: 1.847s\n",
            "| SGD | epoch: 835 | loss: 0.56675 - acc: 0.8381 -- iter: 3000/8000\n",
            "Training Step: 9489  | total loss: \u001b[1m\u001b[32m0.56562\u001b[0m\u001b[0m | time: 2.464s\n",
            "| SGD | epoch: 835 | loss: 0.56562 - acc: 0.8392 -- iter: 4000/8000\n",
            "Training Step: 9490  | total loss: \u001b[1m\u001b[32m0.56172\u001b[0m\u001b[0m | time: 3.102s\n",
            "| SGD | epoch: 835 | loss: 0.56172 - acc: 0.8417 -- iter: 5000/8000\n",
            "Training Step: 9491  | total loss: \u001b[1m\u001b[32m0.56283\u001b[0m\u001b[0m | time: 3.723s\n",
            "| SGD | epoch: 835 | loss: 0.56283 - acc: 0.8419 -- iter: 6000/8000\n",
            "Training Step: 9492  | total loss: \u001b[1m\u001b[32m0.56433\u001b[0m\u001b[0m | time: 4.344s\n",
            "| SGD | epoch: 835 | loss: 0.56433 - acc: 0.8419 -- iter: 7000/8000\n",
            "Training Step: 9493  | total loss: \u001b[1m\u001b[32m0.56018\u001b[0m\u001b[0m | time: 4.967s\n",
            "| SGD | epoch: 835 | loss: 0.56018 - acc: 0.8432 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9494  | total loss: \u001b[1m\u001b[32m0.56018\u001b[0m\u001b[0m | time: 0.625s\n",
            "| SGD | epoch: 836 | loss: 0.56018 - acc: 0.8419 -- iter: 1000/8000\n",
            "Training Step: 9495  | total loss: \u001b[1m\u001b[32m0.56113\u001b[0m\u001b[0m | time: 1.253s\n",
            "| SGD | epoch: 836 | loss: 0.56113 - acc: 0.8418 -- iter: 2000/8000\n",
            "Training Step: 9496  | total loss: \u001b[1m\u001b[32m0.56256\u001b[0m\u001b[0m | time: 1.879s\n",
            "| SGD | epoch: 836 | loss: 0.56256 - acc: 0.8425 -- iter: 3000/8000\n",
            "Training Step: 9497  | total loss: \u001b[1m\u001b[32m0.56086\u001b[0m\u001b[0m | time: 2.503s\n",
            "| SGD | epoch: 836 | loss: 0.56086 - acc: 0.8423 -- iter: 4000/8000\n",
            "Training Step: 9498  | total loss: \u001b[1m\u001b[32m0.55915\u001b[0m\u001b[0m | time: 3.128s\n",
            "| SGD | epoch: 836 | loss: 0.55915 - acc: 0.8426 -- iter: 5000/8000\n",
            "Training Step: 9499  | total loss: \u001b[1m\u001b[32m0.56104\u001b[0m\u001b[0m | time: 3.744s\n",
            "| SGD | epoch: 836 | loss: 0.56104 - acc: 0.8417 -- iter: 6000/8000\n",
            "Training Step: 9500  | total loss: \u001b[1m\u001b[32m0.56336\u001b[0m\u001b[0m | time: 4.382s\n",
            "| SGD | epoch: 836 | loss: 0.56336 - acc: 0.8421 -- iter: 7000/8000\n",
            "Training Step: 9501  | total loss: \u001b[1m\u001b[32m0.56137\u001b[0m\u001b[0m | time: 4.996s\n",
            "| SGD | epoch: 836 | loss: 0.56137 - acc: 0.8425 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9502  | total loss: \u001b[1m\u001b[32m0.56114\u001b[0m\u001b[0m | time: 0.612s\n",
            "| SGD | epoch: 837 | loss: 0.56114 - acc: 0.8434 -- iter: 1000/8000\n",
            "Training Step: 9503  | total loss: \u001b[1m\u001b[32m0.56017\u001b[0m\u001b[0m | time: 1.243s\n",
            "| SGD | epoch: 837 | loss: 0.56017 - acc: 0.8436 -- iter: 2000/8000\n",
            "Training Step: 9504  | total loss: \u001b[1m\u001b[32m0.56012\u001b[0m\u001b[0m | time: 1.870s\n",
            "| SGD | epoch: 837 | loss: 0.56012 - acc: 0.8449 -- iter: 3000/8000\n",
            "Training Step: 9505  | total loss: \u001b[1m\u001b[32m0.56149\u001b[0m\u001b[0m | time: 2.501s\n",
            "| SGD | epoch: 837 | loss: 0.56149 - acc: 0.8437 -- iter: 4000/8000\n",
            "Training Step: 9506  | total loss: \u001b[1m\u001b[32m0.56112\u001b[0m\u001b[0m | time: 3.129s\n",
            "| SGD | epoch: 837 | loss: 0.56112 - acc: 0.8438 -- iter: 5000/8000\n",
            "Training Step: 9507  | total loss: \u001b[1m\u001b[32m0.56013\u001b[0m\u001b[0m | time: 3.759s\n",
            "| SGD | epoch: 837 | loss: 0.56013 - acc: 0.8441 -- iter: 6000/8000\n",
            "Training Step: 9508  | total loss: \u001b[1m\u001b[32m0.56136\u001b[0m\u001b[0m | time: 4.391s\n",
            "| SGD | epoch: 837 | loss: 0.56136 - acc: 0.8421 -- iter: 7000/8000\n",
            "Training Step: 9509  | total loss: \u001b[1m\u001b[32m0.56399\u001b[0m\u001b[0m | time: 5.022s\n",
            "| SGD | epoch: 837 | loss: 0.56399 - acc: 0.8421 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9510  | total loss: \u001b[1m\u001b[32m0.56626\u001b[0m\u001b[0m | time: 0.632s\n",
            "| SGD | epoch: 838 | loss: 0.56626 - acc: 0.8407 -- iter: 1000/8000\n",
            "Training Step: 9511  | total loss: \u001b[1m\u001b[32m0.56828\u001b[0m\u001b[0m | time: 1.266s\n",
            "| SGD | epoch: 838 | loss: 0.56828 - acc: 0.8398 -- iter: 2000/8000\n",
            "Training Step: 9512  | total loss: \u001b[1m\u001b[32m0.56834\u001b[0m\u001b[0m | time: 1.978s\n",
            "| SGD | epoch: 838 | loss: 0.56834 - acc: 0.8392 -- iter: 3000/8000\n",
            "Training Step: 9513  | total loss: \u001b[1m\u001b[32m0.56743\u001b[0m\u001b[0m | time: 2.601s\n",
            "| SGD | epoch: 838 | loss: 0.56743 - acc: 0.8410 -- iter: 4000/8000\n",
            "Training Step: 9514  | total loss: \u001b[1m\u001b[32m0.56584\u001b[0m\u001b[0m | time: 3.223s\n",
            "| SGD | epoch: 838 | loss: 0.56584 - acc: 0.8407 -- iter: 5000/8000\n",
            "Training Step: 9515  | total loss: \u001b[1m\u001b[32m0.56258\u001b[0m\u001b[0m | time: 3.848s\n",
            "| SGD | epoch: 838 | loss: 0.56258 - acc: 0.8409 -- iter: 6000/8000\n",
            "Training Step: 9516  | total loss: \u001b[1m\u001b[32m0.55964\u001b[0m\u001b[0m | time: 4.471s\n",
            "| SGD | epoch: 838 | loss: 0.55964 - acc: 0.8411 -- iter: 7000/8000\n",
            "Training Step: 9517  | total loss: \u001b[1m\u001b[32m0.55951\u001b[0m\u001b[0m | time: 5.115s\n",
            "| SGD | epoch: 838 | loss: 0.55951 - acc: 0.8419 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9518  | total loss: \u001b[1m\u001b[32m0.55968\u001b[0m\u001b[0m | time: 0.650s\n",
            "| SGD | epoch: 839 | loss: 0.55968 - acc: 0.8425 -- iter: 1000/8000\n",
            "Training Step: 9519  | total loss: \u001b[1m\u001b[32m0.56154\u001b[0m\u001b[0m | time: 1.278s\n",
            "| SGD | epoch: 839 | loss: 0.56154 - acc: 0.8420 -- iter: 2000/8000\n",
            "Training Step: 9520  | total loss: \u001b[1m\u001b[32m0.56222\u001b[0m\u001b[0m | time: 1.899s\n",
            "| SGD | epoch: 839 | loss: 0.56222 - acc: 0.8408 -- iter: 3000/8000\n",
            "Training Step: 9521  | total loss: \u001b[1m\u001b[32m0.56149\u001b[0m\u001b[0m | time: 2.514s\n",
            "| SGD | epoch: 839 | loss: 0.56149 - acc: 0.8419 -- iter: 4000/8000\n",
            "Training Step: 9522  | total loss: \u001b[1m\u001b[32m0.56389\u001b[0m\u001b[0m | time: 3.132s\n",
            "| SGD | epoch: 839 | loss: 0.56389 - acc: 0.8392 -- iter: 5000/8000\n",
            "Training Step: 9523  | total loss: \u001b[1m\u001b[32m0.56094\u001b[0m\u001b[0m | time: 3.752s\n",
            "| SGD | epoch: 839 | loss: 0.56094 - acc: 0.8408 -- iter: 6000/8000\n",
            "Training Step: 9524  | total loss: \u001b[1m\u001b[32m0.56436\u001b[0m\u001b[0m | time: 4.367s\n",
            "| SGD | epoch: 839 | loss: 0.56436 - acc: 0.8411 -- iter: 7000/8000\n",
            "Training Step: 9525  | total loss: \u001b[1m\u001b[32m0.56679\u001b[0m\u001b[0m | time: 4.983s\n",
            "| SGD | epoch: 839 | loss: 0.56679 - acc: 0.8422 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9526  | total loss: \u001b[1m\u001b[32m0.56204\u001b[0m\u001b[0m | time: 0.618s\n",
            "| SGD | epoch: 840 | loss: 0.56204 - acc: 0.8443 -- iter: 1000/8000\n",
            "Training Step: 9527  | total loss: \u001b[1m\u001b[32m0.56334\u001b[0m\u001b[0m | time: 1.237s\n",
            "| SGD | epoch: 840 | loss: 0.56334 - acc: 0.8430 -- iter: 2000/8000\n",
            "Training Step: 9528  | total loss: \u001b[1m\u001b[32m0.56495\u001b[0m\u001b[0m | time: 1.849s\n",
            "| SGD | epoch: 840 | loss: 0.56495 - acc: 0.8434 -- iter: 3000/8000\n",
            "Training Step: 9529  | total loss: \u001b[1m\u001b[32m0.56269\u001b[0m\u001b[0m | time: 2.460s\n",
            "| SGD | epoch: 840 | loss: 0.56269 - acc: 0.8445 -- iter: 4000/8000\n",
            "Training Step: 9530  | total loss: \u001b[1m\u001b[32m0.56020\u001b[0m\u001b[0m | time: 3.074s\n",
            "| SGD | epoch: 840 | loss: 0.56020 - acc: 0.8447 -- iter: 5000/8000\n",
            "Training Step: 9531  | total loss: \u001b[1m\u001b[32m0.55850\u001b[0m\u001b[0m | time: 3.703s\n",
            "| SGD | epoch: 840 | loss: 0.55850 - acc: 0.8444 -- iter: 6000/8000\n",
            "Training Step: 9532  | total loss: \u001b[1m\u001b[32m0.55862\u001b[0m\u001b[0m | time: 4.338s\n",
            "| SGD | epoch: 840 | loss: 0.55862 - acc: 0.8431 -- iter: 7000/8000\n",
            "Training Step: 9533  | total loss: \u001b[1m\u001b[32m0.56302\u001b[0m\u001b[0m | time: 4.967s\n",
            "| SGD | epoch: 840 | loss: 0.56302 - acc: 0.8417 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9534  | total loss: \u001b[1m\u001b[32m0.56649\u001b[0m\u001b[0m | time: 0.635s\n",
            "| SGD | epoch: 841 | loss: 0.56649 - acc: 0.8406 -- iter: 1000/8000\n",
            "Training Step: 9535  | total loss: \u001b[1m\u001b[32m0.56364\u001b[0m\u001b[0m | time: 1.274s\n",
            "| SGD | epoch: 841 | loss: 0.56364 - acc: 0.8427 -- iter: 2000/8000\n",
            "Training Step: 9536  | total loss: \u001b[1m\u001b[32m0.56737\u001b[0m\u001b[0m | time: 1.916s\n",
            "| SGD | epoch: 841 | loss: 0.56737 - acc: 0.8405 -- iter: 3000/8000\n",
            "Training Step: 9537  | total loss: \u001b[1m\u001b[32m0.56765\u001b[0m\u001b[0m | time: 2.561s\n",
            "| SGD | epoch: 841 | loss: 0.56765 - acc: 0.8421 -- iter: 4000/8000\n",
            "Training Step: 9538  | total loss: \u001b[1m\u001b[32m0.56448\u001b[0m\u001b[0m | time: 3.207s\n",
            "| SGD | epoch: 841 | loss: 0.56448 - acc: 0.8418 -- iter: 5000/8000\n",
            "Training Step: 9539  | total loss: \u001b[1m\u001b[32m0.56403\u001b[0m\u001b[0m | time: 3.858s\n",
            "| SGD | epoch: 841 | loss: 0.56403 - acc: 0.8413 -- iter: 6000/8000\n",
            "Training Step: 9540  | total loss: \u001b[1m\u001b[32m0.56547\u001b[0m\u001b[0m | time: 4.484s\n",
            "| SGD | epoch: 841 | loss: 0.56547 - acc: 0.8404 -- iter: 7000/8000\n",
            "Training Step: 9541  | total loss: \u001b[1m\u001b[32m0.56363\u001b[0m\u001b[0m | time: 5.106s\n",
            "| SGD | epoch: 841 | loss: 0.56363 - acc: 0.8416 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9542  | total loss: \u001b[1m\u001b[32m0.56428\u001b[0m\u001b[0m | time: 0.618s\n",
            "| SGD | epoch: 842 | loss: 0.56428 - acc: 0.8423 -- iter: 1000/8000\n",
            "Training Step: 9543  | total loss: \u001b[1m\u001b[32m0.56455\u001b[0m\u001b[0m | time: 1.232s\n",
            "| SGD | epoch: 842 | loss: 0.56455 - acc: 0.8432 -- iter: 2000/8000\n",
            "Training Step: 9544  | total loss: \u001b[1m\u001b[32m0.56588\u001b[0m\u001b[0m | time: 1.848s\n",
            "| SGD | epoch: 842 | loss: 0.56588 - acc: 0.8424 -- iter: 3000/8000\n",
            "Training Step: 9545  | total loss: \u001b[1m\u001b[32m0.56370\u001b[0m\u001b[0m | time: 2.462s\n",
            "| SGD | epoch: 842 | loss: 0.56370 - acc: 0.8428 -- iter: 4000/8000\n",
            "Training Step: 9546  | total loss: \u001b[1m\u001b[32m0.56251\u001b[0m\u001b[0m | time: 3.077s\n",
            "| SGD | epoch: 842 | loss: 0.56251 - acc: 0.8447 -- iter: 5000/8000\n",
            "Training Step: 9547  | total loss: \u001b[1m\u001b[32m0.56344\u001b[0m\u001b[0m | time: 3.711s\n",
            "| SGD | epoch: 842 | loss: 0.56344 - acc: 0.8438 -- iter: 6000/8000\n",
            "Training Step: 9548  | total loss: \u001b[1m\u001b[32m0.56501\u001b[0m\u001b[0m | time: 4.349s\n",
            "| SGD | epoch: 842 | loss: 0.56501 - acc: 0.8426 -- iter: 7000/8000\n",
            "Training Step: 9549  | total loss: \u001b[1m\u001b[32m0.56163\u001b[0m\u001b[0m | time: 4.978s\n",
            "| SGD | epoch: 842 | loss: 0.56163 - acc: 0.8443 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9550  | total loss: \u001b[1m\u001b[32m0.56300\u001b[0m\u001b[0m | time: 0.627s\n",
            "| SGD | epoch: 843 | loss: 0.56300 - acc: 0.8433 -- iter: 1000/8000\n",
            "Training Step: 9551  | total loss: \u001b[1m\u001b[32m0.56168\u001b[0m\u001b[0m | time: 1.258s\n",
            "| SGD | epoch: 843 | loss: 0.56168 - acc: 0.8448 -- iter: 2000/8000\n",
            "Training Step: 9552  | total loss: \u001b[1m\u001b[32m0.56041\u001b[0m\u001b[0m | time: 1.890s\n",
            "| SGD | epoch: 843 | loss: 0.56041 - acc: 0.8473 -- iter: 3000/8000\n",
            "Training Step: 9553  | total loss: \u001b[1m\u001b[32m0.56251\u001b[0m\u001b[0m | time: 2.526s\n",
            "| SGD | epoch: 843 | loss: 0.56251 - acc: 0.8438 -- iter: 4000/8000\n",
            "Training Step: 9554  | total loss: \u001b[1m\u001b[32m0.56095\u001b[0m\u001b[0m | time: 3.156s\n",
            "| SGD | epoch: 843 | loss: 0.56095 - acc: 0.8436 -- iter: 5000/8000\n",
            "Training Step: 9555  | total loss: \u001b[1m\u001b[32m0.55998\u001b[0m\u001b[0m | time: 3.786s\n",
            "| SGD | epoch: 843 | loss: 0.55998 - acc: 0.8434 -- iter: 6000/8000\n",
            "Training Step: 9556  | total loss: \u001b[1m\u001b[32m0.56283\u001b[0m\u001b[0m | time: 4.418s\n",
            "| SGD | epoch: 843 | loss: 0.56283 - acc: 0.8431 -- iter: 7000/8000\n",
            "Training Step: 9557  | total loss: \u001b[1m\u001b[32m0.56300\u001b[0m\u001b[0m | time: 5.060s\n",
            "| SGD | epoch: 843 | loss: 0.56300 - acc: 0.8433 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9558  | total loss: \u001b[1m\u001b[32m0.56084\u001b[0m\u001b[0m | time: 0.638s\n",
            "| SGD | epoch: 844 | loss: 0.56084 - acc: 0.8455 -- iter: 1000/8000\n",
            "Training Step: 9559  | total loss: \u001b[1m\u001b[32m0.56409\u001b[0m\u001b[0m | time: 1.275s\n",
            "| SGD | epoch: 844 | loss: 0.56409 - acc: 0.8449 -- iter: 2000/8000\n",
            "Training Step: 9560  | total loss: \u001b[1m\u001b[32m0.56414\u001b[0m\u001b[0m | time: 1.916s\n",
            "| SGD | epoch: 844 | loss: 0.56414 - acc: 0.8425 -- iter: 3000/8000\n",
            "Training Step: 9561  | total loss: \u001b[1m\u001b[32m0.56302\u001b[0m\u001b[0m | time: 2.554s\n",
            "| SGD | epoch: 844 | loss: 0.56302 - acc: 0.8419 -- iter: 4000/8000\n",
            "Training Step: 9562  | total loss: \u001b[1m\u001b[32m0.56253\u001b[0m\u001b[0m | time: 3.196s\n",
            "| SGD | epoch: 844 | loss: 0.56253 - acc: 0.8427 -- iter: 5000/8000\n",
            "Training Step: 9563  | total loss: \u001b[1m\u001b[32m0.56122\u001b[0m\u001b[0m | time: 3.832s\n",
            "| SGD | epoch: 844 | loss: 0.56122 - acc: 0.8424 -- iter: 6000/8000\n",
            "Training Step: 9564  | total loss: \u001b[1m\u001b[32m0.56172\u001b[0m\u001b[0m | time: 4.469s\n",
            "| SGD | epoch: 844 | loss: 0.56172 - acc: 0.8441 -- iter: 7000/8000\n",
            "Training Step: 9565  | total loss: \u001b[1m\u001b[32m0.56202\u001b[0m\u001b[0m | time: 5.103s\n",
            "| SGD | epoch: 844 | loss: 0.56202 - acc: 0.8441 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9566  | total loss: \u001b[1m\u001b[32m0.55959\u001b[0m\u001b[0m | time: 0.628s\n",
            "| SGD | epoch: 845 | loss: 0.55959 - acc: 0.8452 -- iter: 1000/8000\n",
            "Training Step: 9567  | total loss: \u001b[1m\u001b[32m0.55919\u001b[0m\u001b[0m | time: 1.260s\n",
            "| SGD | epoch: 845 | loss: 0.55919 - acc: 0.8447 -- iter: 2000/8000\n",
            "Training Step: 9568  | total loss: \u001b[1m\u001b[32m0.55999\u001b[0m\u001b[0m | time: 1.888s\n",
            "| SGD | epoch: 845 | loss: 0.55999 - acc: 0.8451 -- iter: 3000/8000\n",
            "Training Step: 9569  | total loss: \u001b[1m\u001b[32m0.55931\u001b[0m\u001b[0m | time: 2.514s\n",
            "| SGD | epoch: 845 | loss: 0.55931 - acc: 0.8447 -- iter: 4000/8000\n",
            "Training Step: 9570  | total loss: \u001b[1m\u001b[32m0.55814\u001b[0m\u001b[0m | time: 3.147s\n",
            "| SGD | epoch: 845 | loss: 0.55814 - acc: 0.8439 -- iter: 5000/8000\n",
            "Training Step: 9571  | total loss: \u001b[1m\u001b[32m0.56096\u001b[0m\u001b[0m | time: 3.775s\n",
            "| SGD | epoch: 845 | loss: 0.56096 - acc: 0.8414 -- iter: 6000/8000\n",
            "Training Step: 9572  | total loss: \u001b[1m\u001b[32m0.55949\u001b[0m\u001b[0m | time: 4.405s\n",
            "| SGD | epoch: 845 | loss: 0.55949 - acc: 0.8430 -- iter: 7000/8000\n",
            "Training Step: 9573  | total loss: \u001b[1m\u001b[32m0.56009\u001b[0m\u001b[0m | time: 5.034s\n",
            "| SGD | epoch: 845 | loss: 0.56009 - acc: 0.8434 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9574  | total loss: \u001b[1m\u001b[32m0.55793\u001b[0m\u001b[0m | time: 0.628s\n",
            "| SGD | epoch: 846 | loss: 0.55793 - acc: 0.8446 -- iter: 1000/8000\n",
            "Training Step: 9575  | total loss: \u001b[1m\u001b[32m0.55936\u001b[0m\u001b[0m | time: 1.259s\n",
            "| SGD | epoch: 846 | loss: 0.55936 - acc: 0.8452 -- iter: 2000/8000\n",
            "Training Step: 9576  | total loss: \u001b[1m\u001b[32m0.55603\u001b[0m\u001b[0m | time: 1.887s\n",
            "| SGD | epoch: 846 | loss: 0.55603 - acc: 0.8457 -- iter: 3000/8000\n",
            "Training Step: 9577  | total loss: \u001b[1m\u001b[32m0.55787\u001b[0m\u001b[0m | time: 2.516s\n",
            "| SGD | epoch: 846 | loss: 0.55787 - acc: 0.8443 -- iter: 4000/8000\n",
            "Training Step: 9578  | total loss: \u001b[1m\u001b[32m0.56072\u001b[0m\u001b[0m | time: 3.150s\n",
            "| SGD | epoch: 846 | loss: 0.56072 - acc: 0.8437 -- iter: 5000/8000\n",
            "Training Step: 9579  | total loss: \u001b[1m\u001b[32m0.56244\u001b[0m\u001b[0m | time: 3.785s\n",
            "| SGD | epoch: 846 | loss: 0.56244 - acc: 0.8427 -- iter: 6000/8000\n",
            "Training Step: 9580  | total loss: \u001b[1m\u001b[32m0.56216\u001b[0m\u001b[0m | time: 4.430s\n",
            "| SGD | epoch: 846 | loss: 0.56216 - acc: 0.8423 -- iter: 7000/8000\n",
            "Training Step: 9581  | total loss: \u001b[1m\u001b[32m0.56101\u001b[0m\u001b[0m | time: 5.073s\n",
            "| SGD | epoch: 846 | loss: 0.56101 - acc: 0.8433 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9582  | total loss: \u001b[1m\u001b[32m0.56149\u001b[0m\u001b[0m | time: 0.649s\n",
            "| SGD | epoch: 847 | loss: 0.56149 - acc: 0.8431 -- iter: 1000/8000\n",
            "Training Step: 9583  | total loss: \u001b[1m\u001b[32m0.55950\u001b[0m\u001b[0m | time: 1.299s\n",
            "| SGD | epoch: 847 | loss: 0.55950 - acc: 0.8428 -- iter: 2000/8000\n",
            "Training Step: 9584  | total loss: \u001b[1m\u001b[32m0.55883\u001b[0m\u001b[0m | time: 1.945s\n",
            "| SGD | epoch: 847 | loss: 0.55883 - acc: 0.8425 -- iter: 3000/8000\n",
            "Training Step: 9585  | total loss: \u001b[1m\u001b[32m0.55825\u001b[0m\u001b[0m | time: 2.581s\n",
            "| SGD | epoch: 847 | loss: 0.55825 - acc: 0.8418 -- iter: 4000/8000\n",
            "Training Step: 9586  | total loss: \u001b[1m\u001b[32m0.55801\u001b[0m\u001b[0m | time: 3.216s\n",
            "| SGD | epoch: 847 | loss: 0.55801 - acc: 0.8424 -- iter: 5000/8000\n",
            "Training Step: 9587  | total loss: \u001b[1m\u001b[32m0.55844\u001b[0m\u001b[0m | time: 3.847s\n",
            "| SGD | epoch: 847 | loss: 0.55844 - acc: 0.8430 -- iter: 6000/8000\n",
            "Training Step: 9588  | total loss: \u001b[1m\u001b[32m0.55842\u001b[0m\u001b[0m | time: 4.483s\n",
            "| SGD | epoch: 847 | loss: 0.55842 - acc: 0.8428 -- iter: 7000/8000\n",
            "Training Step: 9589  | total loss: \u001b[1m\u001b[32m0.55883\u001b[0m\u001b[0m | time: 5.092s\n",
            "| SGD | epoch: 847 | loss: 0.55883 - acc: 0.8429 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9590  | total loss: \u001b[1m\u001b[32m0.56125\u001b[0m\u001b[0m | time: 0.616s\n",
            "| SGD | epoch: 848 | loss: 0.56125 - acc: 0.8437 -- iter: 1000/8000\n",
            "Training Step: 9591  | total loss: \u001b[1m\u001b[32m0.56118\u001b[0m\u001b[0m | time: 1.250s\n",
            "| SGD | epoch: 848 | loss: 0.56118 - acc: 0.8445 -- iter: 2000/8000\n",
            "Training Step: 9592  | total loss: \u001b[1m\u001b[32m0.55733\u001b[0m\u001b[0m | time: 1.877s\n",
            "| SGD | epoch: 848 | loss: 0.55733 - acc: 0.8446 -- iter: 3000/8000\n",
            "Training Step: 9593  | total loss: \u001b[1m\u001b[32m0.56044\u001b[0m\u001b[0m | time: 2.513s\n",
            "| SGD | epoch: 848 | loss: 0.56044 - acc: 0.8426 -- iter: 4000/8000\n",
            "Training Step: 9594  | total loss: \u001b[1m\u001b[32m0.56230\u001b[0m\u001b[0m | time: 3.150s\n",
            "| SGD | epoch: 848 | loss: 0.56230 - acc: 0.8409 -- iter: 5000/8000\n",
            "Training Step: 9595  | total loss: \u001b[1m\u001b[32m0.56441\u001b[0m\u001b[0m | time: 3.765s\n",
            "| SGD | epoch: 848 | loss: 0.56441 - acc: 0.8412 -- iter: 6000/8000\n",
            "Training Step: 9596  | total loss: \u001b[1m\u001b[32m0.56296\u001b[0m\u001b[0m | time: 4.398s\n",
            "| SGD | epoch: 848 | loss: 0.56296 - acc: 0.8419 -- iter: 7000/8000\n",
            "Training Step: 9597  | total loss: \u001b[1m\u001b[32m0.56035\u001b[0m\u001b[0m | time: 5.029s\n",
            "| SGD | epoch: 848 | loss: 0.56035 - acc: 0.8434 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9598  | total loss: \u001b[1m\u001b[32m0.55792\u001b[0m\u001b[0m | time: 0.636s\n",
            "| SGD | epoch: 849 | loss: 0.55792 - acc: 0.8463 -- iter: 1000/8000\n",
            "Training Step: 9599  | total loss: \u001b[1m\u001b[32m0.55673\u001b[0m\u001b[0m | time: 1.268s\n",
            "| SGD | epoch: 849 | loss: 0.55673 - acc: 0.8450 -- iter: 2000/8000\n",
            "Training Step: 9600  | total loss: \u001b[1m\u001b[32m0.55780\u001b[0m\u001b[0m | time: 1.901s\n",
            "| SGD | epoch: 849 | loss: 0.55780 - acc: 0.8451 -- iter: 3000/8000\n",
            "Training Step: 9601  | total loss: \u001b[1m\u001b[32m0.55874\u001b[0m\u001b[0m | time: 2.540s\n",
            "| SGD | epoch: 849 | loss: 0.55874 - acc: 0.8457 -- iter: 4000/8000\n",
            "Training Step: 9602  | total loss: \u001b[1m\u001b[32m0.56188\u001b[0m\u001b[0m | time: 3.180s\n",
            "| SGD | epoch: 849 | loss: 0.56188 - acc: 0.8445 -- iter: 5000/8000\n",
            "Training Step: 9603  | total loss: \u001b[1m\u001b[32m0.56321\u001b[0m\u001b[0m | time: 3.824s\n",
            "| SGD | epoch: 849 | loss: 0.56321 - acc: 0.8435 -- iter: 6000/8000\n",
            "Training Step: 9604  | total loss: \u001b[1m\u001b[32m0.56703\u001b[0m\u001b[0m | time: 4.443s\n",
            "| SGD | epoch: 849 | loss: 0.56703 - acc: 0.8426 -- iter: 7000/8000\n",
            "Training Step: 9605  | total loss: \u001b[1m\u001b[32m0.56229\u001b[0m\u001b[0m | time: 5.072s\n",
            "| SGD | epoch: 849 | loss: 0.56229 - acc: 0.8434 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9606  | total loss: \u001b[1m\u001b[32m0.55789\u001b[0m\u001b[0m | time: 0.627s\n",
            "| SGD | epoch: 850 | loss: 0.55789 - acc: 0.8435 -- iter: 1000/8000\n",
            "Training Step: 9607  | total loss: \u001b[1m\u001b[32m0.55676\u001b[0m\u001b[0m | time: 1.245s\n",
            "| SGD | epoch: 850 | loss: 0.55676 - acc: 0.8431 -- iter: 2000/8000\n",
            "Training Step: 9608  | total loss: \u001b[1m\u001b[32m0.55142\u001b[0m\u001b[0m | time: 1.859s\n",
            "| SGD | epoch: 850 | loss: 0.55142 - acc: 0.8455 -- iter: 3000/8000\n",
            "Training Step: 9609  | total loss: \u001b[1m\u001b[32m0.55614\u001b[0m\u001b[0m | time: 2.470s\n",
            "| SGD | epoch: 850 | loss: 0.55614 - acc: 0.8434 -- iter: 4000/8000\n",
            "Training Step: 9610  | total loss: \u001b[1m\u001b[32m0.55316\u001b[0m\u001b[0m | time: 3.082s\n",
            "| SGD | epoch: 850 | loss: 0.55316 - acc: 0.8465 -- iter: 5000/8000\n",
            "Training Step: 9611  | total loss: \u001b[1m\u001b[32m0.55459\u001b[0m\u001b[0m | time: 3.699s\n",
            "| SGD | epoch: 850 | loss: 0.55459 - acc: 0.8457 -- iter: 6000/8000\n",
            "Training Step: 9612  | total loss: \u001b[1m\u001b[32m0.55370\u001b[0m\u001b[0m | time: 4.309s\n",
            "| SGD | epoch: 850 | loss: 0.55370 - acc: 0.8469 -- iter: 7000/8000\n",
            "Training Step: 9613  | total loss: \u001b[1m\u001b[32m0.55347\u001b[0m\u001b[0m | time: 4.920s\n",
            "| SGD | epoch: 850 | loss: 0.55347 - acc: 0.8468 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9614  | total loss: \u001b[1m\u001b[32m0.55158\u001b[0m\u001b[0m | time: 0.620s\n",
            "| SGD | epoch: 851 | loss: 0.55158 - acc: 0.8454 -- iter: 1000/8000\n",
            "Training Step: 9615  | total loss: \u001b[1m\u001b[32m0.54924\u001b[0m\u001b[0m | time: 1.233s\n",
            "| SGD | epoch: 851 | loss: 0.54924 - acc: 0.8447 -- iter: 2000/8000\n",
            "Training Step: 9616  | total loss: \u001b[1m\u001b[32m0.54992\u001b[0m\u001b[0m | time: 1.845s\n",
            "| SGD | epoch: 851 | loss: 0.54992 - acc: 0.8455 -- iter: 3000/8000\n",
            "Training Step: 9617  | total loss: \u001b[1m\u001b[32m0.55221\u001b[0m\u001b[0m | time: 2.457s\n",
            "| SGD | epoch: 851 | loss: 0.55221 - acc: 0.8434 -- iter: 4000/8000\n",
            "Training Step: 9618  | total loss: \u001b[1m\u001b[32m0.55717\u001b[0m\u001b[0m | time: 3.072s\n",
            "| SGD | epoch: 851 | loss: 0.55717 - acc: 0.8430 -- iter: 5000/8000\n",
            "Training Step: 9619  | total loss: \u001b[1m\u001b[32m0.55698\u001b[0m\u001b[0m | time: 3.684s\n",
            "| SGD | epoch: 851 | loss: 0.55698 - acc: 0.8428 -- iter: 6000/8000\n",
            "Training Step: 9620  | total loss: \u001b[1m\u001b[32m0.55366\u001b[0m\u001b[0m | time: 4.312s\n",
            "| SGD | epoch: 851 | loss: 0.55366 - acc: 0.8440 -- iter: 7000/8000\n",
            "Training Step: 9621  | total loss: \u001b[1m\u001b[32m0.55601\u001b[0m\u001b[0m | time: 4.944s\n",
            "| SGD | epoch: 851 | loss: 0.55601 - acc: 0.8425 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9622  | total loss: \u001b[1m\u001b[32m0.55486\u001b[0m\u001b[0m | time: 0.632s\n",
            "| SGD | epoch: 852 | loss: 0.55486 - acc: 0.8428 -- iter: 1000/8000\n",
            "Training Step: 9623  | total loss: \u001b[1m\u001b[32m0.55205\u001b[0m\u001b[0m | time: 1.267s\n",
            "| SGD | epoch: 852 | loss: 0.55205 - acc: 0.8427 -- iter: 2000/8000\n",
            "Training Step: 9624  | total loss: \u001b[1m\u001b[32m0.54880\u001b[0m\u001b[0m | time: 1.904s\n",
            "| SGD | epoch: 852 | loss: 0.54880 - acc: 0.8444 -- iter: 3000/8000\n",
            "Training Step: 9625  | total loss: \u001b[1m\u001b[32m0.54989\u001b[0m\u001b[0m | time: 2.543s\n",
            "| SGD | epoch: 852 | loss: 0.54989 - acc: 0.8454 -- iter: 4000/8000\n",
            "Training Step: 9626  | total loss: \u001b[1m\u001b[32m0.55224\u001b[0m\u001b[0m | time: 3.186s\n",
            "| SGD | epoch: 852 | loss: 0.55224 - acc: 0.8448 -- iter: 5000/8000\n",
            "Training Step: 9627  | total loss: \u001b[1m\u001b[32m0.55136\u001b[0m\u001b[0m | time: 3.829s\n",
            "| SGD | epoch: 852 | loss: 0.55136 - acc: 0.8446 -- iter: 6000/8000\n",
            "Training Step: 9628  | total loss: \u001b[1m\u001b[32m0.54960\u001b[0m\u001b[0m | time: 4.348s\n",
            "| SGD | epoch: 852 | loss: 0.54960 - acc: 0.8462 -- iter: 7000/8000\n",
            "Training Step: 9629  | total loss: \u001b[1m\u001b[32m0.54885\u001b[0m\u001b[0m | time: 4.871s\n",
            "| SGD | epoch: 852 | loss: 0.54885 - acc: 0.8463 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9630  | total loss: \u001b[1m\u001b[32m0.55415\u001b[0m\u001b[0m | time: 0.522s\n",
            "| SGD | epoch: 853 | loss: 0.55415 - acc: 0.8454 -- iter: 1000/8000\n",
            "Training Step: 9631  | total loss: \u001b[1m\u001b[32m0.55688\u001b[0m\u001b[0m | time: 1.034s\n",
            "| SGD | epoch: 853 | loss: 0.55688 - acc: 0.8438 -- iter: 2000/8000\n",
            "Training Step: 9632  | total loss: \u001b[1m\u001b[32m0.55718\u001b[0m\u001b[0m | time: 1.546s\n",
            "| SGD | epoch: 853 | loss: 0.55718 - acc: 0.8425 -- iter: 3000/8000\n",
            "Training Step: 9633  | total loss: \u001b[1m\u001b[32m0.55630\u001b[0m\u001b[0m | time: 2.049s\n",
            "| SGD | epoch: 853 | loss: 0.55630 - acc: 0.8439 -- iter: 4000/8000\n",
            "Training Step: 9634  | total loss: \u001b[1m\u001b[32m0.55557\u001b[0m\u001b[0m | time: 2.553s\n",
            "| SGD | epoch: 853 | loss: 0.55557 - acc: 0.8437 -- iter: 5000/8000\n",
            "Training Step: 9635  | total loss: \u001b[1m\u001b[32m0.55372\u001b[0m\u001b[0m | time: 3.059s\n",
            "| SGD | epoch: 853 | loss: 0.55372 - acc: 0.8451 -- iter: 6000/8000\n",
            "Training Step: 9636  | total loss: \u001b[1m\u001b[32m0.55220\u001b[0m\u001b[0m | time: 3.567s\n",
            "| SGD | epoch: 853 | loss: 0.55220 - acc: 0.8452 -- iter: 7000/8000\n",
            "Training Step: 9637  | total loss: \u001b[1m\u001b[32m0.54951\u001b[0m\u001b[0m | time: 4.072s\n",
            "| SGD | epoch: 853 | loss: 0.54951 - acc: 0.8463 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9638  | total loss: \u001b[1m\u001b[32m0.54959\u001b[0m\u001b[0m | time: 0.509s\n",
            "| SGD | epoch: 854 | loss: 0.54959 - acc: 0.8463 -- iter: 1000/8000\n",
            "Training Step: 9639  | total loss: \u001b[1m\u001b[32m0.54878\u001b[0m\u001b[0m | time: 1.019s\n",
            "| SGD | epoch: 854 | loss: 0.54878 - acc: 0.8470 -- iter: 2000/8000\n",
            "Training Step: 9640  | total loss: \u001b[1m\u001b[32m0.54987\u001b[0m\u001b[0m | time: 1.524s\n",
            "| SGD | epoch: 854 | loss: 0.54987 - acc: 0.8459 -- iter: 3000/8000\n",
            "Training Step: 9641  | total loss: \u001b[1m\u001b[32m0.55245\u001b[0m\u001b[0m | time: 2.031s\n",
            "| SGD | epoch: 854 | loss: 0.55245 - acc: 0.8445 -- iter: 4000/8000\n",
            "Training Step: 9642  | total loss: \u001b[1m\u001b[32m0.55393\u001b[0m\u001b[0m | time: 2.541s\n",
            "| SGD | epoch: 854 | loss: 0.55393 - acc: 0.8451 -- iter: 5000/8000\n",
            "Training Step: 9643  | total loss: \u001b[1m\u001b[32m0.55370\u001b[0m\u001b[0m | time: 3.067s\n",
            "| SGD | epoch: 854 | loss: 0.55370 - acc: 0.8442 -- iter: 6000/8000\n",
            "Training Step: 9644  | total loss: \u001b[1m\u001b[32m0.55665\u001b[0m\u001b[0m | time: 3.578s\n",
            "| SGD | epoch: 854 | loss: 0.55665 - acc: 0.8455 -- iter: 7000/8000\n",
            "Training Step: 9645  | total loss: \u001b[1m\u001b[32m0.55473\u001b[0m\u001b[0m | time: 4.083s\n",
            "| SGD | epoch: 854 | loss: 0.55473 - acc: 0.8463 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9646  | total loss: \u001b[1m\u001b[32m0.55454\u001b[0m\u001b[0m | time: 0.507s\n",
            "| SGD | epoch: 855 | loss: 0.55454 - acc: 0.8459 -- iter: 1000/8000\n",
            "Training Step: 9647  | total loss: \u001b[1m\u001b[32m0.55455\u001b[0m\u001b[0m | time: 1.018s\n",
            "| SGD | epoch: 855 | loss: 0.55455 - acc: 0.8469 -- iter: 2000/8000\n",
            "Training Step: 9648  | total loss: \u001b[1m\u001b[32m0.55533\u001b[0m\u001b[0m | time: 1.531s\n",
            "| SGD | epoch: 855 | loss: 0.55533 - acc: 0.8470 -- iter: 3000/8000\n",
            "Training Step: 9649  | total loss: \u001b[1m\u001b[32m0.55575\u001b[0m\u001b[0m | time: 2.052s\n",
            "| SGD | epoch: 855 | loss: 0.55575 - acc: 0.8474 -- iter: 4000/8000\n",
            "Training Step: 9650  | total loss: \u001b[1m\u001b[32m0.55484\u001b[0m\u001b[0m | time: 2.573s\n",
            "| SGD | epoch: 855 | loss: 0.55484 - acc: 0.8485 -- iter: 5000/8000\n",
            "Training Step: 9651  | total loss: \u001b[1m\u001b[32m0.55383\u001b[0m\u001b[0m | time: 3.090s\n",
            "| SGD | epoch: 855 | loss: 0.55383 - acc: 0.8486 -- iter: 6000/8000\n",
            "Training Step: 9652  | total loss: \u001b[1m\u001b[32m0.55194\u001b[0m\u001b[0m | time: 3.611s\n",
            "| SGD | epoch: 855 | loss: 0.55194 - acc: 0.8487 -- iter: 7000/8000\n",
            "Training Step: 9653  | total loss: \u001b[1m\u001b[32m0.55280\u001b[0m\u001b[0m | time: 4.128s\n",
            "| SGD | epoch: 855 | loss: 0.55280 - acc: 0.8472 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9654  | total loss: \u001b[1m\u001b[32m0.55440\u001b[0m\u001b[0m | time: 0.522s\n",
            "| SGD | epoch: 856 | loss: 0.55440 - acc: 0.8484 -- iter: 1000/8000\n",
            "Training Step: 9655  | total loss: \u001b[1m\u001b[32m0.55217\u001b[0m\u001b[0m | time: 1.040s\n",
            "| SGD | epoch: 856 | loss: 0.55217 - acc: 0.8486 -- iter: 2000/8000\n",
            "Training Step: 9656  | total loss: \u001b[1m\u001b[32m0.55539\u001b[0m\u001b[0m | time: 1.560s\n",
            "| SGD | epoch: 856 | loss: 0.55539 - acc: 0.8480 -- iter: 3000/8000\n",
            "Training Step: 9657  | total loss: \u001b[1m\u001b[32m0.55194\u001b[0m\u001b[0m | time: 2.074s\n",
            "| SGD | epoch: 856 | loss: 0.55194 - acc: 0.8490 -- iter: 4000/8000\n",
            "Training Step: 9658  | total loss: \u001b[1m\u001b[32m0.55265\u001b[0m\u001b[0m | time: 2.578s\n",
            "| SGD | epoch: 856 | loss: 0.55265 - acc: 0.8478 -- iter: 5000/8000\n",
            "Training Step: 9659  | total loss: \u001b[1m\u001b[32m0.55324\u001b[0m\u001b[0m | time: 3.089s\n",
            "| SGD | epoch: 856 | loss: 0.55324 - acc: 0.8473 -- iter: 6000/8000\n",
            "Training Step: 9660  | total loss: \u001b[1m\u001b[32m0.55390\u001b[0m\u001b[0m | time: 3.598s\n",
            "| SGD | epoch: 856 | loss: 0.55390 - acc: 0.8467 -- iter: 7000/8000\n",
            "Training Step: 9661  | total loss: \u001b[1m\u001b[32m0.55201\u001b[0m\u001b[0m | time: 4.112s\n",
            "| SGD | epoch: 856 | loss: 0.55201 - acc: 0.8467 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9662  | total loss: \u001b[1m\u001b[32m0.55255\u001b[0m\u001b[0m | time: 0.508s\n",
            "| SGD | epoch: 857 | loss: 0.55255 - acc: 0.8452 -- iter: 1000/8000\n",
            "Training Step: 9663  | total loss: \u001b[1m\u001b[32m0.55232\u001b[0m\u001b[0m | time: 1.022s\n",
            "| SGD | epoch: 857 | loss: 0.55232 - acc: 0.8449 -- iter: 2000/8000\n",
            "Training Step: 9664  | total loss: \u001b[1m\u001b[32m0.55204\u001b[0m\u001b[0m | time: 1.535s\n",
            "| SGD | epoch: 857 | loss: 0.55204 - acc: 0.8455 -- iter: 3000/8000\n",
            "Training Step: 9665  | total loss: \u001b[1m\u001b[32m0.55156\u001b[0m\u001b[0m | time: 2.054s\n",
            "| SGD | epoch: 857 | loss: 0.55156 - acc: 0.8460 -- iter: 4000/8000\n",
            "Training Step: 9666  | total loss: \u001b[1m\u001b[32m0.55590\u001b[0m\u001b[0m | time: 2.572s\n",
            "| SGD | epoch: 857 | loss: 0.55590 - acc: 0.8451 -- iter: 5000/8000\n",
            "Training Step: 9667  | total loss: \u001b[1m\u001b[32m0.55534\u001b[0m\u001b[0m | time: 3.077s\n",
            "| SGD | epoch: 857 | loss: 0.55534 - acc: 0.8453 -- iter: 6000/8000\n",
            "Training Step: 9668  | total loss: \u001b[1m\u001b[32m0.55397\u001b[0m\u001b[0m | time: 3.591s\n",
            "| SGD | epoch: 857 | loss: 0.55397 - acc: 0.8448 -- iter: 7000/8000\n",
            "Training Step: 9669  | total loss: \u001b[1m\u001b[32m0.55181\u001b[0m\u001b[0m | time: 4.105s\n",
            "| SGD | epoch: 857 | loss: 0.55181 - acc: 0.8465 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9670  | total loss: \u001b[1m\u001b[32m0.55258\u001b[0m\u001b[0m | time: 0.512s\n",
            "| SGD | epoch: 858 | loss: 0.55258 - acc: 0.8461 -- iter: 1000/8000\n",
            "Training Step: 9671  | total loss: \u001b[1m\u001b[32m0.54819\u001b[0m\u001b[0m | time: 1.033s\n",
            "| SGD | epoch: 858 | loss: 0.54819 - acc: 0.8477 -- iter: 2000/8000\n",
            "Training Step: 9672  | total loss: \u001b[1m\u001b[32m0.55041\u001b[0m\u001b[0m | time: 1.538s\n",
            "| SGD | epoch: 858 | loss: 0.55041 - acc: 0.8476 -- iter: 3000/8000\n",
            "Training Step: 9673  | total loss: \u001b[1m\u001b[32m0.55111\u001b[0m\u001b[0m | time: 2.054s\n",
            "| SGD | epoch: 858 | loss: 0.55111 - acc: 0.8473 -- iter: 4000/8000\n",
            "Training Step: 9674  | total loss: \u001b[1m\u001b[32m0.55091\u001b[0m\u001b[0m | time: 2.571s\n",
            "| SGD | epoch: 858 | loss: 0.55091 - acc: 0.8474 -- iter: 5000/8000\n",
            "Training Step: 9675  | total loss: \u001b[1m\u001b[32m0.55214\u001b[0m\u001b[0m | time: 3.095s\n",
            "| SGD | epoch: 858 | loss: 0.55214 - acc: 0.8457 -- iter: 6000/8000\n",
            "Training Step: 9676  | total loss: \u001b[1m\u001b[32m0.55292\u001b[0m\u001b[0m | time: 3.618s\n",
            "| SGD | epoch: 858 | loss: 0.55292 - acc: 0.8456 -- iter: 7000/8000\n",
            "Training Step: 9677  | total loss: \u001b[1m\u001b[32m0.55115\u001b[0m\u001b[0m | time: 4.138s\n",
            "| SGD | epoch: 858 | loss: 0.55115 - acc: 0.8471 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9678  | total loss: \u001b[1m\u001b[32m0.54950\u001b[0m\u001b[0m | time: 0.520s\n",
            "| SGD | epoch: 859 | loss: 0.54950 - acc: 0.8484 -- iter: 1000/8000\n",
            "Training Step: 9679  | total loss: \u001b[1m\u001b[32m0.54869\u001b[0m\u001b[0m | time: 1.042s\n",
            "| SGD | epoch: 859 | loss: 0.54869 - acc: 0.8481 -- iter: 2000/8000\n",
            "Training Step: 9680  | total loss: \u001b[1m\u001b[32m0.54878\u001b[0m\u001b[0m | time: 1.561s\n",
            "| SGD | epoch: 859 | loss: 0.54878 - acc: 0.8490 -- iter: 3000/8000\n",
            "Training Step: 9681  | total loss: \u001b[1m\u001b[32m0.54991\u001b[0m\u001b[0m | time: 2.090s\n",
            "| SGD | epoch: 859 | loss: 0.54991 - acc: 0.8484 -- iter: 4000/8000\n",
            "Training Step: 9682  | total loss: \u001b[1m\u001b[32m0.55348\u001b[0m\u001b[0m | time: 2.605s\n",
            "| SGD | epoch: 859 | loss: 0.55348 - acc: 0.8480 -- iter: 5000/8000\n",
            "Training Step: 9683  | total loss: \u001b[1m\u001b[32m0.55514\u001b[0m\u001b[0m | time: 3.118s\n",
            "| SGD | epoch: 859 | loss: 0.55514 - acc: 0.8469 -- iter: 6000/8000\n",
            "Training Step: 9684  | total loss: \u001b[1m\u001b[32m0.55535\u001b[0m\u001b[0m | time: 3.640s\n",
            "| SGD | epoch: 859 | loss: 0.55535 - acc: 0.8478 -- iter: 7000/8000\n",
            "Training Step: 9685  | total loss: \u001b[1m\u001b[32m0.55000\u001b[0m\u001b[0m | time: 4.147s\n",
            "| SGD | epoch: 859 | loss: 0.55000 - acc: 0.8503 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9686  | total loss: \u001b[1m\u001b[32m0.55117\u001b[0m\u001b[0m | time: 0.517s\n",
            "| SGD | epoch: 860 | loss: 0.55117 - acc: 0.8489 -- iter: 1000/8000\n",
            "Training Step: 9687  | total loss: \u001b[1m\u001b[32m0.55184\u001b[0m\u001b[0m | time: 1.028s\n",
            "| SGD | epoch: 860 | loss: 0.55184 - acc: 0.8478 -- iter: 2000/8000\n",
            "Training Step: 9688  | total loss: \u001b[1m\u001b[32m0.54774\u001b[0m\u001b[0m | time: 1.554s\n",
            "| SGD | epoch: 860 | loss: 0.54774 - acc: 0.8488 -- iter: 3000/8000\n",
            "Training Step: 9689  | total loss: \u001b[1m\u001b[32m0.54786\u001b[0m\u001b[0m | time: 2.070s\n",
            "| SGD | epoch: 860 | loss: 0.54786 - acc: 0.8485 -- iter: 4000/8000\n",
            "Training Step: 9690  | total loss: \u001b[1m\u001b[32m0.54644\u001b[0m\u001b[0m | time: 2.591s\n",
            "| SGD | epoch: 860 | loss: 0.54644 - acc: 0.8486 -- iter: 5000/8000\n",
            "Training Step: 9691  | total loss: \u001b[1m\u001b[32m0.54523\u001b[0m\u001b[0m | time: 3.112s\n",
            "| SGD | epoch: 860 | loss: 0.54523 - acc: 0.8505 -- iter: 6000/8000\n",
            "Training Step: 9692  | total loss: \u001b[1m\u001b[32m0.54681\u001b[0m\u001b[0m | time: 3.635s\n",
            "| SGD | epoch: 860 | loss: 0.54681 - acc: 0.8495 -- iter: 7000/8000\n",
            "Training Step: 9693  | total loss: \u001b[1m\u001b[32m0.54895\u001b[0m\u001b[0m | time: 4.156s\n",
            "| SGD | epoch: 860 | loss: 0.54895 - acc: 0.8477 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9694  | total loss: \u001b[1m\u001b[32m0.54991\u001b[0m\u001b[0m | time: 0.523s\n",
            "| SGD | epoch: 861 | loss: 0.54991 - acc: 0.8472 -- iter: 1000/8000\n",
            "Training Step: 9695  | total loss: \u001b[1m\u001b[32m0.55035\u001b[0m\u001b[0m | time: 1.035s\n",
            "| SGD | epoch: 861 | loss: 0.55035 - acc: 0.8466 -- iter: 2000/8000\n",
            "Training Step: 9696  | total loss: \u001b[1m\u001b[32m0.55068\u001b[0m\u001b[0m | time: 1.553s\n",
            "| SGD | epoch: 861 | loss: 0.55068 - acc: 0.8457 -- iter: 3000/8000\n",
            "Training Step: 9697  | total loss: \u001b[1m\u001b[32m0.55121\u001b[0m\u001b[0m | time: 2.073s\n",
            "| SGD | epoch: 861 | loss: 0.55121 - acc: 0.8480 -- iter: 4000/8000\n",
            "Training Step: 9698  | total loss: \u001b[1m\u001b[32m0.54915\u001b[0m\u001b[0m | time: 2.591s\n",
            "| SGD | epoch: 861 | loss: 0.54915 - acc: 0.8468 -- iter: 5000/8000\n",
            "Training Step: 9699  | total loss: \u001b[1m\u001b[32m0.55183\u001b[0m\u001b[0m | time: 3.112s\n",
            "| SGD | epoch: 861 | loss: 0.55183 - acc: 0.8462 -- iter: 6000/8000\n",
            "Training Step: 9700  | total loss: \u001b[1m\u001b[32m0.55371\u001b[0m\u001b[0m | time: 3.627s\n",
            "| SGD | epoch: 861 | loss: 0.55371 - acc: 0.8448 -- iter: 7000/8000\n",
            "Training Step: 9701  | total loss: \u001b[1m\u001b[32m0.55441\u001b[0m\u001b[0m | time: 4.143s\n",
            "| SGD | epoch: 861 | loss: 0.55441 - acc: 0.8450 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9702  | total loss: \u001b[1m\u001b[32m0.55439\u001b[0m\u001b[0m | time: 0.534s\n",
            "| SGD | epoch: 862 | loss: 0.55439 - acc: 0.8457 -- iter: 1000/8000\n",
            "Training Step: 9703  | total loss: \u001b[1m\u001b[32m0.55117\u001b[0m\u001b[0m | time: 1.061s\n",
            "| SGD | epoch: 862 | loss: 0.55117 - acc: 0.8473 -- iter: 2000/8000\n",
            "Training Step: 9704  | total loss: \u001b[1m\u001b[32m0.55121\u001b[0m\u001b[0m | time: 1.588s\n",
            "| SGD | epoch: 862 | loss: 0.55121 - acc: 0.8484 -- iter: 3000/8000\n",
            "Training Step: 9705  | total loss: \u001b[1m\u001b[32m0.55083\u001b[0m\u001b[0m | time: 2.111s\n",
            "| SGD | epoch: 862 | loss: 0.55083 - acc: 0.8503 -- iter: 4000/8000\n",
            "Training Step: 9706  | total loss: \u001b[1m\u001b[32m0.55040\u001b[0m\u001b[0m | time: 2.641s\n",
            "| SGD | epoch: 862 | loss: 0.55040 - acc: 0.8502 -- iter: 5000/8000\n",
            "Training Step: 9707  | total loss: \u001b[1m\u001b[32m0.54826\u001b[0m\u001b[0m | time: 3.164s\n",
            "| SGD | epoch: 862 | loss: 0.54826 - acc: 0.8508 -- iter: 6000/8000\n",
            "Training Step: 9708  | total loss: \u001b[1m\u001b[32m0.55120\u001b[0m\u001b[0m | time: 3.694s\n",
            "| SGD | epoch: 862 | loss: 0.55120 - acc: 0.8487 -- iter: 7000/8000\n",
            "Training Step: 9709  | total loss: \u001b[1m\u001b[32m0.54831\u001b[0m\u001b[0m | time: 4.230s\n",
            "| SGD | epoch: 862 | loss: 0.54831 - acc: 0.8497 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9710  | total loss: \u001b[1m\u001b[32m0.55018\u001b[0m\u001b[0m | time: 0.540s\n",
            "| SGD | epoch: 863 | loss: 0.55018 - acc: 0.8492 -- iter: 1000/8000\n",
            "Training Step: 9711  | total loss: \u001b[1m\u001b[32m0.55257\u001b[0m\u001b[0m | time: 1.056s\n",
            "| SGD | epoch: 863 | loss: 0.55257 - acc: 0.8479 -- iter: 2000/8000\n",
            "Training Step: 9712  | total loss: \u001b[1m\u001b[32m0.55359\u001b[0m\u001b[0m | time: 1.697s\n",
            "| SGD | epoch: 863 | loss: 0.55359 - acc: 0.8468 -- iter: 3000/8000\n",
            "Training Step: 9713  | total loss: \u001b[1m\u001b[32m0.55269\u001b[0m\u001b[0m | time: 2.331s\n",
            "| SGD | epoch: 863 | loss: 0.55269 - acc: 0.8476 -- iter: 4000/8000\n",
            "Training Step: 9714  | total loss: \u001b[1m\u001b[32m0.55162\u001b[0m\u001b[0m | time: 2.963s\n",
            "| SGD | epoch: 863 | loss: 0.55162 - acc: 0.8479 -- iter: 5000/8000\n",
            "Training Step: 9715  | total loss: \u001b[1m\u001b[32m0.55023\u001b[0m\u001b[0m | time: 3.596s\n",
            "| SGD | epoch: 863 | loss: 0.55023 - acc: 0.8484 -- iter: 6000/8000\n",
            "Training Step: 9716  | total loss: \u001b[1m\u001b[32m0.54507\u001b[0m\u001b[0m | time: 4.224s\n",
            "| SGD | epoch: 863 | loss: 0.54507 - acc: 0.8505 -- iter: 7000/8000\n",
            "Training Step: 9717  | total loss: \u001b[1m\u001b[32m0.54434\u001b[0m\u001b[0m | time: 4.855s\n",
            "| SGD | epoch: 863 | loss: 0.54434 - acc: 0.8509 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9718  | total loss: \u001b[1m\u001b[32m0.54326\u001b[0m\u001b[0m | time: 0.628s\n",
            "| SGD | epoch: 864 | loss: 0.54326 - acc: 0.8512 -- iter: 1000/8000\n",
            "Training Step: 9719  | total loss: \u001b[1m\u001b[32m0.54257\u001b[0m\u001b[0m | time: 1.259s\n",
            "| SGD | epoch: 864 | loss: 0.54257 - acc: 0.8521 -- iter: 2000/8000\n",
            "Training Step: 9720  | total loss: \u001b[1m\u001b[32m0.54739\u001b[0m\u001b[0m | time: 1.892s\n",
            "| SGD | epoch: 864 | loss: 0.54739 - acc: 0.8517 -- iter: 3000/8000\n",
            "Training Step: 9721  | total loss: \u001b[1m\u001b[32m0.54877\u001b[0m\u001b[0m | time: 2.521s\n",
            "| SGD | epoch: 864 | loss: 0.54877 - acc: 0.8507 -- iter: 4000/8000\n",
            "Training Step: 9722  | total loss: \u001b[1m\u001b[32m0.54799\u001b[0m\u001b[0m | time: 3.150s\n",
            "| SGD | epoch: 864 | loss: 0.54799 - acc: 0.8504 -- iter: 5000/8000\n",
            "Training Step: 9723  | total loss: \u001b[1m\u001b[32m0.54742\u001b[0m\u001b[0m | time: 3.778s\n",
            "| SGD | epoch: 864 | loss: 0.54742 - acc: 0.8514 -- iter: 6000/8000\n",
            "Training Step: 9724  | total loss: \u001b[1m\u001b[32m0.54377\u001b[0m\u001b[0m | time: 4.407s\n",
            "| SGD | epoch: 864 | loss: 0.54377 - acc: 0.8514 -- iter: 7000/8000\n",
            "Training Step: 9725  | total loss: \u001b[1m\u001b[32m0.54345\u001b[0m\u001b[0m | time: 5.040s\n",
            "| SGD | epoch: 864 | loss: 0.54345 - acc: 0.8518 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9726  | total loss: \u001b[1m\u001b[32m0.54347\u001b[0m\u001b[0m | time: 0.643s\n",
            "| SGD | epoch: 865 | loss: 0.54347 - acc: 0.8509 -- iter: 1000/8000\n",
            "Training Step: 9727  | total loss: \u001b[1m\u001b[32m0.54112\u001b[0m\u001b[0m | time: 1.273s\n",
            "| SGD | epoch: 865 | loss: 0.54112 - acc: 0.8514 -- iter: 2000/8000\n",
            "Training Step: 9728  | total loss: \u001b[1m\u001b[32m0.54162\u001b[0m\u001b[0m | time: 1.895s\n",
            "| SGD | epoch: 865 | loss: 0.54162 - acc: 0.8520 -- iter: 3000/8000\n",
            "Training Step: 9729  | total loss: \u001b[1m\u001b[32m0.54242\u001b[0m\u001b[0m | time: 2.519s\n",
            "| SGD | epoch: 865 | loss: 0.54242 - acc: 0.8523 -- iter: 4000/8000\n",
            "Training Step: 9730  | total loss: \u001b[1m\u001b[32m0.54116\u001b[0m\u001b[0m | time: 3.157s\n",
            "| SGD | epoch: 865 | loss: 0.54116 - acc: 0.8530 -- iter: 5000/8000\n",
            "Training Step: 9731  | total loss: \u001b[1m\u001b[32m0.54437\u001b[0m\u001b[0m | time: 3.796s\n",
            "| SGD | epoch: 865 | loss: 0.54437 - acc: 0.8522 -- iter: 6000/8000\n",
            "Training Step: 9732  | total loss: \u001b[1m\u001b[32m0.54604\u001b[0m\u001b[0m | time: 4.435s\n",
            "| SGD | epoch: 865 | loss: 0.54604 - acc: 0.8519 -- iter: 7000/8000\n",
            "Training Step: 9733  | total loss: \u001b[1m\u001b[32m0.54677\u001b[0m\u001b[0m | time: 5.072s\n",
            "| SGD | epoch: 865 | loss: 0.54677 - acc: 0.8511 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9734  | total loss: \u001b[1m\u001b[32m0.54786\u001b[0m\u001b[0m | time: 0.646s\n",
            "| SGD | epoch: 866 | loss: 0.54786 - acc: 0.8516 -- iter: 1000/8000\n",
            "Training Step: 9735  | total loss: \u001b[1m\u001b[32m0.54773\u001b[0m\u001b[0m | time: 1.288s\n",
            "| SGD | epoch: 866 | loss: 0.54773 - acc: 0.8516 -- iter: 2000/8000\n",
            "Training Step: 9736  | total loss: \u001b[1m\u001b[32m0.54755\u001b[0m\u001b[0m | time: 1.920s\n",
            "| SGD | epoch: 866 | loss: 0.54755 - acc: 0.8515 -- iter: 3000/8000\n",
            "Training Step: 9737  | total loss: \u001b[1m\u001b[32m0.54946\u001b[0m\u001b[0m | time: 2.552s\n",
            "| SGD | epoch: 866 | loss: 0.54946 - acc: 0.8511 -- iter: 4000/8000\n",
            "Training Step: 9738  | total loss: \u001b[1m\u001b[32m0.54849\u001b[0m\u001b[0m | time: 3.182s\n",
            "| SGD | epoch: 866 | loss: 0.54849 - acc: 0.8494 -- iter: 5000/8000\n",
            "Training Step: 9739  | total loss: \u001b[1m\u001b[32m0.55071\u001b[0m\u001b[0m | time: 3.811s\n",
            "| SGD | epoch: 866 | loss: 0.55071 - acc: 0.8495 -- iter: 6000/8000\n",
            "Training Step: 9740  | total loss: \u001b[1m\u001b[32m0.54811\u001b[0m\u001b[0m | time: 4.439s\n",
            "| SGD | epoch: 866 | loss: 0.54811 - acc: 0.8512 -- iter: 7000/8000\n",
            "Training Step: 9741  | total loss: \u001b[1m\u001b[32m0.54466\u001b[0m\u001b[0m | time: 5.075s\n",
            "| SGD | epoch: 866 | loss: 0.54466 - acc: 0.8539 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9742  | total loss: \u001b[1m\u001b[32m0.54051\u001b[0m\u001b[0m | time: 0.628s\n",
            "| SGD | epoch: 867 | loss: 0.54051 - acc: 0.8567 -- iter: 1000/8000\n",
            "Training Step: 9743  | total loss: \u001b[1m\u001b[32m0.54262\u001b[0m\u001b[0m | time: 1.258s\n",
            "| SGD | epoch: 867 | loss: 0.54262 - acc: 0.8546 -- iter: 2000/8000\n",
            "Training Step: 9744  | total loss: \u001b[1m\u001b[32m0.54506\u001b[0m\u001b[0m | time: 1.891s\n",
            "| SGD | epoch: 867 | loss: 0.54506 - acc: 0.8544 -- iter: 3000/8000\n",
            "Training Step: 9745  | total loss: \u001b[1m\u001b[32m0.54476\u001b[0m\u001b[0m | time: 2.519s\n",
            "| SGD | epoch: 867 | loss: 0.54476 - acc: 0.8550 -- iter: 4000/8000\n",
            "Training Step: 9746  | total loss: \u001b[1m\u001b[32m0.54709\u001b[0m\u001b[0m | time: 3.150s\n",
            "| SGD | epoch: 867 | loss: 0.54709 - acc: 0.8533 -- iter: 5000/8000\n",
            "Training Step: 9747  | total loss: \u001b[1m\u001b[32m0.54178\u001b[0m\u001b[0m | time: 3.781s\n",
            "| SGD | epoch: 867 | loss: 0.54178 - acc: 0.8546 -- iter: 6000/8000\n",
            "Training Step: 9748  | total loss: \u001b[1m\u001b[32m0.54150\u001b[0m\u001b[0m | time: 4.422s\n",
            "| SGD | epoch: 867 | loss: 0.54150 - acc: 0.8534 -- iter: 7000/8000\n",
            "Training Step: 9749  | total loss: \u001b[1m\u001b[32m0.54199\u001b[0m\u001b[0m | time: 5.067s\n",
            "| SGD | epoch: 867 | loss: 0.54199 - acc: 0.8530 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9750  | total loss: \u001b[1m\u001b[32m0.54230\u001b[0m\u001b[0m | time: 0.645s\n",
            "| SGD | epoch: 868 | loss: 0.54230 - acc: 0.8518 -- iter: 1000/8000\n",
            "Training Step: 9751  | total loss: \u001b[1m\u001b[32m0.54064\u001b[0m\u001b[0m | time: 1.170s\n",
            "| SGD | epoch: 868 | loss: 0.54064 - acc: 0.8516 -- iter: 2000/8000\n",
            "Training Step: 9752  | total loss: \u001b[1m\u001b[32m0.54496\u001b[0m\u001b[0m | time: 1.688s\n",
            "| SGD | epoch: 868 | loss: 0.54496 - acc: 0.8498 -- iter: 3000/8000\n",
            "Training Step: 9753  | total loss: \u001b[1m\u001b[32m0.54569\u001b[0m\u001b[0m | time: 2.209s\n",
            "| SGD | epoch: 868 | loss: 0.54569 - acc: 0.8499 -- iter: 4000/8000\n",
            "Training Step: 9754  | total loss: \u001b[1m\u001b[32m0.54321\u001b[0m\u001b[0m | time: 2.730s\n",
            "| SGD | epoch: 868 | loss: 0.54321 - acc: 0.8509 -- iter: 5000/8000\n",
            "Training Step: 9755  | total loss: \u001b[1m\u001b[32m0.54229\u001b[0m\u001b[0m | time: 3.245s\n",
            "| SGD | epoch: 868 | loss: 0.54229 - acc: 0.8524 -- iter: 6000/8000\n",
            "Training Step: 9756  | total loss: \u001b[1m\u001b[32m0.54398\u001b[0m\u001b[0m | time: 3.750s\n",
            "| SGD | epoch: 868 | loss: 0.54398 - acc: 0.8510 -- iter: 7000/8000\n",
            "Training Step: 9757  | total loss: \u001b[1m\u001b[32m0.54541\u001b[0m\u001b[0m | time: 4.256s\n",
            "| SGD | epoch: 868 | loss: 0.54541 - acc: 0.8507 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9758  | total loss: \u001b[1m\u001b[32m0.54338\u001b[0m\u001b[0m | time: 0.505s\n",
            "| SGD | epoch: 869 | loss: 0.54338 - acc: 0.8526 -- iter: 1000/8000\n",
            "Training Step: 9759  | total loss: \u001b[1m\u001b[32m0.54132\u001b[0m\u001b[0m | time: 1.009s\n",
            "| SGD | epoch: 869 | loss: 0.54132 - acc: 0.8537 -- iter: 2000/8000\n",
            "Training Step: 9760  | total loss: \u001b[1m\u001b[32m0.54089\u001b[0m\u001b[0m | time: 1.514s\n",
            "| SGD | epoch: 869 | loss: 0.54089 - acc: 0.8538 -- iter: 3000/8000\n",
            "Training Step: 9761  | total loss: \u001b[1m\u001b[32m0.54060\u001b[0m\u001b[0m | time: 2.030s\n",
            "| SGD | epoch: 869 | loss: 0.54060 - acc: 0.8542 -- iter: 4000/8000\n",
            "Training Step: 9762  | total loss: \u001b[1m\u001b[32m0.54191\u001b[0m\u001b[0m | time: 2.538s\n",
            "| SGD | epoch: 869 | loss: 0.54191 - acc: 0.8536 -- iter: 5000/8000\n",
            "Training Step: 9763  | total loss: \u001b[1m\u001b[32m0.54168\u001b[0m\u001b[0m | time: 3.047s\n",
            "| SGD | epoch: 869 | loss: 0.54168 - acc: 0.8548 -- iter: 6000/8000\n",
            "Training Step: 9764  | total loss: \u001b[1m\u001b[32m0.54169\u001b[0m\u001b[0m | time: 3.555s\n",
            "| SGD | epoch: 869 | loss: 0.54169 - acc: 0.8542 -- iter: 7000/8000\n",
            "Training Step: 9765  | total loss: \u001b[1m\u001b[32m0.54048\u001b[0m\u001b[0m | time: 4.059s\n",
            "| SGD | epoch: 869 | loss: 0.54048 - acc: 0.8551 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9766  | total loss: \u001b[1m\u001b[32m0.54217\u001b[0m\u001b[0m | time: 0.509s\n",
            "| SGD | epoch: 870 | loss: 0.54217 - acc: 0.8538 -- iter: 1000/8000\n",
            "Training Step: 9767  | total loss: \u001b[1m\u001b[32m0.54305\u001b[0m\u001b[0m | time: 1.021s\n",
            "| SGD | epoch: 870 | loss: 0.54305 - acc: 0.8522 -- iter: 2000/8000\n",
            "Training Step: 9768  | total loss: \u001b[1m\u001b[32m0.54250\u001b[0m\u001b[0m | time: 1.526s\n",
            "| SGD | epoch: 870 | loss: 0.54250 - acc: 0.8522 -- iter: 3000/8000\n",
            "Training Step: 9769  | total loss: \u001b[1m\u001b[32m0.54155\u001b[0m\u001b[0m | time: 2.030s\n",
            "| SGD | epoch: 870 | loss: 0.54155 - acc: 0.8536 -- iter: 4000/8000\n",
            "Training Step: 9770  | total loss: \u001b[1m\u001b[32m0.54093\u001b[0m\u001b[0m | time: 2.537s\n",
            "| SGD | epoch: 870 | loss: 0.54093 - acc: 0.8526 -- iter: 5000/8000\n",
            "Training Step: 9771  | total loss: \u001b[1m\u001b[32m0.54299\u001b[0m\u001b[0m | time: 3.045s\n",
            "| SGD | epoch: 870 | loss: 0.54299 - acc: 0.8524 -- iter: 6000/8000\n",
            "Training Step: 9772  | total loss: \u001b[1m\u001b[32m0.54409\u001b[0m\u001b[0m | time: 3.546s\n",
            "| SGD | epoch: 870 | loss: 0.54409 - acc: 0.8518 -- iter: 7000/8000\n",
            "Training Step: 9773  | total loss: \u001b[1m\u001b[32m0.54298\u001b[0m\u001b[0m | time: 4.056s\n",
            "| SGD | epoch: 870 | loss: 0.54298 - acc: 0.8519 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9774  | total loss: \u001b[1m\u001b[32m0.54245\u001b[0m\u001b[0m | time: 0.508s\n",
            "| SGD | epoch: 871 | loss: 0.54245 - acc: 0.8519 -- iter: 1000/8000\n",
            "Training Step: 9775  | total loss: \u001b[1m\u001b[32m0.54151\u001b[0m\u001b[0m | time: 1.026s\n",
            "| SGD | epoch: 871 | loss: 0.54151 - acc: 0.8535 -- iter: 2000/8000\n",
            "Training Step: 9776  | total loss: \u001b[1m\u001b[32m0.54105\u001b[0m\u001b[0m | time: 1.543s\n",
            "| SGD | epoch: 871 | loss: 0.54105 - acc: 0.8529 -- iter: 3000/8000\n",
            "Training Step: 9777  | total loss: \u001b[1m\u001b[32m0.53912\u001b[0m\u001b[0m | time: 2.063s\n",
            "| SGD | epoch: 871 | loss: 0.53912 - acc: 0.8532 -- iter: 4000/8000\n",
            "Training Step: 9778  | total loss: \u001b[1m\u001b[32m0.53920\u001b[0m\u001b[0m | time: 2.581s\n",
            "| SGD | epoch: 871 | loss: 0.53920 - acc: 0.8523 -- iter: 5000/8000\n",
            "Training Step: 9779  | total loss: \u001b[1m\u001b[32m0.54127\u001b[0m\u001b[0m | time: 3.100s\n",
            "| SGD | epoch: 871 | loss: 0.54127 - acc: 0.8510 -- iter: 6000/8000\n",
            "Training Step: 9780  | total loss: \u001b[1m\u001b[32m0.54103\u001b[0m\u001b[0m | time: 3.620s\n",
            "| SGD | epoch: 871 | loss: 0.54103 - acc: 0.8510 -- iter: 7000/8000\n",
            "Training Step: 9781  | total loss: \u001b[1m\u001b[32m0.54112\u001b[0m\u001b[0m | time: 4.129s\n",
            "| SGD | epoch: 871 | loss: 0.54112 - acc: 0.8492 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9782  | total loss: \u001b[1m\u001b[32m0.53720\u001b[0m\u001b[0m | time: 0.541s\n",
            "| SGD | epoch: 872 | loss: 0.53720 - acc: 0.8514 -- iter: 1000/8000\n",
            "Training Step: 9783  | total loss: \u001b[1m\u001b[32m0.54206\u001b[0m\u001b[0m | time: 1.059s\n",
            "| SGD | epoch: 872 | loss: 0.54206 - acc: 0.8493 -- iter: 2000/8000\n",
            "Training Step: 9784  | total loss: \u001b[1m\u001b[32m0.54112\u001b[0m\u001b[0m | time: 1.571s\n",
            "| SGD | epoch: 872 | loss: 0.54112 - acc: 0.8507 -- iter: 3000/8000\n",
            "Training Step: 9785  | total loss: \u001b[1m\u001b[32m0.54372\u001b[0m\u001b[0m | time: 2.076s\n",
            "| SGD | epoch: 872 | loss: 0.54372 - acc: 0.8502 -- iter: 4000/8000\n",
            "Training Step: 9786  | total loss: \u001b[1m\u001b[32m0.54570\u001b[0m\u001b[0m | time: 2.591s\n",
            "| SGD | epoch: 872 | loss: 0.54570 - acc: 0.8495 -- iter: 5000/8000\n",
            "Training Step: 9787  | total loss: \u001b[1m\u001b[32m0.54619\u001b[0m\u001b[0m | time: 3.096s\n",
            "| SGD | epoch: 872 | loss: 0.54619 - acc: 0.8503 -- iter: 6000/8000\n",
            "Training Step: 9788  | total loss: \u001b[1m\u001b[32m0.54544\u001b[0m\u001b[0m | time: 3.603s\n",
            "| SGD | epoch: 872 | loss: 0.54544 - acc: 0.8505 -- iter: 7000/8000\n",
            "Training Step: 9789  | total loss: \u001b[1m\u001b[32m0.54466\u001b[0m\u001b[0m | time: 4.106s\n",
            "| SGD | epoch: 872 | loss: 0.54466 - acc: 0.8502 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9790  | total loss: \u001b[1m\u001b[32m0.54328\u001b[0m\u001b[0m | time: 0.512s\n",
            "| SGD | epoch: 873 | loss: 0.54328 - acc: 0.8517 -- iter: 1000/8000\n",
            "Training Step: 9791  | total loss: \u001b[1m\u001b[32m0.54238\u001b[0m\u001b[0m | time: 1.150s\n",
            "| SGD | epoch: 873 | loss: 0.54238 - acc: 0.8527 -- iter: 2000/8000\n",
            "Training Step: 9792  | total loss: \u001b[1m\u001b[32m0.54231\u001b[0m\u001b[0m | time: 1.658s\n",
            "| SGD | epoch: 873 | loss: 0.54231 - acc: 0.8525 -- iter: 3000/8000\n",
            "Training Step: 9793  | total loss: \u001b[1m\u001b[32m0.54087\u001b[0m\u001b[0m | time: 2.164s\n",
            "| SGD | epoch: 873 | loss: 0.54087 - acc: 0.8530 -- iter: 4000/8000\n",
            "Training Step: 9794  | total loss: \u001b[1m\u001b[32m0.54076\u001b[0m\u001b[0m | time: 2.667s\n",
            "| SGD | epoch: 873 | loss: 0.54076 - acc: 0.8511 -- iter: 5000/8000\n",
            "Training Step: 9795  | total loss: \u001b[1m\u001b[32m0.53967\u001b[0m\u001b[0m | time: 3.171s\n",
            "| SGD | epoch: 873 | loss: 0.53967 - acc: 0.8512 -- iter: 6000/8000\n",
            "Training Step: 9796  | total loss: \u001b[1m\u001b[32m0.54084\u001b[0m\u001b[0m | time: 3.675s\n",
            "| SGD | epoch: 873 | loss: 0.54084 - acc: 0.8501 -- iter: 7000/8000\n",
            "Training Step: 9797  | total loss: \u001b[1m\u001b[32m0.54512\u001b[0m\u001b[0m | time: 4.185s\n",
            "| SGD | epoch: 873 | loss: 0.54512 - acc: 0.8490 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9798  | total loss: \u001b[1m\u001b[32m0.54485\u001b[0m\u001b[0m | time: 0.513s\n",
            "| SGD | epoch: 874 | loss: 0.54485 - acc: 0.8502 -- iter: 1000/8000\n",
            "Training Step: 9799  | total loss: \u001b[1m\u001b[32m0.54203\u001b[0m\u001b[0m | time: 1.021s\n",
            "| SGD | epoch: 874 | loss: 0.54203 - acc: 0.8524 -- iter: 2000/8000\n",
            "Training Step: 9800  | total loss: \u001b[1m\u001b[32m0.54213\u001b[0m\u001b[0m | time: 1.526s\n",
            "| SGD | epoch: 874 | loss: 0.54213 - acc: 0.8514 -- iter: 3000/8000\n",
            "Training Step: 9801  | total loss: \u001b[1m\u001b[32m0.54435\u001b[0m\u001b[0m | time: 2.038s\n",
            "| SGD | epoch: 874 | loss: 0.54435 - acc: 0.8506 -- iter: 4000/8000\n",
            "Training Step: 9802  | total loss: \u001b[1m\u001b[32m0.54676\u001b[0m\u001b[0m | time: 2.555s\n",
            "| SGD | epoch: 874 | loss: 0.54676 - acc: 0.8498 -- iter: 5000/8000\n",
            "Training Step: 9803  | total loss: \u001b[1m\u001b[32m0.54699\u001b[0m\u001b[0m | time: 3.070s\n",
            "| SGD | epoch: 874 | loss: 0.54699 - acc: 0.8489 -- iter: 6000/8000\n",
            "Training Step: 9804  | total loss: \u001b[1m\u001b[32m0.54634\u001b[0m\u001b[0m | time: 3.582s\n",
            "| SGD | epoch: 874 | loss: 0.54634 - acc: 0.8488 -- iter: 7000/8000\n",
            "Training Step: 9805  | total loss: \u001b[1m\u001b[32m0.54334\u001b[0m\u001b[0m | time: 4.104s\n",
            "| SGD | epoch: 874 | loss: 0.54334 - acc: 0.8508 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9806  | total loss: \u001b[1m\u001b[32m0.54333\u001b[0m\u001b[0m | time: 0.519s\n",
            "| SGD | epoch: 875 | loss: 0.54333 - acc: 0.8495 -- iter: 1000/8000\n",
            "Training Step: 9807  | total loss: \u001b[1m\u001b[32m0.54092\u001b[0m\u001b[0m | time: 1.036s\n",
            "| SGD | epoch: 875 | loss: 0.54092 - acc: 0.8508 -- iter: 2000/8000\n",
            "Training Step: 9808  | total loss: \u001b[1m\u001b[32m0.54346\u001b[0m\u001b[0m | time: 1.551s\n",
            "| SGD | epoch: 875 | loss: 0.54346 - acc: 0.8502 -- iter: 3000/8000\n",
            "Training Step: 9809  | total loss: \u001b[1m\u001b[32m0.54199\u001b[0m\u001b[0m | time: 2.071s\n",
            "| SGD | epoch: 875 | loss: 0.54199 - acc: 0.8513 -- iter: 4000/8000\n",
            "Training Step: 9810  | total loss: \u001b[1m\u001b[32m0.54177\u001b[0m\u001b[0m | time: 2.581s\n",
            "| SGD | epoch: 875 | loss: 0.54177 - acc: 0.8522 -- iter: 5000/8000\n",
            "Training Step: 9811  | total loss: \u001b[1m\u001b[32m0.53838\u001b[0m\u001b[0m | time: 3.084s\n",
            "| SGD | epoch: 875 | loss: 0.53838 - acc: 0.8535 -- iter: 6000/8000\n",
            "Training Step: 9812  | total loss: \u001b[1m\u001b[32m0.54302\u001b[0m\u001b[0m | time: 3.592s\n",
            "| SGD | epoch: 875 | loss: 0.54302 - acc: 0.8518 -- iter: 7000/8000\n",
            "Training Step: 9813  | total loss: \u001b[1m\u001b[32m0.54592\u001b[0m\u001b[0m | time: 4.101s\n",
            "| SGD | epoch: 875 | loss: 0.54592 - acc: 0.8524 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9814  | total loss: \u001b[1m\u001b[32m0.54204\u001b[0m\u001b[0m | time: 0.505s\n",
            "| SGD | epoch: 876 | loss: 0.54204 - acc: 0.8525 -- iter: 1000/8000\n",
            "Training Step: 9815  | total loss: \u001b[1m\u001b[32m0.54635\u001b[0m\u001b[0m | time: 1.011s\n",
            "| SGD | epoch: 876 | loss: 0.54635 - acc: 0.8500 -- iter: 2000/8000\n",
            "Training Step: 9816  | total loss: \u001b[1m\u001b[32m0.54622\u001b[0m\u001b[0m | time: 1.517s\n",
            "| SGD | epoch: 876 | loss: 0.54622 - acc: 0.8501 -- iter: 3000/8000\n",
            "Training Step: 9817  | total loss: \u001b[1m\u001b[32m0.54778\u001b[0m\u001b[0m | time: 2.023s\n",
            "| SGD | epoch: 876 | loss: 0.54778 - acc: 0.8479 -- iter: 4000/8000\n",
            "Training Step: 9818  | total loss: \u001b[1m\u001b[32m0.54778\u001b[0m\u001b[0m | time: 2.532s\n",
            "| SGD | epoch: 876 | loss: 0.54778 - acc: 0.8481 -- iter: 5000/8000\n",
            "Training Step: 9819  | total loss: \u001b[1m\u001b[32m0.54507\u001b[0m\u001b[0m | time: 3.035s\n",
            "| SGD | epoch: 876 | loss: 0.54507 - acc: 0.8476 -- iter: 6000/8000\n",
            "Training Step: 9820  | total loss: \u001b[1m\u001b[32m0.54873\u001b[0m\u001b[0m | time: 3.546s\n",
            "| SGD | epoch: 876 | loss: 0.54873 - acc: 0.8486 -- iter: 7000/8000\n",
            "Training Step: 9821  | total loss: \u001b[1m\u001b[32m0.54801\u001b[0m\u001b[0m | time: 4.053s\n",
            "| SGD | epoch: 876 | loss: 0.54801 - acc: 0.8486 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9822  | total loss: \u001b[1m\u001b[32m0.54678\u001b[0m\u001b[0m | time: 0.513s\n",
            "| SGD | epoch: 877 | loss: 0.54678 - acc: 0.8496 -- iter: 1000/8000\n",
            "Training Step: 9823  | total loss: \u001b[1m\u001b[32m0.54409\u001b[0m\u001b[0m | time: 1.021s\n",
            "| SGD | epoch: 877 | loss: 0.54409 - acc: 0.8496 -- iter: 2000/8000\n",
            "Training Step: 9824  | total loss: \u001b[1m\u001b[32m0.54334\u001b[0m\u001b[0m | time: 1.531s\n",
            "| SGD | epoch: 877 | loss: 0.54334 - acc: 0.8506 -- iter: 3000/8000\n",
            "Training Step: 9825  | total loss: \u001b[1m\u001b[32m0.54489\u001b[0m\u001b[0m | time: 2.040s\n",
            "| SGD | epoch: 877 | loss: 0.54489 - acc: 0.8496 -- iter: 4000/8000\n",
            "Training Step: 9826  | total loss: \u001b[1m\u001b[32m0.54432\u001b[0m\u001b[0m | time: 2.548s\n",
            "| SGD | epoch: 877 | loss: 0.54432 - acc: 0.8496 -- iter: 5000/8000\n",
            "Training Step: 9827  | total loss: \u001b[1m\u001b[32m0.54224\u001b[0m\u001b[0m | time: 3.058s\n",
            "| SGD | epoch: 877 | loss: 0.54224 - acc: 0.8511 -- iter: 6000/8000\n",
            "Training Step: 9828  | total loss: \u001b[1m\u001b[32m0.54084\u001b[0m\u001b[0m | time: 3.570s\n",
            "| SGD | epoch: 877 | loss: 0.54084 - acc: 0.8520 -- iter: 7000/8000\n",
            "Training Step: 9829  | total loss: \u001b[1m\u001b[32m0.54432\u001b[0m\u001b[0m | time: 4.086s\n",
            "| SGD | epoch: 877 | loss: 0.54432 - acc: 0.8516 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9830  | total loss: \u001b[1m\u001b[32m0.54506\u001b[0m\u001b[0m | time: 0.528s\n",
            "| SGD | epoch: 878 | loss: 0.54506 - acc: 0.8525 -- iter: 1000/8000\n",
            "Training Step: 9831  | total loss: \u001b[1m\u001b[32m0.54613\u001b[0m\u001b[0m | time: 1.049s\n",
            "| SGD | epoch: 878 | loss: 0.54613 - acc: 0.8524 -- iter: 2000/8000\n",
            "Training Step: 9832  | total loss: \u001b[1m\u001b[32m0.54535\u001b[0m\u001b[0m | time: 1.585s\n",
            "| SGD | epoch: 878 | loss: 0.54535 - acc: 0.8516 -- iter: 3000/8000\n",
            "Training Step: 9833  | total loss: \u001b[1m\u001b[32m0.54145\u001b[0m\u001b[0m | time: 2.103s\n",
            "| SGD | epoch: 878 | loss: 0.54145 - acc: 0.8529 -- iter: 4000/8000\n",
            "Training Step: 9834  | total loss: \u001b[1m\u001b[32m0.54352\u001b[0m\u001b[0m | time: 2.633s\n",
            "| SGD | epoch: 878 | loss: 0.54352 - acc: 0.8525 -- iter: 5000/8000\n",
            "Training Step: 9835  | total loss: \u001b[1m\u001b[32m0.54021\u001b[0m\u001b[0m | time: 3.159s\n",
            "| SGD | epoch: 878 | loss: 0.54021 - acc: 0.8531 -- iter: 6000/8000\n",
            "Training Step: 9836  | total loss: \u001b[1m\u001b[32m0.54035\u001b[0m\u001b[0m | time: 3.685s\n",
            "| SGD | epoch: 878 | loss: 0.54035 - acc: 0.8523 -- iter: 7000/8000\n",
            "Training Step: 9837  | total loss: \u001b[1m\u001b[32m0.54059\u001b[0m\u001b[0m | time: 4.207s\n",
            "| SGD | epoch: 878 | loss: 0.54059 - acc: 0.8531 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9838  | total loss: \u001b[1m\u001b[32m0.54080\u001b[0m\u001b[0m | time: 0.522s\n",
            "| SGD | epoch: 879 | loss: 0.54080 - acc: 0.8518 -- iter: 1000/8000\n",
            "Training Step: 9839  | total loss: \u001b[1m\u001b[32m0.54176\u001b[0m\u001b[0m | time: 1.046s\n",
            "| SGD | epoch: 879 | loss: 0.54176 - acc: 0.8502 -- iter: 2000/8000\n",
            "Training Step: 9840  | total loss: \u001b[1m\u001b[32m0.54178\u001b[0m\u001b[0m | time: 1.578s\n",
            "| SGD | epoch: 879 | loss: 0.54178 - acc: 0.8495 -- iter: 3000/8000\n",
            "Training Step: 9841  | total loss: \u001b[1m\u001b[32m0.54250\u001b[0m\u001b[0m | time: 2.097s\n",
            "| SGD | epoch: 879 | loss: 0.54250 - acc: 0.8502 -- iter: 4000/8000\n",
            "Training Step: 9842  | total loss: \u001b[1m\u001b[32m0.54121\u001b[0m\u001b[0m | time: 2.619s\n",
            "| SGD | epoch: 879 | loss: 0.54121 - acc: 0.8510 -- iter: 5000/8000\n",
            "Training Step: 9843  | total loss: \u001b[1m\u001b[32m0.54014\u001b[0m\u001b[0m | time: 3.147s\n",
            "| SGD | epoch: 879 | loss: 0.54014 - acc: 0.8520 -- iter: 6000/8000\n",
            "Training Step: 9844  | total loss: \u001b[1m\u001b[32m0.53836\u001b[0m\u001b[0m | time: 3.660s\n",
            "| SGD | epoch: 879 | loss: 0.53836 - acc: 0.8536 -- iter: 7000/8000\n",
            "Training Step: 9845  | total loss: \u001b[1m\u001b[32m0.53643\u001b[0m\u001b[0m | time: 4.189s\n",
            "| SGD | epoch: 879 | loss: 0.53643 - acc: 0.8539 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9846  | total loss: \u001b[1m\u001b[32m0.53543\u001b[0m\u001b[0m | time: 0.518s\n",
            "| SGD | epoch: 880 | loss: 0.53543 - acc: 0.8543 -- iter: 1000/8000\n",
            "Training Step: 9847  | total loss: \u001b[1m\u001b[32m0.53443\u001b[0m\u001b[0m | time: 1.048s\n",
            "| SGD | epoch: 880 | loss: 0.53443 - acc: 0.8530 -- iter: 2000/8000\n",
            "Training Step: 9848  | total loss: \u001b[1m\u001b[32m0.53627\u001b[0m\u001b[0m | time: 1.568s\n",
            "| SGD | epoch: 880 | loss: 0.53627 - acc: 0.8540 -- iter: 3000/8000\n",
            "Training Step: 9849  | total loss: \u001b[1m\u001b[32m0.53803\u001b[0m\u001b[0m | time: 2.084s\n",
            "| SGD | epoch: 880 | loss: 0.53803 - acc: 0.8545 -- iter: 4000/8000\n",
            "Training Step: 9850  | total loss: \u001b[1m\u001b[32m0.53952\u001b[0m\u001b[0m | time: 2.612s\n",
            "| SGD | epoch: 880 | loss: 0.53952 - acc: 0.8536 -- iter: 5000/8000\n",
            "Training Step: 9851  | total loss: \u001b[1m\u001b[32m0.54330\u001b[0m\u001b[0m | time: 3.138s\n",
            "| SGD | epoch: 880 | loss: 0.54330 - acc: 0.8526 -- iter: 6000/8000\n",
            "Training Step: 9852  | total loss: \u001b[1m\u001b[32m0.54403\u001b[0m\u001b[0m | time: 3.653s\n",
            "| SGD | epoch: 880 | loss: 0.54403 - acc: 0.8518 -- iter: 7000/8000\n",
            "Training Step: 9853  | total loss: \u001b[1m\u001b[32m0.54206\u001b[0m\u001b[0m | time: 4.167s\n",
            "| SGD | epoch: 880 | loss: 0.54206 - acc: 0.8541 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9854  | total loss: \u001b[1m\u001b[32m0.54048\u001b[0m\u001b[0m | time: 0.515s\n",
            "| SGD | epoch: 881 | loss: 0.54048 - acc: 0.8535 -- iter: 1000/8000\n",
            "Training Step: 9855  | total loss: \u001b[1m\u001b[32m0.53985\u001b[0m\u001b[0m | time: 1.045s\n",
            "| SGD | epoch: 881 | loss: 0.53985 - acc: 0.8537 -- iter: 2000/8000\n",
            "Training Step: 9856  | total loss: \u001b[1m\u001b[32m0.54042\u001b[0m\u001b[0m | time: 1.561s\n",
            "| SGD | epoch: 881 | loss: 0.54042 - acc: 0.8532 -- iter: 3000/8000\n",
            "Training Step: 9857  | total loss: \u001b[1m\u001b[32m0.54250\u001b[0m\u001b[0m | time: 2.092s\n",
            "| SGD | epoch: 881 | loss: 0.54250 - acc: 0.8531 -- iter: 4000/8000\n",
            "Training Step: 9858  | total loss: \u001b[1m\u001b[32m0.54373\u001b[0m\u001b[0m | time: 2.617s\n",
            "| SGD | epoch: 881 | loss: 0.54373 - acc: 0.8534 -- iter: 5000/8000\n",
            "Training Step: 9859  | total loss: \u001b[1m\u001b[32m0.54143\u001b[0m\u001b[0m | time: 3.146s\n",
            "| SGD | epoch: 881 | loss: 0.54143 - acc: 0.8538 -- iter: 6000/8000\n",
            "Training Step: 9860  | total loss: \u001b[1m\u001b[32m0.54307\u001b[0m\u001b[0m | time: 3.668s\n",
            "| SGD | epoch: 881 | loss: 0.54307 - acc: 0.8515 -- iter: 7000/8000\n",
            "Training Step: 9861  | total loss: \u001b[1m\u001b[32m0.54133\u001b[0m\u001b[0m | time: 4.205s\n",
            "| SGD | epoch: 881 | loss: 0.54133 - acc: 0.8527 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9862  | total loss: \u001b[1m\u001b[32m0.54003\u001b[0m\u001b[0m | time: 0.532s\n",
            "| SGD | epoch: 882 | loss: 0.54003 - acc: 0.8520 -- iter: 1000/8000\n",
            "Training Step: 9863  | total loss: \u001b[1m\u001b[32m0.53914\u001b[0m\u001b[0m | time: 1.049s\n",
            "| SGD | epoch: 882 | loss: 0.53914 - acc: 0.8525 -- iter: 2000/8000\n",
            "Training Step: 9864  | total loss: \u001b[1m\u001b[32m0.53987\u001b[0m\u001b[0m | time: 1.566s\n",
            "| SGD | epoch: 882 | loss: 0.53987 - acc: 0.8511 -- iter: 3000/8000\n",
            "Training Step: 9865  | total loss: \u001b[1m\u001b[32m0.54018\u001b[0m\u001b[0m | time: 2.085s\n",
            "| SGD | epoch: 882 | loss: 0.54018 - acc: 0.8516 -- iter: 4000/8000\n",
            "Training Step: 9866  | total loss: \u001b[1m\u001b[32m0.54304\u001b[0m\u001b[0m | time: 2.593s\n",
            "| SGD | epoch: 882 | loss: 0.54304 - acc: 0.8496 -- iter: 5000/8000\n",
            "Training Step: 9867  | total loss: \u001b[1m\u001b[32m0.54482\u001b[0m\u001b[0m | time: 3.116s\n",
            "| SGD | epoch: 882 | loss: 0.54482 - acc: 0.8489 -- iter: 6000/8000\n",
            "Training Step: 9868  | total loss: \u001b[1m\u001b[32m0.54193\u001b[0m\u001b[0m | time: 3.640s\n",
            "| SGD | epoch: 882 | loss: 0.54193 - acc: 0.8508 -- iter: 7000/8000\n",
            "Training Step: 9869  | total loss: \u001b[1m\u001b[32m0.54408\u001b[0m\u001b[0m | time: 4.153s\n",
            "| SGD | epoch: 882 | loss: 0.54408 - acc: 0.8494 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9870  | total loss: \u001b[1m\u001b[32m0.54413\u001b[0m\u001b[0m | time: 0.517s\n",
            "| SGD | epoch: 883 | loss: 0.54413 - acc: 0.8495 -- iter: 1000/8000\n",
            "Training Step: 9871  | total loss: \u001b[1m\u001b[32m0.54140\u001b[0m\u001b[0m | time: 1.049s\n",
            "| SGD | epoch: 883 | loss: 0.54140 - acc: 0.8505 -- iter: 2000/8000\n",
            "Training Step: 9872  | total loss: \u001b[1m\u001b[32m0.54104\u001b[0m\u001b[0m | time: 1.564s\n",
            "| SGD | epoch: 883 | loss: 0.54104 - acc: 0.8518 -- iter: 3000/8000\n",
            "Training Step: 9873  | total loss: \u001b[1m\u001b[32m0.53998\u001b[0m\u001b[0m | time: 2.084s\n",
            "| SGD | epoch: 883 | loss: 0.53998 - acc: 0.8532 -- iter: 4000/8000\n",
            "Training Step: 9874  | total loss: \u001b[1m\u001b[32m0.53905\u001b[0m\u001b[0m | time: 2.586s\n",
            "| SGD | epoch: 883 | loss: 0.53905 - acc: 0.8533 -- iter: 5000/8000\n",
            "Training Step: 9875  | total loss: \u001b[1m\u001b[32m0.54091\u001b[0m\u001b[0m | time: 3.113s\n",
            "| SGD | epoch: 883 | loss: 0.54091 - acc: 0.8522 -- iter: 6000/8000\n",
            "Training Step: 9876  | total loss: \u001b[1m\u001b[32m0.54154\u001b[0m\u001b[0m | time: 3.634s\n",
            "| SGD | epoch: 883 | loss: 0.54154 - acc: 0.8522 -- iter: 7000/8000\n",
            "Training Step: 9877  | total loss: \u001b[1m\u001b[32m0.54360\u001b[0m\u001b[0m | time: 4.152s\n",
            "| SGD | epoch: 883 | loss: 0.54360 - acc: 0.8505 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9878  | total loss: \u001b[1m\u001b[32m0.54186\u001b[0m\u001b[0m | time: 0.517s\n",
            "| SGD | epoch: 884 | loss: 0.54186 - acc: 0.8519 -- iter: 1000/8000\n",
            "Training Step: 9879  | total loss: \u001b[1m\u001b[32m0.54282\u001b[0m\u001b[0m | time: 1.043s\n",
            "| SGD | epoch: 884 | loss: 0.54282 - acc: 0.8530 -- iter: 2000/8000\n",
            "Training Step: 9880  | total loss: \u001b[1m\u001b[32m0.54412\u001b[0m\u001b[0m | time: 1.555s\n",
            "| SGD | epoch: 884 | loss: 0.54412 - acc: 0.8513 -- iter: 3000/8000\n",
            "Training Step: 9881  | total loss: \u001b[1m\u001b[32m0.54366\u001b[0m\u001b[0m | time: 2.078s\n",
            "| SGD | epoch: 884 | loss: 0.54366 - acc: 0.8523 -- iter: 4000/8000\n",
            "Training Step: 9882  | total loss: \u001b[1m\u001b[32m0.54269\u001b[0m\u001b[0m | time: 2.606s\n",
            "| SGD | epoch: 884 | loss: 0.54269 - acc: 0.8517 -- iter: 5000/8000\n",
            "Training Step: 9883  | total loss: \u001b[1m\u001b[32m0.53922\u001b[0m\u001b[0m | time: 3.138s\n",
            "| SGD | epoch: 884 | loss: 0.53922 - acc: 0.8524 -- iter: 6000/8000\n",
            "Training Step: 9884  | total loss: \u001b[1m\u001b[32m0.53783\u001b[0m\u001b[0m | time: 3.666s\n",
            "| SGD | epoch: 884 | loss: 0.53783 - acc: 0.8526 -- iter: 7000/8000\n",
            "Training Step: 9885  | total loss: \u001b[1m\u001b[32m0.53568\u001b[0m\u001b[0m | time: 4.189s\n",
            "| SGD | epoch: 884 | loss: 0.53568 - acc: 0.8531 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9886  | total loss: \u001b[1m\u001b[32m0.53672\u001b[0m\u001b[0m | time: 0.524s\n",
            "| SGD | epoch: 885 | loss: 0.53672 - acc: 0.8530 -- iter: 1000/8000\n",
            "Training Step: 9887  | total loss: \u001b[1m\u001b[32m0.53577\u001b[0m\u001b[0m | time: 1.054s\n",
            "| SGD | epoch: 885 | loss: 0.53577 - acc: 0.8523 -- iter: 2000/8000\n",
            "Training Step: 9888  | total loss: \u001b[1m\u001b[32m0.53439\u001b[0m\u001b[0m | time: 1.585s\n",
            "| SGD | epoch: 885 | loss: 0.53439 - acc: 0.8524 -- iter: 3000/8000\n",
            "Training Step: 9889  | total loss: \u001b[1m\u001b[32m0.53705\u001b[0m\u001b[0m | time: 2.103s\n",
            "| SGD | epoch: 885 | loss: 0.53705 - acc: 0.8518 -- iter: 4000/8000\n",
            "Training Step: 9890  | total loss: \u001b[1m\u001b[32m0.53687\u001b[0m\u001b[0m | time: 2.632s\n",
            "| SGD | epoch: 885 | loss: 0.53687 - acc: 0.8515 -- iter: 5000/8000\n",
            "Training Step: 9891  | total loss: \u001b[1m\u001b[32m0.53402\u001b[0m\u001b[0m | time: 3.159s\n",
            "| SGD | epoch: 885 | loss: 0.53402 - acc: 0.8530 -- iter: 6000/8000\n",
            "Training Step: 9892  | total loss: \u001b[1m\u001b[32m0.53304\u001b[0m\u001b[0m | time: 3.677s\n",
            "| SGD | epoch: 885 | loss: 0.53304 - acc: 0.8539 -- iter: 7000/8000\n",
            "Training Step: 9893  | total loss: \u001b[1m\u001b[32m0.53468\u001b[0m\u001b[0m | time: 4.200s\n",
            "| SGD | epoch: 885 | loss: 0.53468 - acc: 0.8547 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9894  | total loss: \u001b[1m\u001b[32m0.53529\u001b[0m\u001b[0m | time: 0.526s\n",
            "| SGD | epoch: 886 | loss: 0.53529 - acc: 0.8546 -- iter: 1000/8000\n",
            "Training Step: 9895  | total loss: \u001b[1m\u001b[32m0.53816\u001b[0m\u001b[0m | time: 1.044s\n",
            "| SGD | epoch: 886 | loss: 0.53816 - acc: 0.8536 -- iter: 2000/8000\n",
            "Training Step: 9896  | total loss: \u001b[1m\u001b[32m0.53758\u001b[0m\u001b[0m | time: 1.575s\n",
            "| SGD | epoch: 886 | loss: 0.53758 - acc: 0.8525 -- iter: 3000/8000\n",
            "Training Step: 9897  | total loss: \u001b[1m\u001b[32m0.53528\u001b[0m\u001b[0m | time: 2.093s\n",
            "| SGD | epoch: 886 | loss: 0.53528 - acc: 0.8528 -- iter: 4000/8000\n",
            "Training Step: 9898  | total loss: \u001b[1m\u001b[32m0.53713\u001b[0m\u001b[0m | time: 2.617s\n",
            "| SGD | epoch: 886 | loss: 0.53713 - acc: 0.8521 -- iter: 5000/8000\n",
            "Training Step: 9899  | total loss: \u001b[1m\u001b[32m0.53442\u001b[0m\u001b[0m | time: 3.135s\n",
            "| SGD | epoch: 886 | loss: 0.53442 - acc: 0.8550 -- iter: 6000/8000\n",
            "Training Step: 9900  | total loss: \u001b[1m\u001b[32m0.53356\u001b[0m\u001b[0m | time: 3.658s\n",
            "| SGD | epoch: 886 | loss: 0.53356 - acc: 0.8547 -- iter: 7000/8000\n",
            "Training Step: 9901  | total loss: \u001b[1m\u001b[32m0.53292\u001b[0m\u001b[0m | time: 4.171s\n",
            "| SGD | epoch: 886 | loss: 0.53292 - acc: 0.8558 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9902  | total loss: \u001b[1m\u001b[32m0.53721\u001b[0m\u001b[0m | time: 0.528s\n",
            "| SGD | epoch: 887 | loss: 0.53721 - acc: 0.8536 -- iter: 1000/8000\n",
            "Training Step: 9903  | total loss: \u001b[1m\u001b[32m0.54097\u001b[0m\u001b[0m | time: 1.037s\n",
            "| SGD | epoch: 887 | loss: 0.54097 - acc: 0.8522 -- iter: 2000/8000\n",
            "Training Step: 9904  | total loss: \u001b[1m\u001b[32m0.54256\u001b[0m\u001b[0m | time: 1.553s\n",
            "| SGD | epoch: 887 | loss: 0.54256 - acc: 0.8508 -- iter: 3000/8000\n",
            "Training Step: 9905  | total loss: \u001b[1m\u001b[32m0.54092\u001b[0m\u001b[0m | time: 2.076s\n",
            "| SGD | epoch: 887 | loss: 0.54092 - acc: 0.8511 -- iter: 4000/8000\n",
            "Training Step: 9906  | total loss: \u001b[1m\u001b[32m0.54042\u001b[0m\u001b[0m | time: 2.593s\n",
            "| SGD | epoch: 887 | loss: 0.54042 - acc: 0.8523 -- iter: 5000/8000\n",
            "Training Step: 9907  | total loss: \u001b[1m\u001b[32m0.53948\u001b[0m\u001b[0m | time: 3.101s\n",
            "| SGD | epoch: 887 | loss: 0.53948 - acc: 0.8534 -- iter: 6000/8000\n",
            "Training Step: 9908  | total loss: \u001b[1m\u001b[32m0.54040\u001b[0m\u001b[0m | time: 3.628s\n",
            "| SGD | epoch: 887 | loss: 0.54040 - acc: 0.8524 -- iter: 7000/8000\n",
            "Training Step: 9909  | total loss: \u001b[1m\u001b[32m0.53863\u001b[0m\u001b[0m | time: 4.156s\n",
            "| SGD | epoch: 887 | loss: 0.53863 - acc: 0.8532 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9910  | total loss: \u001b[1m\u001b[32m0.53806\u001b[0m\u001b[0m | time: 0.526s\n",
            "| SGD | epoch: 888 | loss: 0.53806 - acc: 0.8518 -- iter: 1000/8000\n",
            "Training Step: 9911  | total loss: \u001b[1m\u001b[32m0.53549\u001b[0m\u001b[0m | time: 1.046s\n",
            "| SGD | epoch: 888 | loss: 0.53549 - acc: 0.8526 -- iter: 2000/8000\n",
            "Training Step: 9912  | total loss: \u001b[1m\u001b[32m0.53338\u001b[0m\u001b[0m | time: 1.589s\n",
            "| SGD | epoch: 888 | loss: 0.53338 - acc: 0.8524 -- iter: 3000/8000\n",
            "Training Step: 9913  | total loss: \u001b[1m\u001b[32m0.53652\u001b[0m\u001b[0m | time: 2.110s\n",
            "| SGD | epoch: 888 | loss: 0.53652 - acc: 0.8528 -- iter: 4000/8000\n",
            "Training Step: 9914  | total loss: \u001b[1m\u001b[32m0.53492\u001b[0m\u001b[0m | time: 2.638s\n",
            "| SGD | epoch: 888 | loss: 0.53492 - acc: 0.8533 -- iter: 5000/8000\n",
            "Training Step: 9915  | total loss: \u001b[1m\u001b[32m0.53316\u001b[0m\u001b[0m | time: 3.154s\n",
            "| SGD | epoch: 888 | loss: 0.53316 - acc: 0.8538 -- iter: 6000/8000\n",
            "Training Step: 9916  | total loss: \u001b[1m\u001b[32m0.53206\u001b[0m\u001b[0m | time: 3.677s\n",
            "| SGD | epoch: 888 | loss: 0.53206 - acc: 0.8530 -- iter: 7000/8000\n",
            "Training Step: 9917  | total loss: \u001b[1m\u001b[32m0.53000\u001b[0m\u001b[0m | time: 4.185s\n",
            "| SGD | epoch: 888 | loss: 0.53000 - acc: 0.8545 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9918  | total loss: \u001b[1m\u001b[32m0.53205\u001b[0m\u001b[0m | time: 0.514s\n",
            "| SGD | epoch: 889 | loss: 0.53205 - acc: 0.8543 -- iter: 1000/8000\n",
            "Training Step: 9919  | total loss: \u001b[1m\u001b[32m0.53246\u001b[0m\u001b[0m | time: 1.018s\n",
            "| SGD | epoch: 889 | loss: 0.53246 - acc: 0.8533 -- iter: 2000/8000\n",
            "Training Step: 9920  | total loss: \u001b[1m\u001b[32m0.53508\u001b[0m\u001b[0m | time: 1.543s\n",
            "| SGD | epoch: 889 | loss: 0.53508 - acc: 0.8524 -- iter: 3000/8000\n",
            "Training Step: 9921  | total loss: \u001b[1m\u001b[32m0.53705\u001b[0m\u001b[0m | time: 2.048s\n",
            "| SGD | epoch: 889 | loss: 0.53705 - acc: 0.8524 -- iter: 4000/8000\n",
            "Training Step: 9922  | total loss: \u001b[1m\u001b[32m0.53833\u001b[0m\u001b[0m | time: 2.551s\n",
            "| SGD | epoch: 889 | loss: 0.53833 - acc: 0.8524 -- iter: 5000/8000\n",
            "Training Step: 9923  | total loss: \u001b[1m\u001b[32m0.53853\u001b[0m\u001b[0m | time: 3.065s\n",
            "| SGD | epoch: 889 | loss: 0.53853 - acc: 0.8523 -- iter: 6000/8000\n",
            "Training Step: 9924  | total loss: \u001b[1m\u001b[32m0.53623\u001b[0m\u001b[0m | time: 3.582s\n",
            "| SGD | epoch: 889 | loss: 0.53623 - acc: 0.8531 -- iter: 7000/8000\n",
            "Training Step: 9925  | total loss: \u001b[1m\u001b[32m0.53623\u001b[0m\u001b[0m | time: 4.092s\n",
            "| SGD | epoch: 889 | loss: 0.53623 - acc: 0.8531 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9926  | total loss: \u001b[1m\u001b[32m0.53937\u001b[0m\u001b[0m | time: 0.523s\n",
            "| SGD | epoch: 890 | loss: 0.53937 - acc: 0.8516 -- iter: 1000/8000\n",
            "Training Step: 9927  | total loss: \u001b[1m\u001b[32m0.54007\u001b[0m\u001b[0m | time: 1.028s\n",
            "| SGD | epoch: 890 | loss: 0.54007 - acc: 0.8522 -- iter: 2000/8000\n",
            "Training Step: 9928  | total loss: \u001b[1m\u001b[32m0.54221\u001b[0m\u001b[0m | time: 1.544s\n",
            "| SGD | epoch: 890 | loss: 0.54221 - acc: 0.8496 -- iter: 3000/8000\n",
            "Training Step: 9929  | total loss: \u001b[1m\u001b[32m0.54000\u001b[0m\u001b[0m | time: 2.048s\n",
            "| SGD | epoch: 890 | loss: 0.54000 - acc: 0.8511 -- iter: 4000/8000\n",
            "Training Step: 9930  | total loss: \u001b[1m\u001b[32m0.53807\u001b[0m\u001b[0m | time: 2.573s\n",
            "| SGD | epoch: 890 | loss: 0.53807 - acc: 0.8520 -- iter: 5000/8000\n",
            "Training Step: 9931  | total loss: \u001b[1m\u001b[32m0.53698\u001b[0m\u001b[0m | time: 3.097s\n",
            "| SGD | epoch: 890 | loss: 0.53698 - acc: 0.8520 -- iter: 6000/8000\n",
            "Training Step: 9932  | total loss: \u001b[1m\u001b[32m0.53236\u001b[0m\u001b[0m | time: 3.620s\n",
            "| SGD | epoch: 890 | loss: 0.53236 - acc: 0.8532 -- iter: 7000/8000\n",
            "Training Step: 9933  | total loss: \u001b[1m\u001b[32m0.53525\u001b[0m\u001b[0m | time: 4.141s\n",
            "| SGD | epoch: 890 | loss: 0.53525 - acc: 0.8514 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9934  | total loss: \u001b[1m\u001b[32m0.53255\u001b[0m\u001b[0m | time: 0.520s\n",
            "| SGD | epoch: 891 | loss: 0.53255 - acc: 0.8527 -- iter: 1000/8000\n",
            "Training Step: 9935  | total loss: \u001b[1m\u001b[32m0.53020\u001b[0m\u001b[0m | time: 1.041s\n",
            "| SGD | epoch: 891 | loss: 0.53020 - acc: 0.8531 -- iter: 2000/8000\n",
            "Training Step: 9936  | total loss: \u001b[1m\u001b[32m0.52628\u001b[0m\u001b[0m | time: 1.564s\n",
            "| SGD | epoch: 891 | loss: 0.52628 - acc: 0.8554 -- iter: 3000/8000\n",
            "Training Step: 9937  | total loss: \u001b[1m\u001b[32m0.52997\u001b[0m\u001b[0m | time: 2.091s\n",
            "| SGD | epoch: 891 | loss: 0.52997 - acc: 0.8546 -- iter: 4000/8000\n",
            "Training Step: 9938  | total loss: \u001b[1m\u001b[32m0.52930\u001b[0m\u001b[0m | time: 2.618s\n",
            "| SGD | epoch: 891 | loss: 0.52930 - acc: 0.8551 -- iter: 5000/8000\n",
            "Training Step: 9939  | total loss: \u001b[1m\u001b[32m0.52761\u001b[0m\u001b[0m | time: 3.141s\n",
            "| SGD | epoch: 891 | loss: 0.52761 - acc: 0.8582 -- iter: 6000/8000\n",
            "Training Step: 9940  | total loss: \u001b[1m\u001b[32m0.53004\u001b[0m\u001b[0m | time: 3.661s\n",
            "| SGD | epoch: 891 | loss: 0.53004 - acc: 0.8562 -- iter: 7000/8000\n",
            "Training Step: 9941  | total loss: \u001b[1m\u001b[32m0.53428\u001b[0m\u001b[0m | time: 4.194s\n",
            "| SGD | epoch: 891 | loss: 0.53428 - acc: 0.8545 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9942  | total loss: \u001b[1m\u001b[32m0.53634\u001b[0m\u001b[0m | time: 0.521s\n",
            "| SGD | epoch: 892 | loss: 0.53634 - acc: 0.8546 -- iter: 1000/8000\n",
            "Training Step: 9943  | total loss: \u001b[1m\u001b[32m0.53551\u001b[0m\u001b[0m | time: 1.043s\n",
            "| SGD | epoch: 892 | loss: 0.53551 - acc: 0.8558 -- iter: 2000/8000\n",
            "Training Step: 9944  | total loss: \u001b[1m\u001b[32m0.53482\u001b[0m\u001b[0m | time: 1.562s\n",
            "| SGD | epoch: 892 | loss: 0.53482 - acc: 0.8566 -- iter: 3000/8000\n",
            "Training Step: 9945  | total loss: \u001b[1m\u001b[32m0.53223\u001b[0m\u001b[0m | time: 2.082s\n",
            "| SGD | epoch: 892 | loss: 0.53223 - acc: 0.8574 -- iter: 4000/8000\n",
            "Training Step: 9946  | total loss: \u001b[1m\u001b[32m0.53364\u001b[0m\u001b[0m | time: 2.602s\n",
            "| SGD | epoch: 892 | loss: 0.53364 - acc: 0.8559 -- iter: 5000/8000\n",
            "Training Step: 9947  | total loss: \u001b[1m\u001b[32m0.53228\u001b[0m\u001b[0m | time: 3.238s\n",
            "| SGD | epoch: 892 | loss: 0.53228 - acc: 0.8563 -- iter: 6000/8000\n",
            "Training Step: 9948  | total loss: \u001b[1m\u001b[32m0.53107\u001b[0m\u001b[0m | time: 3.757s\n",
            "| SGD | epoch: 892 | loss: 0.53107 - acc: 0.8556 -- iter: 7000/8000\n",
            "Training Step: 9949  | total loss: \u001b[1m\u001b[32m0.53208\u001b[0m\u001b[0m | time: 4.294s\n",
            "| SGD | epoch: 892 | loss: 0.53208 - acc: 0.8546 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9950  | total loss: \u001b[1m\u001b[32m0.53257\u001b[0m\u001b[0m | time: 0.506s\n",
            "| SGD | epoch: 893 | loss: 0.53257 - acc: 0.8543 -- iter: 1000/8000\n",
            "Training Step: 9951  | total loss: \u001b[1m\u001b[32m0.53032\u001b[0m\u001b[0m | time: 1.019s\n",
            "| SGD | epoch: 893 | loss: 0.53032 - acc: 0.8564 -- iter: 2000/8000\n",
            "Training Step: 9952  | total loss: \u001b[1m\u001b[32m0.53092\u001b[0m\u001b[0m | time: 1.524s\n",
            "| SGD | epoch: 893 | loss: 0.53092 - acc: 0.8556 -- iter: 3000/8000\n",
            "Training Step: 9953  | total loss: \u001b[1m\u001b[32m0.52634\u001b[0m\u001b[0m | time: 2.039s\n",
            "| SGD | epoch: 893 | loss: 0.52634 - acc: 0.8584 -- iter: 4000/8000\n",
            "Training Step: 9954  | total loss: \u001b[1m\u001b[32m0.53055\u001b[0m\u001b[0m | time: 2.556s\n",
            "| SGD | epoch: 893 | loss: 0.53055 - acc: 0.8564 -- iter: 5000/8000\n",
            "Training Step: 9955  | total loss: \u001b[1m\u001b[32m0.53016\u001b[0m\u001b[0m | time: 3.071s\n",
            "| SGD | epoch: 893 | loss: 0.53016 - acc: 0.8550 -- iter: 6000/8000\n",
            "Training Step: 9956  | total loss: \u001b[1m\u001b[32m0.53133\u001b[0m\u001b[0m | time: 3.587s\n",
            "| SGD | epoch: 893 | loss: 0.53133 - acc: 0.8548 -- iter: 7000/8000\n",
            "Training Step: 9957  | total loss: \u001b[1m\u001b[32m0.53227\u001b[0m\u001b[0m | time: 4.108s\n",
            "| SGD | epoch: 893 | loss: 0.53227 - acc: 0.8554 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9958  | total loss: \u001b[1m\u001b[32m0.53423\u001b[0m\u001b[0m | time: 0.504s\n",
            "| SGD | epoch: 894 | loss: 0.53423 - acc: 0.8545 -- iter: 1000/8000\n",
            "Training Step: 9959  | total loss: \u001b[1m\u001b[32m0.53381\u001b[0m\u001b[0m | time: 1.009s\n",
            "| SGD | epoch: 894 | loss: 0.53381 - acc: 0.8553 -- iter: 2000/8000\n",
            "Training Step: 9960  | total loss: \u001b[1m\u001b[32m0.53328\u001b[0m\u001b[0m | time: 1.522s\n",
            "| SGD | epoch: 894 | loss: 0.53328 - acc: 0.8544 -- iter: 3000/8000\n",
            "Training Step: 9961  | total loss: \u001b[1m\u001b[32m0.53165\u001b[0m\u001b[0m | time: 2.035s\n",
            "| SGD | epoch: 894 | loss: 0.53165 - acc: 0.8550 -- iter: 4000/8000\n",
            "Training Step: 9962  | total loss: \u001b[1m\u001b[32m0.53535\u001b[0m\u001b[0m | time: 2.544s\n",
            "| SGD | epoch: 894 | loss: 0.53535 - acc: 0.8520 -- iter: 5000/8000\n",
            "Training Step: 9963  | total loss: \u001b[1m\u001b[32m0.53256\u001b[0m\u001b[0m | time: 3.061s\n",
            "| SGD | epoch: 894 | loss: 0.53256 - acc: 0.8536 -- iter: 6000/8000\n",
            "Training Step: 9964  | total loss: \u001b[1m\u001b[32m0.53246\u001b[0m\u001b[0m | time: 3.590s\n",
            "| SGD | epoch: 894 | loss: 0.53246 - acc: 0.8538 -- iter: 7000/8000\n",
            "Training Step: 9965  | total loss: \u001b[1m\u001b[32m0.53179\u001b[0m\u001b[0m | time: 4.111s\n",
            "| SGD | epoch: 894 | loss: 0.53179 - acc: 0.8533 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9966  | total loss: \u001b[1m\u001b[32m0.52940\u001b[0m\u001b[0m | time: 0.526s\n",
            "| SGD | epoch: 895 | loss: 0.52940 - acc: 0.8541 -- iter: 1000/8000\n",
            "Training Step: 9967  | total loss: \u001b[1m\u001b[32m0.53019\u001b[0m\u001b[0m | time: 1.051s\n",
            "| SGD | epoch: 895 | loss: 0.53019 - acc: 0.8552 -- iter: 2000/8000\n",
            "Training Step: 9968  | total loss: \u001b[1m\u001b[32m0.53314\u001b[0m\u001b[0m | time: 1.573s\n",
            "| SGD | epoch: 895 | loss: 0.53314 - acc: 0.8537 -- iter: 3000/8000\n",
            "Training Step: 9969  | total loss: \u001b[1m\u001b[32m0.53417\u001b[0m\u001b[0m | time: 2.094s\n",
            "| SGD | epoch: 895 | loss: 0.53417 - acc: 0.8540 -- iter: 4000/8000\n",
            "Training Step: 9970  | total loss: \u001b[1m\u001b[32m0.53517\u001b[0m\u001b[0m | time: 2.620s\n",
            "| SGD | epoch: 895 | loss: 0.53517 - acc: 0.8528 -- iter: 5000/8000\n",
            "Training Step: 9971  | total loss: \u001b[1m\u001b[32m0.53325\u001b[0m\u001b[0m | time: 3.146s\n",
            "| SGD | epoch: 895 | loss: 0.53325 - acc: 0.8523 -- iter: 6000/8000\n",
            "Training Step: 9972  | total loss: \u001b[1m\u001b[32m0.53462\u001b[0m\u001b[0m | time: 3.657s\n",
            "| SGD | epoch: 895 | loss: 0.53462 - acc: 0.8523 -- iter: 7000/8000\n",
            "Training Step: 9973  | total loss: \u001b[1m\u001b[32m0.53193\u001b[0m\u001b[0m | time: 4.166s\n",
            "| SGD | epoch: 895 | loss: 0.53193 - acc: 0.8536 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9974  | total loss: \u001b[1m\u001b[32m0.53440\u001b[0m\u001b[0m | time: 0.505s\n",
            "| SGD | epoch: 896 | loss: 0.53440 - acc: 0.8526 -- iter: 1000/8000\n",
            "Training Step: 9975  | total loss: \u001b[1m\u001b[32m0.53510\u001b[0m\u001b[0m | time: 1.015s\n",
            "| SGD | epoch: 896 | loss: 0.53510 - acc: 0.8535 -- iter: 2000/8000\n",
            "Training Step: 9976  | total loss: \u001b[1m\u001b[32m0.53206\u001b[0m\u001b[0m | time: 1.528s\n",
            "| SGD | epoch: 896 | loss: 0.53206 - acc: 0.8561 -- iter: 3000/8000\n",
            "Training Step: 9977  | total loss: \u001b[1m\u001b[32m0.53292\u001b[0m\u001b[0m | time: 2.037s\n",
            "| SGD | epoch: 896 | loss: 0.53292 - acc: 0.8540 -- iter: 4000/8000\n",
            "Training Step: 9978  | total loss: \u001b[1m\u001b[32m0.53439\u001b[0m\u001b[0m | time: 2.550s\n",
            "| SGD | epoch: 896 | loss: 0.53439 - acc: 0.8536 -- iter: 5000/8000\n",
            "Training Step: 9979  | total loss: \u001b[1m\u001b[32m0.53112\u001b[0m\u001b[0m | time: 3.060s\n",
            "| SGD | epoch: 896 | loss: 0.53112 - acc: 0.8545 -- iter: 6000/8000\n",
            "Training Step: 9980  | total loss: \u001b[1m\u001b[32m0.53297\u001b[0m\u001b[0m | time: 3.571s\n",
            "| SGD | epoch: 896 | loss: 0.53297 - acc: 0.8534 -- iter: 7000/8000\n",
            "Training Step: 9981  | total loss: \u001b[1m\u001b[32m0.53175\u001b[0m\u001b[0m | time: 4.081s\n",
            "| SGD | epoch: 896 | loss: 0.53175 - acc: 0.8547 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9982  | total loss: \u001b[1m\u001b[32m0.53269\u001b[0m\u001b[0m | time: 0.514s\n",
            "| SGD | epoch: 897 | loss: 0.53269 - acc: 0.8539 -- iter: 1000/8000\n",
            "Training Step: 9983  | total loss: \u001b[1m\u001b[32m0.53228\u001b[0m\u001b[0m | time: 1.017s\n",
            "| SGD | epoch: 897 | loss: 0.53228 - acc: 0.8548 -- iter: 2000/8000\n",
            "Training Step: 9984  | total loss: \u001b[1m\u001b[32m0.53150\u001b[0m\u001b[0m | time: 1.528s\n",
            "| SGD | epoch: 897 | loss: 0.53150 - acc: 0.8549 -- iter: 3000/8000\n",
            "Training Step: 9985  | total loss: \u001b[1m\u001b[32m0.53193\u001b[0m\u001b[0m | time: 2.039s\n",
            "| SGD | epoch: 897 | loss: 0.53193 - acc: 0.8549 -- iter: 4000/8000\n",
            "Training Step: 9986  | total loss: \u001b[1m\u001b[32m0.53130\u001b[0m\u001b[0m | time: 2.550s\n",
            "| SGD | epoch: 897 | loss: 0.53130 - acc: 0.8558 -- iter: 5000/8000\n",
            "Training Step: 9987  | total loss: \u001b[1m\u001b[32m0.53230\u001b[0m\u001b[0m | time: 3.054s\n",
            "| SGD | epoch: 897 | loss: 0.53230 - acc: 0.8563 -- iter: 6000/8000\n",
            "Training Step: 9988  | total loss: \u001b[1m\u001b[32m0.53185\u001b[0m\u001b[0m | time: 3.568s\n",
            "| SGD | epoch: 897 | loss: 0.53185 - acc: 0.8571 -- iter: 7000/8000\n",
            "Training Step: 9989  | total loss: \u001b[1m\u001b[32m0.52984\u001b[0m\u001b[0m | time: 4.077s\n",
            "| SGD | epoch: 897 | loss: 0.52984 - acc: 0.8572 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9990  | total loss: \u001b[1m\u001b[32m0.52895\u001b[0m\u001b[0m | time: 0.520s\n",
            "| SGD | epoch: 898 | loss: 0.52895 - acc: 0.8579 -- iter: 1000/8000\n",
            "Training Step: 9991  | total loss: \u001b[1m\u001b[32m0.52942\u001b[0m\u001b[0m | time: 1.042s\n",
            "| SGD | epoch: 898 | loss: 0.52942 - acc: 0.8566 -- iter: 2000/8000\n",
            "Training Step: 9992  | total loss: \u001b[1m\u001b[32m0.53196\u001b[0m\u001b[0m | time: 1.564s\n",
            "| SGD | epoch: 898 | loss: 0.53196 - acc: 0.8556 -- iter: 3000/8000\n",
            "Training Step: 9993  | total loss: \u001b[1m\u001b[32m0.53375\u001b[0m\u001b[0m | time: 2.085s\n",
            "| SGD | epoch: 898 | loss: 0.53375 - acc: 0.8564 -- iter: 4000/8000\n",
            "Training Step: 9994  | total loss: \u001b[1m\u001b[32m0.53259\u001b[0m\u001b[0m | time: 2.602s\n",
            "| SGD | epoch: 898 | loss: 0.53259 - acc: 0.8569 -- iter: 5000/8000\n",
            "Training Step: 9995  | total loss: \u001b[1m\u001b[32m0.53147\u001b[0m\u001b[0m | time: 3.121s\n",
            "| SGD | epoch: 898 | loss: 0.53147 - acc: 0.8571 -- iter: 6000/8000\n",
            "Training Step: 9996  | total loss: \u001b[1m\u001b[32m0.53035\u001b[0m\u001b[0m | time: 3.641s\n",
            "| SGD | epoch: 898 | loss: 0.53035 - acc: 0.8566 -- iter: 7000/8000\n",
            "Training Step: 9997  | total loss: \u001b[1m\u001b[32m0.53013\u001b[0m\u001b[0m | time: 4.163s\n",
            "| SGD | epoch: 898 | loss: 0.53013 - acc: 0.8567 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 9998  | total loss: \u001b[1m\u001b[32m0.53074\u001b[0m\u001b[0m | time: 0.515s\n",
            "| SGD | epoch: 899 | loss: 0.53074 - acc: 0.8560 -- iter: 1000/8000\n",
            "Training Step: 9999  | total loss: \u001b[1m\u001b[32m0.52836\u001b[0m\u001b[0m | time: 1.026s\n",
            "| SGD | epoch: 899 | loss: 0.52836 - acc: 0.8563 -- iter: 2000/8000\n",
            "Training Step: 10000  | total loss: \u001b[1m\u001b[32m0.52805\u001b[0m\u001b[0m | time: 1.536s\n",
            "| SGD | epoch: 899 | loss: 0.52805 - acc: 0.8578 -- iter: 3000/8000\n",
            "Training Step: 10001  | total loss: \u001b[1m\u001b[32m0.52929\u001b[0m\u001b[0m | time: 2.054s\n",
            "| SGD | epoch: 899 | loss: 0.52929 - acc: 0.8581 -- iter: 4000/8000\n",
            "Training Step: 10002  | total loss: \u001b[1m\u001b[32m0.52949\u001b[0m\u001b[0m | time: 2.565s\n",
            "| SGD | epoch: 899 | loss: 0.52949 - acc: 0.8593 -- iter: 5000/8000\n",
            "Training Step: 10003  | total loss: \u001b[1m\u001b[32m0.53459\u001b[0m\u001b[0m | time: 3.070s\n",
            "| SGD | epoch: 899 | loss: 0.53459 - acc: 0.8574 -- iter: 6000/8000\n",
            "Training Step: 10004  | total loss: \u001b[1m\u001b[32m0.53157\u001b[0m\u001b[0m | time: 3.577s\n",
            "| SGD | epoch: 899 | loss: 0.53157 - acc: 0.8584 -- iter: 7000/8000\n",
            "Training Step: 10005  | total loss: \u001b[1m\u001b[32m0.53213\u001b[0m\u001b[0m | time: 4.089s\n",
            "| SGD | epoch: 899 | loss: 0.53213 - acc: 0.8587 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10006  | total loss: \u001b[1m\u001b[32m0.53281\u001b[0m\u001b[0m | time: 0.513s\n",
            "| SGD | epoch: 900 | loss: 0.53281 - acc: 0.8602 -- iter: 1000/8000\n",
            "Training Step: 10007  | total loss: \u001b[1m\u001b[32m0.52936\u001b[0m\u001b[0m | time: 1.024s\n",
            "| SGD | epoch: 900 | loss: 0.52936 - acc: 0.8610 -- iter: 2000/8000\n",
            "Training Step: 10008  | total loss: \u001b[1m\u001b[32m0.52674\u001b[0m\u001b[0m | time: 1.532s\n",
            "| SGD | epoch: 900 | loss: 0.52674 - acc: 0.8620 -- iter: 3000/8000\n",
            "Training Step: 10009  | total loss: \u001b[1m\u001b[32m0.52655\u001b[0m\u001b[0m | time: 2.037s\n",
            "| SGD | epoch: 900 | loss: 0.52655 - acc: 0.8622 -- iter: 4000/8000\n",
            "Training Step: 10010  | total loss: \u001b[1m\u001b[32m0.52815\u001b[0m\u001b[0m | time: 2.545s\n",
            "| SGD | epoch: 900 | loss: 0.52815 - acc: 0.8601 -- iter: 5000/8000\n",
            "Training Step: 10011  | total loss: \u001b[1m\u001b[32m0.52918\u001b[0m\u001b[0m | time: 3.049s\n",
            "| SGD | epoch: 900 | loss: 0.52918 - acc: 0.8597 -- iter: 6000/8000\n",
            "Training Step: 10012  | total loss: \u001b[1m\u001b[32m0.53103\u001b[0m\u001b[0m | time: 3.562s\n",
            "| SGD | epoch: 900 | loss: 0.53103 - acc: 0.8572 -- iter: 7000/8000\n",
            "Training Step: 10013  | total loss: \u001b[1m\u001b[32m0.53136\u001b[0m\u001b[0m | time: 4.072s\n",
            "| SGD | epoch: 900 | loss: 0.53136 - acc: 0.8559 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10014  | total loss: \u001b[1m\u001b[32m0.53395\u001b[0m\u001b[0m | time: 0.507s\n",
            "| SGD | epoch: 901 | loss: 0.53395 - acc: 0.8566 -- iter: 1000/8000\n",
            "Training Step: 10015  | total loss: \u001b[1m\u001b[32m0.53006\u001b[0m\u001b[0m | time: 1.017s\n",
            "| SGD | epoch: 901 | loss: 0.53006 - acc: 0.8580 -- iter: 2000/8000\n",
            "Training Step: 10016  | total loss: \u001b[1m\u001b[32m0.53069\u001b[0m\u001b[0m | time: 1.529s\n",
            "| SGD | epoch: 901 | loss: 0.53069 - acc: 0.8561 -- iter: 3000/8000\n",
            "Training Step: 10017  | total loss: \u001b[1m\u001b[32m0.53403\u001b[0m\u001b[0m | time: 2.049s\n",
            "| SGD | epoch: 901 | loss: 0.53403 - acc: 0.8568 -- iter: 4000/8000\n",
            "Training Step: 10018  | total loss: \u001b[1m\u001b[32m0.52964\u001b[0m\u001b[0m | time: 2.570s\n",
            "| SGD | epoch: 901 | loss: 0.52964 - acc: 0.8573 -- iter: 5000/8000\n",
            "Training Step: 10019  | total loss: \u001b[1m\u001b[32m0.52957\u001b[0m\u001b[0m | time: 3.101s\n",
            "| SGD | epoch: 901 | loss: 0.52957 - acc: 0.8580 -- iter: 6000/8000\n",
            "Training Step: 10020  | total loss: \u001b[1m\u001b[32m0.52950\u001b[0m\u001b[0m | time: 3.625s\n",
            "| SGD | epoch: 901 | loss: 0.52950 - acc: 0.8579 -- iter: 7000/8000\n",
            "Training Step: 10021  | total loss: \u001b[1m\u001b[32m0.52539\u001b[0m\u001b[0m | time: 4.142s\n",
            "| SGD | epoch: 901 | loss: 0.52539 - acc: 0.8594 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10022  | total loss: \u001b[1m\u001b[32m0.52887\u001b[0m\u001b[0m | time: 0.528s\n",
            "| SGD | epoch: 902 | loss: 0.52887 - acc: 0.8568 -- iter: 1000/8000\n",
            "Training Step: 10023  | total loss: \u001b[1m\u001b[32m0.53084\u001b[0m\u001b[0m | time: 1.043s\n",
            "| SGD | epoch: 902 | loss: 0.53084 - acc: 0.8556 -- iter: 2000/8000\n",
            "Training Step: 10024  | total loss: \u001b[1m\u001b[32m0.53243\u001b[0m\u001b[0m | time: 1.564s\n",
            "| SGD | epoch: 902 | loss: 0.53243 - acc: 0.8556 -- iter: 3000/8000\n",
            "Training Step: 10025  | total loss: \u001b[1m\u001b[32m0.52802\u001b[0m\u001b[0m | time: 2.082s\n",
            "| SGD | epoch: 902 | loss: 0.52802 - acc: 0.8569 -- iter: 4000/8000\n",
            "Training Step: 10026  | total loss: \u001b[1m\u001b[32m0.53104\u001b[0m\u001b[0m | time: 2.590s\n",
            "| SGD | epoch: 902 | loss: 0.53104 - acc: 0.8552 -- iter: 5000/8000\n",
            "Training Step: 10027  | total loss: \u001b[1m\u001b[32m0.52898\u001b[0m\u001b[0m | time: 3.091s\n",
            "| SGD | epoch: 902 | loss: 0.52898 - acc: 0.8569 -- iter: 6000/8000\n",
            "Training Step: 10028  | total loss: \u001b[1m\u001b[32m0.52740\u001b[0m\u001b[0m | time: 3.601s\n",
            "| SGD | epoch: 902 | loss: 0.52740 - acc: 0.8575 -- iter: 7000/8000\n",
            "Training Step: 10029  | total loss: \u001b[1m\u001b[32m0.52558\u001b[0m\u001b[0m | time: 4.101s\n",
            "| SGD | epoch: 902 | loss: 0.52558 - acc: 0.8579 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10030  | total loss: \u001b[1m\u001b[32m0.52869\u001b[0m\u001b[0m | time: 0.512s\n",
            "| SGD | epoch: 903 | loss: 0.52869 - acc: 0.8561 -- iter: 1000/8000\n",
            "Training Step: 10031  | total loss: \u001b[1m\u001b[32m0.52730\u001b[0m\u001b[0m | time: 1.022s\n",
            "| SGD | epoch: 903 | loss: 0.52730 - acc: 0.8575 -- iter: 2000/8000\n",
            "Training Step: 10032  | total loss: \u001b[1m\u001b[32m0.52864\u001b[0m\u001b[0m | time: 1.536s\n",
            "| SGD | epoch: 903 | loss: 0.52864 - acc: 0.8558 -- iter: 3000/8000\n",
            "Training Step: 10033  | total loss: \u001b[1m\u001b[32m0.52702\u001b[0m\u001b[0m | time: 2.039s\n",
            "| SGD | epoch: 903 | loss: 0.52702 - acc: 0.8565 -- iter: 4000/8000\n",
            "Training Step: 10034  | total loss: \u001b[1m\u001b[32m0.53053\u001b[0m\u001b[0m | time: 2.553s\n",
            "| SGD | epoch: 903 | loss: 0.53053 - acc: 0.8548 -- iter: 5000/8000\n",
            "Training Step: 10035  | total loss: \u001b[1m\u001b[32m0.53250\u001b[0m\u001b[0m | time: 3.062s\n",
            "| SGD | epoch: 903 | loss: 0.53250 - acc: 0.8541 -- iter: 6000/8000\n",
            "Training Step: 10036  | total loss: \u001b[1m\u001b[32m0.53149\u001b[0m\u001b[0m | time: 3.567s\n",
            "| SGD | epoch: 903 | loss: 0.53149 - acc: 0.8543 -- iter: 7000/8000\n",
            "Training Step: 10037  | total loss: \u001b[1m\u001b[32m0.53065\u001b[0m\u001b[0m | time: 4.076s\n",
            "| SGD | epoch: 903 | loss: 0.53065 - acc: 0.8544 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10038  | total loss: \u001b[1m\u001b[32m0.53018\u001b[0m\u001b[0m | time: 0.508s\n",
            "| SGD | epoch: 904 | loss: 0.53018 - acc: 0.8539 -- iter: 1000/8000\n",
            "Training Step: 10039  | total loss: \u001b[1m\u001b[32m0.52765\u001b[0m\u001b[0m | time: 1.019s\n",
            "| SGD | epoch: 904 | loss: 0.52765 - acc: 0.8551 -- iter: 2000/8000\n",
            "Training Step: 10040  | total loss: \u001b[1m\u001b[32m0.52469\u001b[0m\u001b[0m | time: 1.533s\n",
            "| SGD | epoch: 904 | loss: 0.52469 - acc: 0.8584 -- iter: 3000/8000\n",
            "Training Step: 10041  | total loss: \u001b[1m\u001b[32m0.52611\u001b[0m\u001b[0m | time: 2.042s\n",
            "| SGD | epoch: 904 | loss: 0.52611 - acc: 0.8583 -- iter: 4000/8000\n",
            "Training Step: 10042  | total loss: \u001b[1m\u001b[32m0.52746\u001b[0m\u001b[0m | time: 2.555s\n",
            "| SGD | epoch: 904 | loss: 0.52746 - acc: 0.8579 -- iter: 5000/8000\n",
            "Training Step: 10043  | total loss: \u001b[1m\u001b[32m0.52877\u001b[0m\u001b[0m | time: 3.056s\n",
            "| SGD | epoch: 904 | loss: 0.52877 - acc: 0.8570 -- iter: 6000/8000\n",
            "Training Step: 10044  | total loss: \u001b[1m\u001b[32m0.52805\u001b[0m\u001b[0m | time: 3.578s\n",
            "| SGD | epoch: 904 | loss: 0.52805 - acc: 0.8574 -- iter: 7000/8000\n",
            "Training Step: 10045  | total loss: \u001b[1m\u001b[32m0.52758\u001b[0m\u001b[0m | time: 4.093s\n",
            "| SGD | epoch: 904 | loss: 0.52758 - acc: 0.8579 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10046  | total loss: \u001b[1m\u001b[32m0.52818\u001b[0m\u001b[0m | time: 0.522s\n",
            "| SGD | epoch: 905 | loss: 0.52818 - acc: 0.8567 -- iter: 1000/8000\n",
            "Training Step: 10047  | total loss: \u001b[1m\u001b[32m0.52818\u001b[0m\u001b[0m | time: 1.049s\n",
            "| SGD | epoch: 905 | loss: 0.52818 - acc: 0.8567 -- iter: 2000/8000\n",
            "Training Step: 10048  | total loss: \u001b[1m\u001b[32m0.52682\u001b[0m\u001b[0m | time: 1.573s\n",
            "| SGD | epoch: 905 | loss: 0.52682 - acc: 0.8570 -- iter: 3000/8000\n",
            "Training Step: 10049  | total loss: \u001b[1m\u001b[32m0.52483\u001b[0m\u001b[0m | time: 2.099s\n",
            "| SGD | epoch: 905 | loss: 0.52483 - acc: 0.8573 -- iter: 4000/8000\n",
            "Training Step: 10050  | total loss: \u001b[1m\u001b[32m0.52419\u001b[0m\u001b[0m | time: 2.621s\n",
            "| SGD | epoch: 905 | loss: 0.52419 - acc: 0.8581 -- iter: 5000/8000\n",
            "Training Step: 10051  | total loss: \u001b[1m\u001b[32m0.52211\u001b[0m\u001b[0m | time: 3.153s\n",
            "| SGD | epoch: 905 | loss: 0.52211 - acc: 0.8591 -- iter: 6000/8000\n",
            "Training Step: 10052  | total loss: \u001b[1m\u001b[32m0.52193\u001b[0m\u001b[0m | time: 3.672s\n",
            "| SGD | epoch: 905 | loss: 0.52193 - acc: 0.8582 -- iter: 7000/8000\n",
            "Training Step: 10053  | total loss: \u001b[1m\u001b[32m0.52440\u001b[0m\u001b[0m | time: 4.183s\n",
            "| SGD | epoch: 905 | loss: 0.52440 - acc: 0.8583 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10054  | total loss: \u001b[1m\u001b[32m0.52121\u001b[0m\u001b[0m | time: 0.514s\n",
            "| SGD | epoch: 906 | loss: 0.52121 - acc: 0.8595 -- iter: 1000/8000\n",
            "Training Step: 10055  | total loss: \u001b[1m\u001b[32m0.52581\u001b[0m\u001b[0m | time: 1.036s\n",
            "| SGD | epoch: 906 | loss: 0.52581 - acc: 0.8577 -- iter: 2000/8000\n",
            "Training Step: 10056  | total loss: \u001b[1m\u001b[32m0.52864\u001b[0m\u001b[0m | time: 1.545s\n",
            "| SGD | epoch: 906 | loss: 0.52864 - acc: 0.8567 -- iter: 3000/8000\n",
            "Training Step: 10057  | total loss: \u001b[1m\u001b[32m0.53326\u001b[0m\u001b[0m | time: 2.070s\n",
            "| SGD | epoch: 906 | loss: 0.53326 - acc: 0.8556 -- iter: 4000/8000\n",
            "Training Step: 10058  | total loss: \u001b[1m\u001b[32m0.52992\u001b[0m\u001b[0m | time: 2.580s\n",
            "| SGD | epoch: 906 | loss: 0.52992 - acc: 0.8566 -- iter: 5000/8000\n",
            "Training Step: 10059  | total loss: \u001b[1m\u001b[32m0.53339\u001b[0m\u001b[0m | time: 3.112s\n",
            "| SGD | epoch: 906 | loss: 0.53339 - acc: 0.8552 -- iter: 6000/8000\n",
            "Training Step: 10060  | total loss: \u001b[1m\u001b[32m0.53289\u001b[0m\u001b[0m | time: 3.624s\n",
            "| SGD | epoch: 906 | loss: 0.53289 - acc: 0.8571 -- iter: 7000/8000\n",
            "Training Step: 10061  | total loss: \u001b[1m\u001b[32m0.53069\u001b[0m\u001b[0m | time: 4.141s\n",
            "| SGD | epoch: 906 | loss: 0.53069 - acc: 0.8573 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10062  | total loss: \u001b[1m\u001b[32m0.52875\u001b[0m\u001b[0m | time: 0.509s\n",
            "| SGD | epoch: 907 | loss: 0.52875 - acc: 0.8582 -- iter: 1000/8000\n",
            "Training Step: 10063  | total loss: \u001b[1m\u001b[32m0.52773\u001b[0m\u001b[0m | time: 1.040s\n",
            "| SGD | epoch: 907 | loss: 0.52773 - acc: 0.8583 -- iter: 2000/8000\n",
            "Training Step: 10064  | total loss: \u001b[1m\u001b[32m0.52672\u001b[0m\u001b[0m | time: 1.552s\n",
            "| SGD | epoch: 907 | loss: 0.52672 - acc: 0.8574 -- iter: 3000/8000\n",
            "Training Step: 10065  | total loss: \u001b[1m\u001b[32m0.52495\u001b[0m\u001b[0m | time: 2.074s\n",
            "| SGD | epoch: 907 | loss: 0.52495 - acc: 0.8578 -- iter: 4000/8000\n",
            "Training Step: 10066  | total loss: \u001b[1m\u001b[32m0.52555\u001b[0m\u001b[0m | time: 2.598s\n",
            "| SGD | epoch: 907 | loss: 0.52555 - acc: 0.8577 -- iter: 5000/8000\n",
            "Training Step: 10067  | total loss: \u001b[1m\u001b[32m0.52487\u001b[0m\u001b[0m | time: 3.112s\n",
            "| SGD | epoch: 907 | loss: 0.52487 - acc: 0.8581 -- iter: 6000/8000\n",
            "Training Step: 10068  | total loss: \u001b[1m\u001b[32m0.52619\u001b[0m\u001b[0m | time: 3.630s\n",
            "| SGD | epoch: 907 | loss: 0.52619 - acc: 0.8573 -- iter: 7000/8000\n",
            "Training Step: 10069  | total loss: \u001b[1m\u001b[32m0.52570\u001b[0m\u001b[0m | time: 4.148s\n",
            "| SGD | epoch: 907 | loss: 0.52570 - acc: 0.8583 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10070  | total loss: \u001b[1m\u001b[32m0.52630\u001b[0m\u001b[0m | time: 0.524s\n",
            "| SGD | epoch: 908 | loss: 0.52630 - acc: 0.8580 -- iter: 1000/8000\n",
            "Training Step: 10071  | total loss: \u001b[1m\u001b[32m0.52775\u001b[0m\u001b[0m | time: 1.054s\n",
            "| SGD | epoch: 908 | loss: 0.52775 - acc: 0.8575 -- iter: 2000/8000\n",
            "Training Step: 10072  | total loss: \u001b[1m\u001b[32m0.52584\u001b[0m\u001b[0m | time: 1.575s\n",
            "| SGD | epoch: 908 | loss: 0.52584 - acc: 0.8589 -- iter: 3000/8000\n",
            "Training Step: 10073  | total loss: \u001b[1m\u001b[32m0.52428\u001b[0m\u001b[0m | time: 2.106s\n",
            "| SGD | epoch: 908 | loss: 0.52428 - acc: 0.8592 -- iter: 4000/8000\n",
            "Training Step: 10074  | total loss: \u001b[1m\u001b[32m0.52237\u001b[0m\u001b[0m | time: 2.624s\n",
            "| SGD | epoch: 908 | loss: 0.52237 - acc: 0.8596 -- iter: 5000/8000\n",
            "Training Step: 10075  | total loss: \u001b[1m\u001b[32m0.52427\u001b[0m\u001b[0m | time: 3.159s\n",
            "| SGD | epoch: 908 | loss: 0.52427 - acc: 0.8592 -- iter: 6000/8000\n",
            "Training Step: 10076  | total loss: \u001b[1m\u001b[32m0.52339\u001b[0m\u001b[0m | time: 3.676s\n",
            "| SGD | epoch: 908 | loss: 0.52339 - acc: 0.8592 -- iter: 7000/8000\n",
            "Training Step: 10077  | total loss: \u001b[1m\u001b[32m0.52427\u001b[0m\u001b[0m | time: 4.205s\n",
            "| SGD | epoch: 908 | loss: 0.52427 - acc: 0.8584 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10078  | total loss: \u001b[1m\u001b[32m0.52436\u001b[0m\u001b[0m | time: 0.532s\n",
            "| SGD | epoch: 909 | loss: 0.52436 - acc: 0.8578 -- iter: 1000/8000\n",
            "Training Step: 10079  | total loss: \u001b[1m\u001b[32m0.52418\u001b[0m\u001b[0m | time: 1.046s\n",
            "| SGD | epoch: 909 | loss: 0.52418 - acc: 0.8588 -- iter: 2000/8000\n",
            "Training Step: 10080  | total loss: \u001b[1m\u001b[32m0.52347\u001b[0m\u001b[0m | time: 1.569s\n",
            "| SGD | epoch: 909 | loss: 0.52347 - acc: 0.8598 -- iter: 3000/8000\n",
            "Training Step: 10081  | total loss: \u001b[1m\u001b[32m0.52623\u001b[0m\u001b[0m | time: 2.085s\n",
            "| SGD | epoch: 909 | loss: 0.52623 - acc: 0.8584 -- iter: 4000/8000\n",
            "Training Step: 10082  | total loss: \u001b[1m\u001b[32m0.52397\u001b[0m\u001b[0m | time: 2.615s\n",
            "| SGD | epoch: 909 | loss: 0.52397 - acc: 0.8604 -- iter: 5000/8000\n",
            "Training Step: 10083  | total loss: \u001b[1m\u001b[32m0.52140\u001b[0m\u001b[0m | time: 3.118s\n",
            "| SGD | epoch: 909 | loss: 0.52140 - acc: 0.8627 -- iter: 6000/8000\n",
            "Training Step: 10084  | total loss: \u001b[1m\u001b[32m0.52239\u001b[0m\u001b[0m | time: 3.650s\n",
            "| SGD | epoch: 909 | loss: 0.52239 - acc: 0.8619 -- iter: 7000/8000\n",
            "Training Step: 10085  | total loss: \u001b[1m\u001b[32m0.52113\u001b[0m\u001b[0m | time: 4.168s\n",
            "| SGD | epoch: 909 | loss: 0.52113 - acc: 0.8620 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10086  | total loss: \u001b[1m\u001b[32m0.52346\u001b[0m\u001b[0m | time: 0.514s\n",
            "| SGD | epoch: 910 | loss: 0.52346 - acc: 0.8600 -- iter: 1000/8000\n",
            "Training Step: 10087  | total loss: \u001b[1m\u001b[32m0.52266\u001b[0m\u001b[0m | time: 1.025s\n",
            "| SGD | epoch: 910 | loss: 0.52266 - acc: 0.8596 -- iter: 2000/8000\n",
            "Training Step: 10088  | total loss: \u001b[1m\u001b[32m0.52592\u001b[0m\u001b[0m | time: 1.551s\n",
            "| SGD | epoch: 910 | loss: 0.52592 - acc: 0.8581 -- iter: 3000/8000\n",
            "Training Step: 10089  | total loss: \u001b[1m\u001b[32m0.52778\u001b[0m\u001b[0m | time: 2.072s\n",
            "| SGD | epoch: 910 | loss: 0.52778 - acc: 0.8577 -- iter: 4000/8000\n",
            "Training Step: 10090  | total loss: \u001b[1m\u001b[32m0.52546\u001b[0m\u001b[0m | time: 2.586s\n",
            "| SGD | epoch: 910 | loss: 0.52546 - acc: 0.8572 -- iter: 5000/8000\n",
            "Training Step: 10091  | total loss: \u001b[1m\u001b[32m0.52378\u001b[0m\u001b[0m | time: 3.109s\n",
            "| SGD | epoch: 910 | loss: 0.52378 - acc: 0.8590 -- iter: 6000/8000\n",
            "Training Step: 10092  | total loss: \u001b[1m\u001b[32m0.52257\u001b[0m\u001b[0m | time: 3.624s\n",
            "| SGD | epoch: 910 | loss: 0.52257 - acc: 0.8600 -- iter: 7000/8000\n",
            "Training Step: 10093  | total loss: \u001b[1m\u001b[32m0.52274\u001b[0m\u001b[0m | time: 4.148s\n",
            "| SGD | epoch: 910 | loss: 0.52274 - acc: 0.8618 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10094  | total loss: \u001b[1m\u001b[32m0.51988\u001b[0m\u001b[0m | time: 0.513s\n",
            "| SGD | epoch: 911 | loss: 0.51988 - acc: 0.8631 -- iter: 1000/8000\n",
            "Training Step: 10095  | total loss: \u001b[1m\u001b[32m0.51896\u001b[0m\u001b[0m | time: 1.038s\n",
            "| SGD | epoch: 911 | loss: 0.51896 - acc: 0.8627 -- iter: 2000/8000\n",
            "Training Step: 10096  | total loss: \u001b[1m\u001b[32m0.51719\u001b[0m\u001b[0m | time: 1.546s\n",
            "| SGD | epoch: 911 | loss: 0.51719 - acc: 0.8627 -- iter: 3000/8000\n",
            "Training Step: 10097  | total loss: \u001b[1m\u001b[32m0.51864\u001b[0m\u001b[0m | time: 2.063s\n",
            "| SGD | epoch: 911 | loss: 0.51864 - acc: 0.8632 -- iter: 4000/8000\n",
            "Training Step: 10098  | total loss: \u001b[1m\u001b[32m0.51977\u001b[0m\u001b[0m | time: 2.596s\n",
            "| SGD | epoch: 911 | loss: 0.51977 - acc: 0.8633 -- iter: 5000/8000\n",
            "Training Step: 10099  | total loss: \u001b[1m\u001b[32m0.52025\u001b[0m\u001b[0m | time: 3.127s\n",
            "| SGD | epoch: 911 | loss: 0.52025 - acc: 0.8615 -- iter: 6000/8000\n",
            "Training Step: 10100  | total loss: \u001b[1m\u001b[32m0.51967\u001b[0m\u001b[0m | time: 3.649s\n",
            "| SGD | epoch: 911 | loss: 0.51967 - acc: 0.8610 -- iter: 7000/8000\n",
            "Training Step: 10101  | total loss: \u001b[1m\u001b[32m0.51838\u001b[0m\u001b[0m | time: 4.161s\n",
            "| SGD | epoch: 911 | loss: 0.51838 - acc: 0.8606 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10102  | total loss: \u001b[1m\u001b[32m0.51984\u001b[0m\u001b[0m | time: 0.531s\n",
            "| SGD | epoch: 912 | loss: 0.51984 - acc: 0.8595 -- iter: 1000/8000\n",
            "Training Step: 10103  | total loss: \u001b[1m\u001b[32m0.52138\u001b[0m\u001b[0m | time: 1.053s\n",
            "| SGD | epoch: 912 | loss: 0.52138 - acc: 0.8599 -- iter: 2000/8000\n",
            "Training Step: 10104  | total loss: \u001b[1m\u001b[32m0.52344\u001b[0m\u001b[0m | time: 1.588s\n",
            "| SGD | epoch: 912 | loss: 0.52344 - acc: 0.8602 -- iter: 3000/8000\n",
            "Training Step: 10105  | total loss: \u001b[1m\u001b[32m0.52184\u001b[0m\u001b[0m | time: 2.101s\n",
            "| SGD | epoch: 912 | loss: 0.52184 - acc: 0.8602 -- iter: 4000/8000\n",
            "Training Step: 10106  | total loss: \u001b[1m\u001b[32m0.52207\u001b[0m\u001b[0m | time: 2.624s\n",
            "| SGD | epoch: 912 | loss: 0.52207 - acc: 0.8593 -- iter: 5000/8000\n",
            "Training Step: 10107  | total loss: \u001b[1m\u001b[32m0.52080\u001b[0m\u001b[0m | time: 3.142s\n",
            "| SGD | epoch: 912 | loss: 0.52080 - acc: 0.8614 -- iter: 6000/8000\n",
            "Training Step: 10108  | total loss: \u001b[1m\u001b[32m0.51986\u001b[0m\u001b[0m | time: 3.663s\n",
            "| SGD | epoch: 912 | loss: 0.51986 - acc: 0.8620 -- iter: 7000/8000\n",
            "Training Step: 10109  | total loss: \u001b[1m\u001b[32m0.52272\u001b[0m\u001b[0m | time: 4.183s\n",
            "| SGD | epoch: 912 | loss: 0.52272 - acc: 0.8603 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10110  | total loss: \u001b[1m\u001b[32m0.52504\u001b[0m\u001b[0m | time: 0.521s\n",
            "| SGD | epoch: 913 | loss: 0.52504 - acc: 0.8588 -- iter: 1000/8000\n",
            "Training Step: 10111  | total loss: \u001b[1m\u001b[32m0.52310\u001b[0m\u001b[0m | time: 1.041s\n",
            "| SGD | epoch: 913 | loss: 0.52310 - acc: 0.8599 -- iter: 2000/8000\n",
            "Training Step: 10112  | total loss: \u001b[1m\u001b[32m0.52313\u001b[0m\u001b[0m | time: 1.560s\n",
            "| SGD | epoch: 913 | loss: 0.52313 - acc: 0.8593 -- iter: 3000/8000\n",
            "Training Step: 10113  | total loss: \u001b[1m\u001b[32m0.52357\u001b[0m\u001b[0m | time: 2.078s\n",
            "| SGD | epoch: 913 | loss: 0.52357 - acc: 0.8581 -- iter: 4000/8000\n",
            "Training Step: 10114  | total loss: \u001b[1m\u001b[32m0.52541\u001b[0m\u001b[0m | time: 2.594s\n",
            "| SGD | epoch: 913 | loss: 0.52541 - acc: 0.8573 -- iter: 5000/8000\n",
            "Training Step: 10115  | total loss: \u001b[1m\u001b[32m0.52939\u001b[0m\u001b[0m | time: 3.115s\n",
            "| SGD | epoch: 913 | loss: 0.52939 - acc: 0.8567 -- iter: 6000/8000\n",
            "Training Step: 10116  | total loss: \u001b[1m\u001b[32m0.53017\u001b[0m\u001b[0m | time: 3.634s\n",
            "| SGD | epoch: 913 | loss: 0.53017 - acc: 0.8557 -- iter: 7000/8000\n",
            "Training Step: 10117  | total loss: \u001b[1m\u001b[32m0.52853\u001b[0m\u001b[0m | time: 4.146s\n",
            "| SGD | epoch: 913 | loss: 0.52853 - acc: 0.8575 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10118  | total loss: \u001b[1m\u001b[32m0.52394\u001b[0m\u001b[0m | time: 0.519s\n",
            "| SGD | epoch: 914 | loss: 0.52394 - acc: 0.8598 -- iter: 1000/8000\n",
            "Training Step: 10119  | total loss: \u001b[1m\u001b[32m0.51853\u001b[0m\u001b[0m | time: 1.037s\n",
            "| SGD | epoch: 914 | loss: 0.51853 - acc: 0.8624 -- iter: 2000/8000\n",
            "Training Step: 10120  | total loss: \u001b[1m\u001b[32m0.51629\u001b[0m\u001b[0m | time: 1.564s\n",
            "| SGD | epoch: 914 | loss: 0.51629 - acc: 0.8638 -- iter: 3000/8000\n",
            "Training Step: 10121  | total loss: \u001b[1m\u001b[32m0.51630\u001b[0m\u001b[0m | time: 2.079s\n",
            "| SGD | epoch: 914 | loss: 0.51630 - acc: 0.8643 -- iter: 4000/8000\n",
            "Training Step: 10122  | total loss: \u001b[1m\u001b[32m0.51737\u001b[0m\u001b[0m | time: 2.599s\n",
            "| SGD | epoch: 914 | loss: 0.51737 - acc: 0.8632 -- iter: 5000/8000\n",
            "Training Step: 10123  | total loss: \u001b[1m\u001b[32m0.52097\u001b[0m\u001b[0m | time: 3.116s\n",
            "| SGD | epoch: 914 | loss: 0.52097 - acc: 0.8627 -- iter: 6000/8000\n",
            "Training Step: 10124  | total loss: \u001b[1m\u001b[32m0.52436\u001b[0m\u001b[0m | time: 3.637s\n",
            "| SGD | epoch: 914 | loss: 0.52436 - acc: 0.8613 -- iter: 7000/8000\n",
            "Training Step: 10125  | total loss: \u001b[1m\u001b[32m0.52293\u001b[0m\u001b[0m | time: 4.155s\n",
            "| SGD | epoch: 914 | loss: 0.52293 - acc: 0.8603 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10126  | total loss: \u001b[1m\u001b[32m0.51722\u001b[0m\u001b[0m | time: 0.525s\n",
            "| SGD | epoch: 915 | loss: 0.51722 - acc: 0.8640 -- iter: 1000/8000\n",
            "Training Step: 10127  | total loss: \u001b[1m\u001b[32m0.51783\u001b[0m\u001b[0m | time: 1.059s\n",
            "| SGD | epoch: 915 | loss: 0.51783 - acc: 0.8622 -- iter: 2000/8000\n",
            "Training Step: 10128  | total loss: \u001b[1m\u001b[32m0.51824\u001b[0m\u001b[0m | time: 1.583s\n",
            "| SGD | epoch: 915 | loss: 0.51824 - acc: 0.8616 -- iter: 3000/8000\n",
            "Training Step: 10129  | total loss: \u001b[1m\u001b[32m0.51723\u001b[0m\u001b[0m | time: 2.108s\n",
            "| SGD | epoch: 915 | loss: 0.51723 - acc: 0.8617 -- iter: 4000/8000\n",
            "Training Step: 10130  | total loss: \u001b[1m\u001b[32m0.51710\u001b[0m\u001b[0m | time: 2.634s\n",
            "| SGD | epoch: 915 | loss: 0.51710 - acc: 0.8622 -- iter: 5000/8000\n",
            "Training Step: 10131  | total loss: \u001b[1m\u001b[32m0.51613\u001b[0m\u001b[0m | time: 3.148s\n",
            "| SGD | epoch: 915 | loss: 0.51613 - acc: 0.8625 -- iter: 6000/8000\n",
            "Training Step: 10132  | total loss: \u001b[1m\u001b[32m0.52119\u001b[0m\u001b[0m | time: 3.676s\n",
            "| SGD | epoch: 915 | loss: 0.52119 - acc: 0.8602 -- iter: 7000/8000\n",
            "Training Step: 10133  | total loss: \u001b[1m\u001b[32m0.52101\u001b[0m\u001b[0m | time: 4.196s\n",
            "| SGD | epoch: 915 | loss: 0.52101 - acc: 0.8598 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10134  | total loss: \u001b[1m\u001b[32m0.52025\u001b[0m\u001b[0m | time: 0.523s\n",
            "| SGD | epoch: 916 | loss: 0.52025 - acc: 0.8604 -- iter: 1000/8000\n",
            "Training Step: 10135  | total loss: \u001b[1m\u001b[32m0.51704\u001b[0m\u001b[0m | time: 1.036s\n",
            "| SGD | epoch: 916 | loss: 0.51704 - acc: 0.8613 -- iter: 2000/8000\n",
            "Training Step: 10136  | total loss: \u001b[1m\u001b[32m0.51591\u001b[0m\u001b[0m | time: 1.558s\n",
            "| SGD | epoch: 916 | loss: 0.51591 - acc: 0.8607 -- iter: 3000/8000\n",
            "Training Step: 10137  | total loss: \u001b[1m\u001b[32m0.51373\u001b[0m\u001b[0m | time: 2.070s\n",
            "| SGD | epoch: 916 | loss: 0.51373 - acc: 0.8617 -- iter: 4000/8000\n",
            "Training Step: 10138  | total loss: \u001b[1m\u001b[32m0.51900\u001b[0m\u001b[0m | time: 2.588s\n",
            "| SGD | epoch: 916 | loss: 0.51900 - acc: 0.8591 -- iter: 5000/8000\n",
            "Training Step: 10139  | total loss: \u001b[1m\u001b[32m0.51716\u001b[0m\u001b[0m | time: 3.098s\n",
            "| SGD | epoch: 916 | loss: 0.51716 - acc: 0.8617 -- iter: 6000/8000\n",
            "Training Step: 10140  | total loss: \u001b[1m\u001b[32m0.52001\u001b[0m\u001b[0m | time: 3.614s\n",
            "| SGD | epoch: 916 | loss: 0.52001 - acc: 0.8599 -- iter: 7000/8000\n",
            "Training Step: 10141  | total loss: \u001b[1m\u001b[32m0.52009\u001b[0m\u001b[0m | time: 4.126s\n",
            "| SGD | epoch: 916 | loss: 0.52009 - acc: 0.8599 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10142  | total loss: \u001b[1m\u001b[32m0.51833\u001b[0m\u001b[0m | time: 0.518s\n",
            "| SGD | epoch: 917 | loss: 0.51833 - acc: 0.8594 -- iter: 1000/8000\n",
            "Training Step: 10143  | total loss: \u001b[1m\u001b[32m0.52009\u001b[0m\u001b[0m | time: 1.032s\n",
            "| SGD | epoch: 917 | loss: 0.52009 - acc: 0.8606 -- iter: 2000/8000\n",
            "Training Step: 10144  | total loss: \u001b[1m\u001b[32m0.52106\u001b[0m\u001b[0m | time: 1.551s\n",
            "| SGD | epoch: 917 | loss: 0.52106 - acc: 0.8597 -- iter: 3000/8000\n",
            "Training Step: 10145  | total loss: \u001b[1m\u001b[32m0.51858\u001b[0m\u001b[0m | time: 2.069s\n",
            "| SGD | epoch: 917 | loss: 0.51858 - acc: 0.8604 -- iter: 4000/8000\n",
            "Training Step: 10146  | total loss: \u001b[1m\u001b[32m0.51450\u001b[0m\u001b[0m | time: 2.587s\n",
            "| SGD | epoch: 917 | loss: 0.51450 - acc: 0.8613 -- iter: 5000/8000\n",
            "Training Step: 10147  | total loss: \u001b[1m\u001b[32m0.51558\u001b[0m\u001b[0m | time: 3.107s\n",
            "| SGD | epoch: 917 | loss: 0.51558 - acc: 0.8622 -- iter: 6000/8000\n",
            "Training Step: 10148  | total loss: \u001b[1m\u001b[32m0.51458\u001b[0m\u001b[0m | time: 3.622s\n",
            "| SGD | epoch: 917 | loss: 0.51458 - acc: 0.8630 -- iter: 7000/8000\n",
            "Training Step: 10149  | total loss: \u001b[1m\u001b[32m0.51679\u001b[0m\u001b[0m | time: 4.145s\n",
            "| SGD | epoch: 917 | loss: 0.51679 - acc: 0.8639 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10150  | total loss: \u001b[1m\u001b[32m0.51636\u001b[0m\u001b[0m | time: 0.516s\n",
            "| SGD | epoch: 918 | loss: 0.51636 - acc: 0.8638 -- iter: 1000/8000\n",
            "Training Step: 10151  | total loss: \u001b[1m\u001b[32m0.51826\u001b[0m\u001b[0m | time: 1.040s\n",
            "| SGD | epoch: 918 | loss: 0.51826 - acc: 0.8623 -- iter: 2000/8000\n",
            "Training Step: 10152  | total loss: \u001b[1m\u001b[32m0.51847\u001b[0m\u001b[0m | time: 1.690s\n",
            "| SGD | epoch: 918 | loss: 0.51847 - acc: 0.8617 -- iter: 3000/8000\n",
            "Training Step: 10153  | total loss: \u001b[1m\u001b[32m0.51613\u001b[0m\u001b[0m | time: 2.331s\n",
            "| SGD | epoch: 918 | loss: 0.51613 - acc: 0.8634 -- iter: 4000/8000\n",
            "Training Step: 10154  | total loss: \u001b[1m\u001b[32m0.51807\u001b[0m\u001b[0m | time: 2.975s\n",
            "| SGD | epoch: 918 | loss: 0.51807 - acc: 0.8623 -- iter: 5000/8000\n",
            "Training Step: 10155  | total loss: \u001b[1m\u001b[32m0.51916\u001b[0m\u001b[0m | time: 3.620s\n",
            "| SGD | epoch: 918 | loss: 0.51916 - acc: 0.8619 -- iter: 6000/8000\n",
            "Training Step: 10156  | total loss: \u001b[1m\u001b[32m0.51600\u001b[0m\u001b[0m | time: 4.265s\n",
            "| SGD | epoch: 918 | loss: 0.51600 - acc: 0.8624 -- iter: 7000/8000\n",
            "Training Step: 10157  | total loss: \u001b[1m\u001b[32m0.51628\u001b[0m\u001b[0m | time: 4.885s\n",
            "| SGD | epoch: 918 | loss: 0.51628 - acc: 0.8623 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10158  | total loss: \u001b[1m\u001b[32m0.51703\u001b[0m\u001b[0m | time: 0.636s\n",
            "| SGD | epoch: 919 | loss: 0.51703 - acc: 0.8627 -- iter: 1000/8000\n",
            "Training Step: 10159  | total loss: \u001b[1m\u001b[32m0.51668\u001b[0m\u001b[0m | time: 1.267s\n",
            "| SGD | epoch: 919 | loss: 0.51668 - acc: 0.8619 -- iter: 2000/8000\n",
            "Training Step: 10160  | total loss: \u001b[1m\u001b[32m0.52047\u001b[0m\u001b[0m | time: 1.902s\n",
            "| SGD | epoch: 919 | loss: 0.52047 - acc: 0.8609 -- iter: 3000/8000\n",
            "Training Step: 10161  | total loss: \u001b[1m\u001b[32m0.51675\u001b[0m\u001b[0m | time: 2.532s\n",
            "| SGD | epoch: 919 | loss: 0.51675 - acc: 0.8625 -- iter: 4000/8000\n",
            "Training Step: 10162  | total loss: \u001b[1m\u001b[32m0.51414\u001b[0m\u001b[0m | time: 3.162s\n",
            "| SGD | epoch: 919 | loss: 0.51414 - acc: 0.8637 -- iter: 5000/8000\n",
            "Training Step: 10163  | total loss: \u001b[1m\u001b[32m0.51691\u001b[0m\u001b[0m | time: 3.793s\n",
            "| SGD | epoch: 919 | loss: 0.51691 - acc: 0.8622 -- iter: 6000/8000\n",
            "Training Step: 10164  | total loss: \u001b[1m\u001b[32m0.51896\u001b[0m\u001b[0m | time: 4.423s\n",
            "| SGD | epoch: 919 | loss: 0.51896 - acc: 0.8616 -- iter: 7000/8000\n",
            "Training Step: 10165  | total loss: \u001b[1m\u001b[32m0.51588\u001b[0m\u001b[0m | time: 5.049s\n",
            "| SGD | epoch: 919 | loss: 0.51588 - acc: 0.8624 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10166  | total loss: \u001b[1m\u001b[32m0.51961\u001b[0m\u001b[0m | time: 0.631s\n",
            "| SGD | epoch: 920 | loss: 0.51961 - acc: 0.8611 -- iter: 1000/8000\n",
            "Training Step: 10167  | total loss: \u001b[1m\u001b[32m0.52006\u001b[0m\u001b[0m | time: 1.268s\n",
            "| SGD | epoch: 920 | loss: 0.52006 - acc: 0.8619 -- iter: 2000/8000\n",
            "Training Step: 10168  | total loss: \u001b[1m\u001b[32m0.51833\u001b[0m\u001b[0m | time: 1.902s\n",
            "| SGD | epoch: 920 | loss: 0.51833 - acc: 0.8630 -- iter: 3000/8000\n",
            "Training Step: 10169  | total loss: \u001b[1m\u001b[32m0.52258\u001b[0m\u001b[0m | time: 2.531s\n",
            "| SGD | epoch: 920 | loss: 0.52258 - acc: 0.8631 -- iter: 4000/8000\n",
            "Training Step: 10170  | total loss: \u001b[1m\u001b[32m0.52047\u001b[0m\u001b[0m | time: 3.163s\n",
            "| SGD | epoch: 920 | loss: 0.52047 - acc: 0.8643 -- iter: 5000/8000\n",
            "Training Step: 10171  | total loss: \u001b[1m\u001b[32m0.52102\u001b[0m\u001b[0m | time: 3.795s\n",
            "| SGD | epoch: 920 | loss: 0.52102 - acc: 0.8638 -- iter: 6000/8000\n",
            "Training Step: 10172  | total loss: \u001b[1m\u001b[32m0.52046\u001b[0m\u001b[0m | time: 4.425s\n",
            "| SGD | epoch: 920 | loss: 0.52046 - acc: 0.8636 -- iter: 7000/8000\n",
            "Training Step: 10173  | total loss: \u001b[1m\u001b[32m0.52023\u001b[0m\u001b[0m | time: 5.069s\n",
            "| SGD | epoch: 920 | loss: 0.52023 - acc: 0.8626 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10174  | total loss: \u001b[1m\u001b[32m0.52029\u001b[0m\u001b[0m | time: 0.639s\n",
            "| SGD | epoch: 921 | loss: 0.52029 - acc: 0.8612 -- iter: 1000/8000\n",
            "Training Step: 10175  | total loss: \u001b[1m\u001b[32m0.51782\u001b[0m\u001b[0m | time: 1.276s\n",
            "| SGD | epoch: 921 | loss: 0.51782 - acc: 0.8615 -- iter: 2000/8000\n",
            "Training Step: 10176  | total loss: \u001b[1m\u001b[32m0.51891\u001b[0m\u001b[0m | time: 1.916s\n",
            "| SGD | epoch: 921 | loss: 0.51891 - acc: 0.8605 -- iter: 3000/8000\n",
            "Training Step: 10177  | total loss: \u001b[1m\u001b[32m0.52026\u001b[0m\u001b[0m | time: 2.556s\n",
            "| SGD | epoch: 921 | loss: 0.52026 - acc: 0.8586 -- iter: 4000/8000\n",
            "Training Step: 10178  | total loss: \u001b[1m\u001b[32m0.51816\u001b[0m\u001b[0m | time: 3.201s\n",
            "| SGD | epoch: 921 | loss: 0.51816 - acc: 0.8607 -- iter: 5000/8000\n",
            "Training Step: 10179  | total loss: \u001b[1m\u001b[32m0.51704\u001b[0m\u001b[0m | time: 3.819s\n",
            "| SGD | epoch: 921 | loss: 0.51704 - acc: 0.8601 -- iter: 6000/8000\n",
            "Training Step: 10180  | total loss: \u001b[1m\u001b[32m0.51584\u001b[0m\u001b[0m | time: 4.441s\n",
            "| SGD | epoch: 921 | loss: 0.51584 - acc: 0.8624 -- iter: 7000/8000\n",
            "Training Step: 10181  | total loss: \u001b[1m\u001b[32m0.51812\u001b[0m\u001b[0m | time: 5.081s\n",
            "| SGD | epoch: 921 | loss: 0.51812 - acc: 0.8612 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10182  | total loss: \u001b[1m\u001b[32m0.51968\u001b[0m\u001b[0m | time: 0.632s\n",
            "| SGD | epoch: 922 | loss: 0.51968 - acc: 0.8618 -- iter: 1000/8000\n",
            "Training Step: 10183  | total loss: \u001b[1m\u001b[32m0.52026\u001b[0m\u001b[0m | time: 1.265s\n",
            "| SGD | epoch: 922 | loss: 0.52026 - acc: 0.8608 -- iter: 2000/8000\n",
            "Training Step: 10184  | total loss: \u001b[1m\u001b[32m0.51999\u001b[0m\u001b[0m | time: 1.898s\n",
            "| SGD | epoch: 922 | loss: 0.51999 - acc: 0.8614 -- iter: 3000/8000\n",
            "Training Step: 10185  | total loss: \u001b[1m\u001b[32m0.51743\u001b[0m\u001b[0m | time: 2.531s\n",
            "| SGD | epoch: 922 | loss: 0.51743 - acc: 0.8618 -- iter: 4000/8000\n",
            "Training Step: 10186  | total loss: \u001b[1m\u001b[32m0.51777\u001b[0m\u001b[0m | time: 3.162s\n",
            "| SGD | epoch: 922 | loss: 0.51777 - acc: 0.8613 -- iter: 5000/8000\n",
            "Training Step: 10187  | total loss: \u001b[1m\u001b[32m0.51847\u001b[0m\u001b[0m | time: 3.794s\n",
            "| SGD | epoch: 922 | loss: 0.51847 - acc: 0.8616 -- iter: 6000/8000\n",
            "Training Step: 10188  | total loss: \u001b[1m\u001b[32m0.52014\u001b[0m\u001b[0m | time: 4.424s\n",
            "| SGD | epoch: 922 | loss: 0.52014 - acc: 0.8613 -- iter: 7000/8000\n",
            "Training Step: 10189  | total loss: \u001b[1m\u001b[32m0.51829\u001b[0m\u001b[0m | time: 5.056s\n",
            "| SGD | epoch: 922 | loss: 0.51829 - acc: 0.8623 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10190  | total loss: \u001b[1m\u001b[32m0.51890\u001b[0m\u001b[0m | time: 0.633s\n",
            "| SGD | epoch: 923 | loss: 0.51890 - acc: 0.8617 -- iter: 1000/8000\n",
            "Training Step: 10191  | total loss: \u001b[1m\u001b[32m0.51879\u001b[0m\u001b[0m | time: 1.266s\n",
            "| SGD | epoch: 923 | loss: 0.51879 - acc: 0.8621 -- iter: 2000/8000\n",
            "Training Step: 10192  | total loss: \u001b[1m\u001b[32m0.51858\u001b[0m\u001b[0m | time: 1.898s\n",
            "| SGD | epoch: 923 | loss: 0.51858 - acc: 0.8631 -- iter: 3000/8000\n",
            "Training Step: 10193  | total loss: \u001b[1m\u001b[32m0.51796\u001b[0m\u001b[0m | time: 2.554s\n",
            "| SGD | epoch: 923 | loss: 0.51796 - acc: 0.8625 -- iter: 4000/8000\n",
            "Training Step: 10194  | total loss: \u001b[1m\u001b[32m0.51931\u001b[0m\u001b[0m | time: 3.193s\n",
            "| SGD | epoch: 923 | loss: 0.51931 - acc: 0.8627 -- iter: 5000/8000\n",
            "Training Step: 10195  | total loss: \u001b[1m\u001b[32m0.51916\u001b[0m\u001b[0m | time: 3.834s\n",
            "| SGD | epoch: 923 | loss: 0.51916 - acc: 0.8619 -- iter: 6000/8000\n",
            "Training Step: 10196  | total loss: \u001b[1m\u001b[32m0.51736\u001b[0m\u001b[0m | time: 4.475s\n",
            "| SGD | epoch: 923 | loss: 0.51736 - acc: 0.8631 -- iter: 7000/8000\n",
            "Training Step: 10197  | total loss: \u001b[1m\u001b[32m0.51689\u001b[0m\u001b[0m | time: 5.116s\n",
            "| SGD | epoch: 923 | loss: 0.51689 - acc: 0.8624 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10198  | total loss: \u001b[1m\u001b[32m0.51223\u001b[0m\u001b[0m | time: 0.643s\n",
            "| SGD | epoch: 924 | loss: 0.51223 - acc: 0.8653 -- iter: 1000/8000\n",
            "Training Step: 10199  | total loss: \u001b[1m\u001b[32m0.51352\u001b[0m\u001b[0m | time: 1.292s\n",
            "| SGD | epoch: 924 | loss: 0.51352 - acc: 0.8642 -- iter: 2000/8000\n",
            "Training Step: 10200  | total loss: \u001b[1m\u001b[32m0.51411\u001b[0m\u001b[0m | time: 1.932s\n",
            "| SGD | epoch: 924 | loss: 0.51411 - acc: 0.8635 -- iter: 3000/8000\n",
            "Training Step: 10201  | total loss: \u001b[1m\u001b[32m0.51535\u001b[0m\u001b[0m | time: 2.576s\n",
            "| SGD | epoch: 924 | loss: 0.51535 - acc: 0.8643 -- iter: 4000/8000\n",
            "Training Step: 10202  | total loss: \u001b[1m\u001b[32m0.51826\u001b[0m\u001b[0m | time: 3.205s\n",
            "| SGD | epoch: 924 | loss: 0.51826 - acc: 0.8642 -- iter: 5000/8000\n",
            "Training Step: 10203  | total loss: \u001b[1m\u001b[32m0.51828\u001b[0m\u001b[0m | time: 3.839s\n",
            "| SGD | epoch: 924 | loss: 0.51828 - acc: 0.8638 -- iter: 6000/8000\n",
            "Training Step: 10204  | total loss: \u001b[1m\u001b[32m0.52003\u001b[0m\u001b[0m | time: 4.471s\n",
            "| SGD | epoch: 924 | loss: 0.52003 - acc: 0.8641 -- iter: 7000/8000\n",
            "Training Step: 10205  | total loss: \u001b[1m\u001b[32m0.51998\u001b[0m\u001b[0m | time: 5.104s\n",
            "| SGD | epoch: 924 | loss: 0.51998 - acc: 0.8642 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10206  | total loss: \u001b[1m\u001b[32m0.51811\u001b[0m\u001b[0m | time: 0.616s\n",
            "| SGD | epoch: 925 | loss: 0.51811 - acc: 0.8648 -- iter: 1000/8000\n",
            "Training Step: 10207  | total loss: \u001b[1m\u001b[32m0.52037\u001b[0m\u001b[0m | time: 1.227s\n",
            "| SGD | epoch: 925 | loss: 0.52037 - acc: 0.8623 -- iter: 2000/8000\n",
            "Training Step: 10208  | total loss: \u001b[1m\u001b[32m0.51937\u001b[0m\u001b[0m | time: 1.842s\n",
            "| SGD | epoch: 925 | loss: 0.51937 - acc: 0.8622 -- iter: 3000/8000\n",
            "Training Step: 10209  | total loss: \u001b[1m\u001b[32m0.51792\u001b[0m\u001b[0m | time: 2.347s\n",
            "| SGD | epoch: 925 | loss: 0.51792 - acc: 0.8625 -- iter: 4000/8000\n",
            "Training Step: 10210  | total loss: \u001b[1m\u001b[32m0.51736\u001b[0m\u001b[0m | time: 2.849s\n",
            "| SGD | epoch: 925 | loss: 0.51736 - acc: 0.8626 -- iter: 5000/8000\n",
            "Training Step: 10211  | total loss: \u001b[1m\u001b[32m0.51603\u001b[0m\u001b[0m | time: 3.352s\n",
            "| SGD | epoch: 925 | loss: 0.51603 - acc: 0.8643 -- iter: 6000/8000\n",
            "Training Step: 10212  | total loss: \u001b[1m\u001b[32m0.51526\u001b[0m\u001b[0m | time: 3.854s\n",
            "| SGD | epoch: 925 | loss: 0.51526 - acc: 0.8652 -- iter: 7000/8000\n",
            "Training Step: 10213  | total loss: \u001b[1m\u001b[32m0.51883\u001b[0m\u001b[0m | time: 4.360s\n",
            "| SGD | epoch: 925 | loss: 0.51883 - acc: 0.8645 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10214  | total loss: \u001b[1m\u001b[32m0.51686\u001b[0m\u001b[0m | time: 0.503s\n",
            "| SGD | epoch: 926 | loss: 0.51686 - acc: 0.8651 -- iter: 1000/8000\n",
            "Training Step: 10215  | total loss: \u001b[1m\u001b[32m0.51921\u001b[0m\u001b[0m | time: 1.017s\n",
            "| SGD | epoch: 926 | loss: 0.51921 - acc: 0.8642 -- iter: 2000/8000\n",
            "Training Step: 10216  | total loss: \u001b[1m\u001b[32m0.51818\u001b[0m\u001b[0m | time: 1.518s\n",
            "| SGD | epoch: 926 | loss: 0.51818 - acc: 0.8641 -- iter: 3000/8000\n",
            "Training Step: 10217  | total loss: \u001b[1m\u001b[32m0.51442\u001b[0m\u001b[0m | time: 2.032s\n",
            "| SGD | epoch: 926 | loss: 0.51442 - acc: 0.8651 -- iter: 4000/8000\n",
            "Training Step: 10218  | total loss: \u001b[1m\u001b[32m0.51113\u001b[0m\u001b[0m | time: 2.555s\n",
            "| SGD | epoch: 926 | loss: 0.51113 - acc: 0.8659 -- iter: 5000/8000\n",
            "Training Step: 10219  | total loss: \u001b[1m\u001b[32m0.51174\u001b[0m\u001b[0m | time: 3.070s\n",
            "| SGD | epoch: 926 | loss: 0.51174 - acc: 0.8662 -- iter: 6000/8000\n",
            "Training Step: 10220  | total loss: \u001b[1m\u001b[32m0.51181\u001b[0m\u001b[0m | time: 3.587s\n",
            "| SGD | epoch: 926 | loss: 0.51181 - acc: 0.8652 -- iter: 7000/8000\n",
            "Training Step: 10221  | total loss: \u001b[1m\u001b[32m0.51143\u001b[0m\u001b[0m | time: 4.099s\n",
            "| SGD | epoch: 926 | loss: 0.51143 - acc: 0.8647 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10222  | total loss: \u001b[1m\u001b[32m0.51024\u001b[0m\u001b[0m | time: 0.524s\n",
            "| SGD | epoch: 927 | loss: 0.51024 - acc: 0.8656 -- iter: 1000/8000\n",
            "Training Step: 10223  | total loss: \u001b[1m\u001b[32m0.51276\u001b[0m\u001b[0m | time: 1.043s\n",
            "| SGD | epoch: 927 | loss: 0.51276 - acc: 0.8658 -- iter: 2000/8000\n",
            "Training Step: 10224  | total loss: \u001b[1m\u001b[32m0.51139\u001b[0m\u001b[0m | time: 1.565s\n",
            "| SGD | epoch: 927 | loss: 0.51139 - acc: 0.8661 -- iter: 3000/8000\n",
            "Training Step: 10225  | total loss: \u001b[1m\u001b[32m0.50882\u001b[0m\u001b[0m | time: 2.084s\n",
            "| SGD | epoch: 927 | loss: 0.50882 - acc: 0.8670 -- iter: 4000/8000\n",
            "Training Step: 10226  | total loss: \u001b[1m\u001b[32m0.51283\u001b[0m\u001b[0m | time: 2.600s\n",
            "| SGD | epoch: 927 | loss: 0.51283 - acc: 0.8649 -- iter: 5000/8000\n",
            "Training Step: 10227  | total loss: \u001b[1m\u001b[32m0.51496\u001b[0m\u001b[0m | time: 3.113s\n",
            "| SGD | epoch: 927 | loss: 0.51496 - acc: 0.8641 -- iter: 6000/8000\n",
            "Training Step: 10228  | total loss: \u001b[1m\u001b[32m0.51815\u001b[0m\u001b[0m | time: 3.620s\n",
            "| SGD | epoch: 927 | loss: 0.51815 - acc: 0.8620 -- iter: 7000/8000\n",
            "Training Step: 10229  | total loss: \u001b[1m\u001b[32m0.51595\u001b[0m\u001b[0m | time: 4.130s\n",
            "| SGD | epoch: 927 | loss: 0.51595 - acc: 0.8625 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10230  | total loss: \u001b[1m\u001b[32m0.51787\u001b[0m\u001b[0m | time: 0.507s\n",
            "| SGD | epoch: 928 | loss: 0.51787 - acc: 0.8602 -- iter: 1000/8000\n",
            "Training Step: 10231  | total loss: \u001b[1m\u001b[32m0.51370\u001b[0m\u001b[0m | time: 1.031s\n",
            "| SGD | epoch: 928 | loss: 0.51370 - acc: 0.8629 -- iter: 2000/8000\n",
            "Training Step: 10232  | total loss: \u001b[1m\u001b[32m0.51150\u001b[0m\u001b[0m | time: 1.553s\n",
            "| SGD | epoch: 928 | loss: 0.51150 - acc: 0.8646 -- iter: 3000/8000\n",
            "Training Step: 10233  | total loss: \u001b[1m\u001b[32m0.51434\u001b[0m\u001b[0m | time: 2.070s\n",
            "| SGD | epoch: 928 | loss: 0.51434 - acc: 0.8624 -- iter: 4000/8000\n",
            "Training Step: 10234  | total loss: \u001b[1m\u001b[32m0.51196\u001b[0m\u001b[0m | time: 2.595s\n",
            "| SGD | epoch: 928 | loss: 0.51196 - acc: 0.8632 -- iter: 5000/8000\n",
            "Training Step: 10235  | total loss: \u001b[1m\u001b[32m0.51236\u001b[0m\u001b[0m | time: 3.100s\n",
            "| SGD | epoch: 928 | loss: 0.51236 - acc: 0.8634 -- iter: 6000/8000\n",
            "Training Step: 10236  | total loss: \u001b[1m\u001b[32m0.51239\u001b[0m\u001b[0m | time: 3.610s\n",
            "| SGD | epoch: 928 | loss: 0.51239 - acc: 0.8626 -- iter: 7000/8000\n",
            "Training Step: 10237  | total loss: \u001b[1m\u001b[32m0.51368\u001b[0m\u001b[0m | time: 4.129s\n",
            "| SGD | epoch: 928 | loss: 0.51368 - acc: 0.8627 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10238  | total loss: \u001b[1m\u001b[32m0.51802\u001b[0m\u001b[0m | time: 0.519s\n",
            "| SGD | epoch: 929 | loss: 0.51802 - acc: 0.8610 -- iter: 1000/8000\n",
            "Training Step: 10239  | total loss: \u001b[1m\u001b[32m0.51730\u001b[0m\u001b[0m | time: 1.034s\n",
            "| SGD | epoch: 929 | loss: 0.51730 - acc: 0.8625 -- iter: 2000/8000\n",
            "Training Step: 10240  | total loss: \u001b[1m\u001b[32m0.51724\u001b[0m\u001b[0m | time: 1.549s\n",
            "| SGD | epoch: 929 | loss: 0.51724 - acc: 0.8635 -- iter: 3000/8000\n",
            "Training Step: 10241  | total loss: \u001b[1m\u001b[32m0.51508\u001b[0m\u001b[0m | time: 2.058s\n",
            "| SGD | epoch: 929 | loss: 0.51508 - acc: 0.8648 -- iter: 4000/8000\n",
            "Training Step: 10242  | total loss: \u001b[1m\u001b[32m0.51516\u001b[0m\u001b[0m | time: 2.605s\n",
            "| SGD | epoch: 929 | loss: 0.51516 - acc: 0.8644 -- iter: 5000/8000\n",
            "Training Step: 10243  | total loss: \u001b[1m\u001b[32m0.51537\u001b[0m\u001b[0m | time: 3.154s\n",
            "| SGD | epoch: 929 | loss: 0.51537 - acc: 0.8632 -- iter: 6000/8000\n",
            "Training Step: 10244  | total loss: \u001b[1m\u001b[32m0.51772\u001b[0m\u001b[0m | time: 3.671s\n",
            "| SGD | epoch: 929 | loss: 0.51772 - acc: 0.8628 -- iter: 7000/8000\n",
            "Training Step: 10245  | total loss: \u001b[1m\u001b[32m0.51940\u001b[0m\u001b[0m | time: 4.191s\n",
            "| SGD | epoch: 929 | loss: 0.51940 - acc: 0.8621 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10246  | total loss: \u001b[1m\u001b[32m0.51787\u001b[0m\u001b[0m | time: 0.534s\n",
            "| SGD | epoch: 930 | loss: 0.51787 - acc: 0.8618 -- iter: 1000/8000\n",
            "Training Step: 10247  | total loss: \u001b[1m\u001b[32m0.51701\u001b[0m\u001b[0m | time: 1.191s\n",
            "| SGD | epoch: 930 | loss: 0.51701 - acc: 0.8620 -- iter: 2000/8000\n",
            "Training Step: 10248  | total loss: \u001b[1m\u001b[32m0.51715\u001b[0m\u001b[0m | time: 1.714s\n",
            "| SGD | epoch: 930 | loss: 0.51715 - acc: 0.8620 -- iter: 3000/8000\n",
            "Training Step: 10249  | total loss: \u001b[1m\u001b[32m0.52045\u001b[0m\u001b[0m | time: 2.243s\n",
            "| SGD | epoch: 930 | loss: 0.52045 - acc: 0.8600 -- iter: 4000/8000\n",
            "Training Step: 10250  | total loss: \u001b[1m\u001b[32m0.52056\u001b[0m\u001b[0m | time: 2.761s\n",
            "| SGD | epoch: 930 | loss: 0.52056 - acc: 0.8592 -- iter: 5000/8000\n",
            "Training Step: 10251  | total loss: \u001b[1m\u001b[32m0.52077\u001b[0m\u001b[0m | time: 3.279s\n",
            "| SGD | epoch: 930 | loss: 0.52077 - acc: 0.8589 -- iter: 6000/8000\n",
            "Training Step: 10252  | total loss: \u001b[1m\u001b[32m0.51973\u001b[0m\u001b[0m | time: 3.794s\n",
            "| SGD | epoch: 930 | loss: 0.51973 - acc: 0.8598 -- iter: 7000/8000\n",
            "Training Step: 10253  | total loss: \u001b[1m\u001b[32m0.51604\u001b[0m\u001b[0m | time: 4.312s\n",
            "| SGD | epoch: 930 | loss: 0.51604 - acc: 0.8615 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10254  | total loss: \u001b[1m\u001b[32m0.51207\u001b[0m\u001b[0m | time: 0.515s\n",
            "| SGD | epoch: 931 | loss: 0.51207 - acc: 0.8631 -- iter: 1000/8000\n",
            "Training Step: 10255  | total loss: \u001b[1m\u001b[32m0.51376\u001b[0m\u001b[0m | time: 1.028s\n",
            "| SGD | epoch: 931 | loss: 0.51376 - acc: 0.8617 -- iter: 2000/8000\n",
            "Training Step: 10256  | total loss: \u001b[1m\u001b[32m0.51169\u001b[0m\u001b[0m | time: 1.542s\n",
            "| SGD | epoch: 931 | loss: 0.51169 - acc: 0.8640 -- iter: 3000/8000\n",
            "Training Step: 10257  | total loss: \u001b[1m\u001b[32m0.51560\u001b[0m\u001b[0m | time: 2.077s\n",
            "| SGD | epoch: 931 | loss: 0.51560 - acc: 0.8625 -- iter: 4000/8000\n",
            "Training Step: 10258  | total loss: \u001b[1m\u001b[32m0.51319\u001b[0m\u001b[0m | time: 2.588s\n",
            "| SGD | epoch: 931 | loss: 0.51319 - acc: 0.8635 -- iter: 5000/8000\n",
            "Training Step: 10259  | total loss: \u001b[1m\u001b[32m0.51160\u001b[0m\u001b[0m | time: 3.093s\n",
            "| SGD | epoch: 931 | loss: 0.51160 - acc: 0.8648 -- iter: 6000/8000\n",
            "Training Step: 10260  | total loss: \u001b[1m\u001b[32m0.51055\u001b[0m\u001b[0m | time: 3.594s\n",
            "| SGD | epoch: 931 | loss: 0.51055 - acc: 0.8655 -- iter: 7000/8000\n",
            "Training Step: 10261  | total loss: \u001b[1m\u001b[32m0.51242\u001b[0m\u001b[0m | time: 4.104s\n",
            "| SGD | epoch: 931 | loss: 0.51242 - acc: 0.8664 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10262  | total loss: \u001b[1m\u001b[32m0.51441\u001b[0m\u001b[0m | time: 0.518s\n",
            "| SGD | epoch: 932 | loss: 0.51441 - acc: 0.8643 -- iter: 1000/8000\n",
            "Training Step: 10263  | total loss: \u001b[1m\u001b[32m0.51652\u001b[0m\u001b[0m | time: 1.033s\n",
            "| SGD | epoch: 932 | loss: 0.51652 - acc: 0.8635 -- iter: 2000/8000\n",
            "Training Step: 10264  | total loss: \u001b[1m\u001b[32m0.51745\u001b[0m\u001b[0m | time: 1.554s\n",
            "| SGD | epoch: 932 | loss: 0.51745 - acc: 0.8624 -- iter: 3000/8000\n",
            "Training Step: 10265  | total loss: \u001b[1m\u001b[32m0.51336\u001b[0m\u001b[0m | time: 2.078s\n",
            "| SGD | epoch: 932 | loss: 0.51336 - acc: 0.8641 -- iter: 4000/8000\n",
            "Training Step: 10266  | total loss: \u001b[1m\u001b[32m0.51661\u001b[0m\u001b[0m | time: 2.600s\n",
            "| SGD | epoch: 932 | loss: 0.51661 - acc: 0.8633 -- iter: 5000/8000\n",
            "Training Step: 10267  | total loss: \u001b[1m\u001b[32m0.51543\u001b[0m\u001b[0m | time: 3.122s\n",
            "| SGD | epoch: 932 | loss: 0.51543 - acc: 0.8632 -- iter: 6000/8000\n",
            "Training Step: 10268  | total loss: \u001b[1m\u001b[32m0.51264\u001b[0m\u001b[0m | time: 3.641s\n",
            "| SGD | epoch: 932 | loss: 0.51264 - acc: 0.8632 -- iter: 7000/8000\n",
            "Training Step: 10269  | total loss: \u001b[1m\u001b[32m0.51291\u001b[0m\u001b[0m | time: 4.159s\n",
            "| SGD | epoch: 932 | loss: 0.51291 - acc: 0.8632 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10270  | total loss: \u001b[1m\u001b[32m0.51180\u001b[0m\u001b[0m | time: 0.522s\n",
            "| SGD | epoch: 933 | loss: 0.51180 - acc: 0.8633 -- iter: 1000/8000\n",
            "Training Step: 10271  | total loss: \u001b[1m\u001b[32m0.51072\u001b[0m\u001b[0m | time: 1.047s\n",
            "| SGD | epoch: 933 | loss: 0.51072 - acc: 0.8631 -- iter: 2000/8000\n",
            "Training Step: 10272  | total loss: \u001b[1m\u001b[32m0.50979\u001b[0m\u001b[0m | time: 1.574s\n",
            "| SGD | epoch: 933 | loss: 0.50979 - acc: 0.8640 -- iter: 3000/8000\n",
            "Training Step: 10273  | total loss: \u001b[1m\u001b[32m0.51170\u001b[0m\u001b[0m | time: 2.105s\n",
            "| SGD | epoch: 933 | loss: 0.51170 - acc: 0.8646 -- iter: 4000/8000\n",
            "Training Step: 10274  | total loss: \u001b[1m\u001b[32m0.51179\u001b[0m\u001b[0m | time: 2.631s\n",
            "| SGD | epoch: 933 | loss: 0.51179 - acc: 0.8648 -- iter: 5000/8000\n",
            "Training Step: 10275  | total loss: \u001b[1m\u001b[32m0.51488\u001b[0m\u001b[0m | time: 3.157s\n",
            "| SGD | epoch: 933 | loss: 0.51488 - acc: 0.8613 -- iter: 6000/8000\n",
            "Training Step: 10276  | total loss: \u001b[1m\u001b[32m0.51138\u001b[0m\u001b[0m | time: 3.683s\n",
            "| SGD | epoch: 933 | loss: 0.51138 - acc: 0.8624 -- iter: 7000/8000\n",
            "Training Step: 10277  | total loss: \u001b[1m\u001b[32m0.51167\u001b[0m\u001b[0m | time: 4.208s\n",
            "| SGD | epoch: 933 | loss: 0.51167 - acc: 0.8634 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10278  | total loss: \u001b[1m\u001b[32m0.51425\u001b[0m\u001b[0m | time: 0.532s\n",
            "| SGD | epoch: 934 | loss: 0.51425 - acc: 0.8636 -- iter: 1000/8000\n",
            "Training Step: 10279  | total loss: \u001b[1m\u001b[32m0.51302\u001b[0m\u001b[0m | time: 1.058s\n",
            "| SGD | epoch: 934 | loss: 0.51302 - acc: 0.8637 -- iter: 2000/8000\n",
            "Training Step: 10280  | total loss: \u001b[1m\u001b[32m0.51236\u001b[0m\u001b[0m | time: 1.583s\n",
            "| SGD | epoch: 934 | loss: 0.51236 - acc: 0.8638 -- iter: 3000/8000\n",
            "Training Step: 10281  | total loss: \u001b[1m\u001b[32m0.51191\u001b[0m\u001b[0m | time: 2.101s\n",
            "| SGD | epoch: 934 | loss: 0.51191 - acc: 0.8639 -- iter: 4000/8000\n",
            "Training Step: 10282  | total loss: \u001b[1m\u001b[32m0.51121\u001b[0m\u001b[0m | time: 2.624s\n",
            "| SGD | epoch: 934 | loss: 0.51121 - acc: 0.8636 -- iter: 5000/8000\n",
            "Training Step: 10283  | total loss: \u001b[1m\u001b[32m0.51149\u001b[0m\u001b[0m | time: 3.145s\n",
            "| SGD | epoch: 934 | loss: 0.51149 - acc: 0.8642 -- iter: 6000/8000\n",
            "Training Step: 10284  | total loss: \u001b[1m\u001b[32m0.51352\u001b[0m\u001b[0m | time: 3.669s\n",
            "| SGD | epoch: 934 | loss: 0.51352 - acc: 0.8641 -- iter: 7000/8000\n",
            "Training Step: 10285  | total loss: \u001b[1m\u001b[32m0.51299\u001b[0m\u001b[0m | time: 4.186s\n",
            "| SGD | epoch: 934 | loss: 0.51299 - acc: 0.8644 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10286  | total loss: \u001b[1m\u001b[32m0.50910\u001b[0m\u001b[0m | time: 0.536s\n",
            "| SGD | epoch: 935 | loss: 0.50910 - acc: 0.8654 -- iter: 1000/8000\n",
            "Training Step: 10287  | total loss: \u001b[1m\u001b[32m0.50524\u001b[0m\u001b[0m | time: 1.051s\n",
            "| SGD | epoch: 935 | loss: 0.50524 - acc: 0.8664 -- iter: 2000/8000\n",
            "Training Step: 10288  | total loss: \u001b[1m\u001b[32m0.50917\u001b[0m\u001b[0m | time: 1.589s\n",
            "| SGD | epoch: 935 | loss: 0.50917 - acc: 0.8640 -- iter: 3000/8000\n",
            "Training Step: 10289  | total loss: \u001b[1m\u001b[32m0.50825\u001b[0m\u001b[0m | time: 2.099s\n",
            "| SGD | epoch: 935 | loss: 0.50825 - acc: 0.8651 -- iter: 4000/8000\n",
            "Training Step: 10290  | total loss: \u001b[1m\u001b[32m0.50752\u001b[0m\u001b[0m | time: 2.627s\n",
            "| SGD | epoch: 935 | loss: 0.50752 - acc: 0.8657 -- iter: 5000/8000\n",
            "Training Step: 10291  | total loss: \u001b[1m\u001b[32m0.51213\u001b[0m\u001b[0m | time: 3.147s\n",
            "| SGD | epoch: 935 | loss: 0.51213 - acc: 0.8628 -- iter: 6000/8000\n",
            "Training Step: 10292  | total loss: \u001b[1m\u001b[32m0.51226\u001b[0m\u001b[0m | time: 3.668s\n",
            "| SGD | epoch: 935 | loss: 0.51226 - acc: 0.8621 -- iter: 7000/8000\n",
            "Training Step: 10293  | total loss: \u001b[1m\u001b[32m0.50980\u001b[0m\u001b[0m | time: 4.192s\n",
            "| SGD | epoch: 935 | loss: 0.50980 - acc: 0.8617 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10294  | total loss: \u001b[1m\u001b[32m0.50748\u001b[0m\u001b[0m | time: 0.520s\n",
            "| SGD | epoch: 936 | loss: 0.50748 - acc: 0.8627 -- iter: 1000/8000\n",
            "Training Step: 10295  | total loss: \u001b[1m\u001b[32m0.50707\u001b[0m\u001b[0m | time: 1.042s\n",
            "| SGD | epoch: 936 | loss: 0.50707 - acc: 0.8618 -- iter: 2000/8000\n",
            "Training Step: 10296  | total loss: \u001b[1m\u001b[32m0.51000\u001b[0m\u001b[0m | time: 1.567s\n",
            "| SGD | epoch: 936 | loss: 0.51000 - acc: 0.8627 -- iter: 3000/8000\n",
            "Training Step: 10297  | total loss: \u001b[1m\u001b[32m0.51187\u001b[0m\u001b[0m | time: 2.092s\n",
            "| SGD | epoch: 936 | loss: 0.51187 - acc: 0.8621 -- iter: 4000/8000\n",
            "Training Step: 10298  | total loss: \u001b[1m\u001b[32m0.51214\u001b[0m\u001b[0m | time: 2.615s\n",
            "| SGD | epoch: 936 | loss: 0.51214 - acc: 0.8613 -- iter: 5000/8000\n",
            "Training Step: 10299  | total loss: \u001b[1m\u001b[32m0.51187\u001b[0m\u001b[0m | time: 3.141s\n",
            "| SGD | epoch: 936 | loss: 0.51187 - acc: 0.8612 -- iter: 6000/8000\n",
            "Training Step: 10300  | total loss: \u001b[1m\u001b[32m0.51127\u001b[0m\u001b[0m | time: 3.673s\n",
            "| SGD | epoch: 936 | loss: 0.51127 - acc: 0.8611 -- iter: 7000/8000\n",
            "Training Step: 10301  | total loss: \u001b[1m\u001b[32m0.51248\u001b[0m\u001b[0m | time: 4.202s\n",
            "| SGD | epoch: 936 | loss: 0.51248 - acc: 0.8624 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10302  | total loss: \u001b[1m\u001b[32m0.51177\u001b[0m\u001b[0m | time: 0.535s\n",
            "| SGD | epoch: 937 | loss: 0.51177 - acc: 0.8635 -- iter: 1000/8000\n",
            "Training Step: 10303  | total loss: \u001b[1m\u001b[32m0.51102\u001b[0m\u001b[0m | time: 1.064s\n",
            "| SGD | epoch: 937 | loss: 0.51102 - acc: 0.8630 -- iter: 2000/8000\n",
            "Training Step: 10304  | total loss: \u001b[1m\u001b[32m0.51063\u001b[0m\u001b[0m | time: 1.595s\n",
            "| SGD | epoch: 937 | loss: 0.51063 - acc: 0.8632 -- iter: 3000/8000\n",
            "Training Step: 10305  | total loss: \u001b[1m\u001b[32m0.51134\u001b[0m\u001b[0m | time: 2.124s\n",
            "| SGD | epoch: 937 | loss: 0.51134 - acc: 0.8640 -- iter: 4000/8000\n",
            "Training Step: 10306  | total loss: \u001b[1m\u001b[32m0.51155\u001b[0m\u001b[0m | time: 2.649s\n",
            "| SGD | epoch: 937 | loss: 0.51155 - acc: 0.8639 -- iter: 5000/8000\n",
            "Training Step: 10307  | total loss: \u001b[1m\u001b[32m0.51316\u001b[0m\u001b[0m | time: 3.179s\n",
            "| SGD | epoch: 937 | loss: 0.51316 - acc: 0.8636 -- iter: 6000/8000\n",
            "Training Step: 10308  | total loss: \u001b[1m\u001b[32m0.51504\u001b[0m\u001b[0m | time: 3.703s\n",
            "| SGD | epoch: 937 | loss: 0.51504 - acc: 0.8632 -- iter: 7000/8000\n",
            "Training Step: 10309  | total loss: \u001b[1m\u001b[32m0.51373\u001b[0m\u001b[0m | time: 4.230s\n",
            "| SGD | epoch: 937 | loss: 0.51373 - acc: 0.8633 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10310  | total loss: \u001b[1m\u001b[32m0.51344\u001b[0m\u001b[0m | time: 0.522s\n",
            "| SGD | epoch: 938 | loss: 0.51344 - acc: 0.8632 -- iter: 1000/8000\n",
            "Training Step: 10311  | total loss: \u001b[1m\u001b[32m0.50987\u001b[0m\u001b[0m | time: 1.047s\n",
            "| SGD | epoch: 938 | loss: 0.50987 - acc: 0.8648 -- iter: 2000/8000\n",
            "Training Step: 10312  | total loss: \u001b[1m\u001b[32m0.50978\u001b[0m\u001b[0m | time: 1.567s\n",
            "| SGD | epoch: 938 | loss: 0.50978 - acc: 0.8652 -- iter: 3000/8000\n",
            "Training Step: 10313  | total loss: \u001b[1m\u001b[32m0.51106\u001b[0m\u001b[0m | time: 2.093s\n",
            "| SGD | epoch: 938 | loss: 0.51106 - acc: 0.8639 -- iter: 4000/8000\n",
            "Training Step: 10314  | total loss: \u001b[1m\u001b[32m0.51014\u001b[0m\u001b[0m | time: 2.615s\n",
            "| SGD | epoch: 938 | loss: 0.51014 - acc: 0.8642 -- iter: 5000/8000\n",
            "Training Step: 10315  | total loss: \u001b[1m\u001b[32m0.50932\u001b[0m\u001b[0m | time: 3.138s\n",
            "| SGD | epoch: 938 | loss: 0.50932 - acc: 0.8644 -- iter: 6000/8000\n",
            "Training Step: 10316  | total loss: \u001b[1m\u001b[32m0.50845\u001b[0m\u001b[0m | time: 3.663s\n",
            "| SGD | epoch: 938 | loss: 0.50845 - acc: 0.8656 -- iter: 7000/8000\n",
            "Training Step: 10317  | total loss: \u001b[1m\u001b[32m0.50672\u001b[0m\u001b[0m | time: 4.182s\n",
            "| SGD | epoch: 938 | loss: 0.50672 - acc: 0.8661 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10318  | total loss: \u001b[1m\u001b[32m0.50682\u001b[0m\u001b[0m | time: 0.519s\n",
            "| SGD | epoch: 939 | loss: 0.50682 - acc: 0.8663 -- iter: 1000/8000\n",
            "Training Step: 10319  | total loss: \u001b[1m\u001b[32m0.51186\u001b[0m\u001b[0m | time: 1.042s\n",
            "| SGD | epoch: 939 | loss: 0.51186 - acc: 0.8637 -- iter: 2000/8000\n",
            "Training Step: 10320  | total loss: \u001b[1m\u001b[32m0.51162\u001b[0m\u001b[0m | time: 1.563s\n",
            "| SGD | epoch: 939 | loss: 0.51162 - acc: 0.8630 -- iter: 3000/8000\n",
            "Training Step: 10321  | total loss: \u001b[1m\u001b[32m0.51171\u001b[0m\u001b[0m | time: 2.081s\n",
            "| SGD | epoch: 939 | loss: 0.51171 - acc: 0.8642 -- iter: 4000/8000\n",
            "Training Step: 10322  | total loss: \u001b[1m\u001b[32m0.50989\u001b[0m\u001b[0m | time: 2.597s\n",
            "| SGD | epoch: 939 | loss: 0.50989 - acc: 0.8664 -- iter: 5000/8000\n",
            "Training Step: 10323  | total loss: \u001b[1m\u001b[32m0.50979\u001b[0m\u001b[0m | time: 3.116s\n",
            "| SGD | epoch: 939 | loss: 0.50979 - acc: 0.8662 -- iter: 6000/8000\n",
            "Training Step: 10324  | total loss: \u001b[1m\u001b[32m0.50656\u001b[0m\u001b[0m | time: 3.638s\n",
            "| SGD | epoch: 939 | loss: 0.50656 - acc: 0.8672 -- iter: 7000/8000\n",
            "Training Step: 10325  | total loss: \u001b[1m\u001b[32m0.50812\u001b[0m\u001b[0m | time: 4.173s\n",
            "| SGD | epoch: 939 | loss: 0.50812 - acc: 0.8680 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10326  | total loss: \u001b[1m\u001b[32m0.50909\u001b[0m\u001b[0m | time: 0.534s\n",
            "| SGD | epoch: 940 | loss: 0.50909 - acc: 0.8678 -- iter: 1000/8000\n",
            "Training Step: 10327  | total loss: \u001b[1m\u001b[32m0.50861\u001b[0m\u001b[0m | time: 1.060s\n",
            "| SGD | epoch: 940 | loss: 0.50861 - acc: 0.8673 -- iter: 2000/8000\n",
            "Training Step: 10328  | total loss: \u001b[1m\u001b[32m0.50832\u001b[0m\u001b[0m | time: 1.587s\n",
            "| SGD | epoch: 940 | loss: 0.50832 - acc: 0.8661 -- iter: 3000/8000\n",
            "Training Step: 10329  | total loss: \u001b[1m\u001b[32m0.50683\u001b[0m\u001b[0m | time: 2.124s\n",
            "| SGD | epoch: 940 | loss: 0.50683 - acc: 0.8680 -- iter: 4000/8000\n",
            "Training Step: 10330  | total loss: \u001b[1m\u001b[32m0.50843\u001b[0m\u001b[0m | time: 2.661s\n",
            "| SGD | epoch: 940 | loss: 0.50843 - acc: 0.8679 -- iter: 5000/8000\n",
            "Training Step: 10331  | total loss: \u001b[1m\u001b[32m0.51257\u001b[0m\u001b[0m | time: 3.177s\n",
            "| SGD | epoch: 940 | loss: 0.51257 - acc: 0.8667 -- iter: 6000/8000\n",
            "Training Step: 10332  | total loss: \u001b[1m\u001b[32m0.50988\u001b[0m\u001b[0m | time: 3.696s\n",
            "| SGD | epoch: 940 | loss: 0.50988 - acc: 0.8660 -- iter: 7000/8000\n",
            "Training Step: 10333  | total loss: \u001b[1m\u001b[32m0.50751\u001b[0m\u001b[0m | time: 4.217s\n",
            "| SGD | epoch: 940 | loss: 0.50751 - acc: 0.8678 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10334  | total loss: \u001b[1m\u001b[32m0.50553\u001b[0m\u001b[0m | time: 0.520s\n",
            "| SGD | epoch: 941 | loss: 0.50553 - acc: 0.8666 -- iter: 1000/8000\n",
            "Training Step: 10335  | total loss: \u001b[1m\u001b[32m0.50336\u001b[0m\u001b[0m | time: 1.038s\n",
            "| SGD | epoch: 941 | loss: 0.50336 - acc: 0.8662 -- iter: 2000/8000\n",
            "Training Step: 10336  | total loss: \u001b[1m\u001b[32m0.50431\u001b[0m\u001b[0m | time: 1.557s\n",
            "| SGD | epoch: 941 | loss: 0.50431 - acc: 0.8665 -- iter: 3000/8000\n",
            "Training Step: 10337  | total loss: \u001b[1m\u001b[32m0.50630\u001b[0m\u001b[0m | time: 2.080s\n",
            "| SGD | epoch: 941 | loss: 0.50630 - acc: 0.8652 -- iter: 4000/8000\n",
            "Training Step: 10338  | total loss: \u001b[1m\u001b[32m0.50427\u001b[0m\u001b[0m | time: 2.596s\n",
            "| SGD | epoch: 941 | loss: 0.50427 - acc: 0.8665 -- iter: 5000/8000\n",
            "Training Step: 10339  | total loss: \u001b[1m\u001b[32m0.50319\u001b[0m\u001b[0m | time: 3.111s\n",
            "| SGD | epoch: 941 | loss: 0.50319 - acc: 0.8666 -- iter: 6000/8000\n",
            "Training Step: 10340  | total loss: \u001b[1m\u001b[32m0.50770\u001b[0m\u001b[0m | time: 3.632s\n",
            "| SGD | epoch: 941 | loss: 0.50770 - acc: 0.8642 -- iter: 7000/8000\n",
            "Training Step: 10341  | total loss: \u001b[1m\u001b[32m0.50860\u001b[0m\u001b[0m | time: 4.150s\n",
            "| SGD | epoch: 941 | loss: 0.50860 - acc: 0.8640 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10342  | total loss: \u001b[1m\u001b[32m0.51112\u001b[0m\u001b[0m | time: 0.511s\n",
            "| SGD | epoch: 942 | loss: 0.51112 - acc: 0.8623 -- iter: 1000/8000\n",
            "Training Step: 10343  | total loss: \u001b[1m\u001b[32m0.50860\u001b[0m\u001b[0m | time: 1.032s\n",
            "| SGD | epoch: 942 | loss: 0.50860 - acc: 0.8628 -- iter: 2000/8000\n",
            "Training Step: 10344  | total loss: \u001b[1m\u001b[32m0.50638\u001b[0m\u001b[0m | time: 1.544s\n",
            "| SGD | epoch: 942 | loss: 0.50638 - acc: 0.8634 -- iter: 3000/8000\n",
            "Training Step: 10345  | total loss: \u001b[1m\u001b[32m0.50599\u001b[0m\u001b[0m | time: 2.055s\n",
            "| SGD | epoch: 942 | loss: 0.50599 - acc: 0.8650 -- iter: 4000/8000\n",
            "Training Step: 10346  | total loss: \u001b[1m\u001b[32m0.50480\u001b[0m\u001b[0m | time: 2.580s\n",
            "| SGD | epoch: 942 | loss: 0.50480 - acc: 0.8663 -- iter: 5000/8000\n",
            "Training Step: 10347  | total loss: \u001b[1m\u001b[32m0.50423\u001b[0m\u001b[0m | time: 3.083s\n",
            "| SGD | epoch: 942 | loss: 0.50423 - acc: 0.8676 -- iter: 6000/8000\n",
            "Training Step: 10348  | total loss: \u001b[1m\u001b[32m0.50418\u001b[0m\u001b[0m | time: 3.594s\n",
            "| SGD | epoch: 942 | loss: 0.50418 - acc: 0.8682 -- iter: 7000/8000\n",
            "Training Step: 10349  | total loss: \u001b[1m\u001b[32m0.50470\u001b[0m\u001b[0m | time: 4.108s\n",
            "| SGD | epoch: 942 | loss: 0.50470 - acc: 0.8677 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10350  | total loss: \u001b[1m\u001b[32m0.50529\u001b[0m\u001b[0m | time: 0.522s\n",
            "| SGD | epoch: 943 | loss: 0.50529 - acc: 0.8678 -- iter: 1000/8000\n",
            "Training Step: 10351  | total loss: \u001b[1m\u001b[32m0.50263\u001b[0m\u001b[0m | time: 1.040s\n",
            "| SGD | epoch: 943 | loss: 0.50263 - acc: 0.8680 -- iter: 2000/8000\n",
            "Training Step: 10352  | total loss: \u001b[1m\u001b[32m0.50265\u001b[0m\u001b[0m | time: 1.570s\n",
            "| SGD | epoch: 943 | loss: 0.50265 - acc: 0.8682 -- iter: 3000/8000\n",
            "Training Step: 10353  | total loss: \u001b[1m\u001b[32m0.50298\u001b[0m\u001b[0m | time: 2.093s\n",
            "| SGD | epoch: 943 | loss: 0.50298 - acc: 0.8681 -- iter: 4000/8000\n",
            "Training Step: 10354  | total loss: \u001b[1m\u001b[32m0.50547\u001b[0m\u001b[0m | time: 2.605s\n",
            "| SGD | epoch: 943 | loss: 0.50547 - acc: 0.8661 -- iter: 5000/8000\n",
            "Training Step: 10355  | total loss: \u001b[1m\u001b[32m0.50818\u001b[0m\u001b[0m | time: 3.122s\n",
            "| SGD | epoch: 943 | loss: 0.50818 - acc: 0.8661 -- iter: 6000/8000\n",
            "Training Step: 10356  | total loss: \u001b[1m\u001b[32m0.50707\u001b[0m\u001b[0m | time: 3.638s\n",
            "| SGD | epoch: 943 | loss: 0.50707 - acc: 0.8663 -- iter: 7000/8000\n",
            "Training Step: 10357  | total loss: \u001b[1m\u001b[32m0.50742\u001b[0m\u001b[0m | time: 4.161s\n",
            "| SGD | epoch: 943 | loss: 0.50742 - acc: 0.8657 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10358  | total loss: \u001b[1m\u001b[32m0.50411\u001b[0m\u001b[0m | time: 0.518s\n",
            "| SGD | epoch: 944 | loss: 0.50411 - acc: 0.8685 -- iter: 1000/8000\n",
            "Training Step: 10359  | total loss: \u001b[1m\u001b[32m0.50426\u001b[0m\u001b[0m | time: 1.032s\n",
            "| SGD | epoch: 944 | loss: 0.50426 - acc: 0.8665 -- iter: 2000/8000\n",
            "Training Step: 10360  | total loss: \u001b[1m\u001b[32m0.51127\u001b[0m\u001b[0m | time: 1.545s\n",
            "| SGD | epoch: 944 | loss: 0.51127 - acc: 0.8653 -- iter: 3000/8000\n",
            "Training Step: 10361  | total loss: \u001b[1m\u001b[32m0.51043\u001b[0m\u001b[0m | time: 2.046s\n",
            "| SGD | epoch: 944 | loss: 0.51043 - acc: 0.8646 -- iter: 4000/8000\n",
            "Training Step: 10362  | total loss: \u001b[1m\u001b[32m0.50933\u001b[0m\u001b[0m | time: 2.545s\n",
            "| SGD | epoch: 944 | loss: 0.50933 - acc: 0.8657 -- iter: 5000/8000\n",
            "Training Step: 10363  | total loss: \u001b[1m\u001b[32m0.50913\u001b[0m\u001b[0m | time: 3.057s\n",
            "| SGD | epoch: 944 | loss: 0.50913 - acc: 0.8666 -- iter: 6000/8000\n",
            "Training Step: 10364  | total loss: \u001b[1m\u001b[32m0.50698\u001b[0m\u001b[0m | time: 3.570s\n",
            "| SGD | epoch: 944 | loss: 0.50698 - acc: 0.8676 -- iter: 7000/8000\n",
            "Training Step: 10365  | total loss: \u001b[1m\u001b[32m0.50835\u001b[0m\u001b[0m | time: 4.084s\n",
            "| SGD | epoch: 944 | loss: 0.50835 - acc: 0.8677 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10366  | total loss: \u001b[1m\u001b[32m0.50852\u001b[0m\u001b[0m | time: 0.529s\n",
            "| SGD | epoch: 945 | loss: 0.50852 - acc: 0.8691 -- iter: 1000/8000\n",
            "Training Step: 10367  | total loss: \u001b[1m\u001b[32m0.51154\u001b[0m\u001b[0m | time: 1.028s\n",
            "| SGD | epoch: 945 | loss: 0.51154 - acc: 0.8669 -- iter: 2000/8000\n",
            "Training Step: 10368  | total loss: \u001b[1m\u001b[32m0.50948\u001b[0m\u001b[0m | time: 1.539s\n",
            "| SGD | epoch: 945 | loss: 0.50948 - acc: 0.8684 -- iter: 3000/8000\n",
            "Training Step: 10369  | total loss: \u001b[1m\u001b[32m0.50536\u001b[0m\u001b[0m | time: 2.053s\n",
            "| SGD | epoch: 945 | loss: 0.50536 - acc: 0.8697 -- iter: 4000/8000\n",
            "Training Step: 10370  | total loss: \u001b[1m\u001b[32m0.50379\u001b[0m\u001b[0m | time: 2.560s\n",
            "| SGD | epoch: 945 | loss: 0.50379 - acc: 0.8690 -- iter: 5000/8000\n",
            "Training Step: 10371  | total loss: \u001b[1m\u001b[32m0.50286\u001b[0m\u001b[0m | time: 3.077s\n",
            "| SGD | epoch: 945 | loss: 0.50286 - acc: 0.8680 -- iter: 6000/8000\n",
            "Training Step: 10372  | total loss: \u001b[1m\u001b[32m0.50352\u001b[0m\u001b[0m | time: 3.595s\n",
            "| SGD | epoch: 945 | loss: 0.50352 - acc: 0.8665 -- iter: 7000/8000\n",
            "Training Step: 10373  | total loss: \u001b[1m\u001b[32m0.50512\u001b[0m\u001b[0m | time: 4.112s\n",
            "| SGD | epoch: 945 | loss: 0.50512 - acc: 0.8662 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10374  | total loss: \u001b[1m\u001b[32m0.50615\u001b[0m\u001b[0m | time: 0.513s\n",
            "| SGD | epoch: 946 | loss: 0.50615 - acc: 0.8660 -- iter: 1000/8000\n",
            "Training Step: 10375  | total loss: \u001b[1m\u001b[32m0.50752\u001b[0m\u001b[0m | time: 1.036s\n",
            "| SGD | epoch: 946 | loss: 0.50752 - acc: 0.8661 -- iter: 2000/8000\n",
            "Training Step: 10376  | total loss: \u001b[1m\u001b[32m0.50739\u001b[0m\u001b[0m | time: 1.551s\n",
            "| SGD | epoch: 946 | loss: 0.50739 - acc: 0.8659 -- iter: 3000/8000\n",
            "Training Step: 10377  | total loss: \u001b[1m\u001b[32m0.50853\u001b[0m\u001b[0m | time: 2.059s\n",
            "| SGD | epoch: 946 | loss: 0.50853 - acc: 0.8666 -- iter: 4000/8000\n",
            "Training Step: 10378  | total loss: \u001b[1m\u001b[32m0.50809\u001b[0m\u001b[0m | time: 2.576s\n",
            "| SGD | epoch: 946 | loss: 0.50809 - acc: 0.8653 -- iter: 5000/8000\n",
            "Training Step: 10379  | total loss: \u001b[1m\u001b[32m0.50512\u001b[0m\u001b[0m | time: 3.087s\n",
            "| SGD | epoch: 946 | loss: 0.50512 - acc: 0.8666 -- iter: 6000/8000\n",
            "Training Step: 10380  | total loss: \u001b[1m\u001b[32m0.50246\u001b[0m\u001b[0m | time: 3.608s\n",
            "| SGD | epoch: 946 | loss: 0.50246 - acc: 0.8678 -- iter: 7000/8000\n",
            "Training Step: 10381  | total loss: \u001b[1m\u001b[32m0.50159\u001b[0m\u001b[0m | time: 4.128s\n",
            "| SGD | epoch: 946 | loss: 0.50159 - acc: 0.8667 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10382  | total loss: \u001b[1m\u001b[32m0.50090\u001b[0m\u001b[0m | time: 0.519s\n",
            "| SGD | epoch: 947 | loss: 0.50090 - acc: 0.8674 -- iter: 1000/8000\n",
            "Training Step: 10383  | total loss: \u001b[1m\u001b[32m0.50184\u001b[0m\u001b[0m | time: 1.045s\n",
            "| SGD | epoch: 947 | loss: 0.50184 - acc: 0.8681 -- iter: 2000/8000\n",
            "Training Step: 10384  | total loss: \u001b[1m\u001b[32m0.49829\u001b[0m\u001b[0m | time: 1.565s\n",
            "| SGD | epoch: 947 | loss: 0.49829 - acc: 0.8695 -- iter: 3000/8000\n",
            "Training Step: 10385  | total loss: \u001b[1m\u001b[32m0.50178\u001b[0m\u001b[0m | time: 2.080s\n",
            "| SGD | epoch: 947 | loss: 0.50178 - acc: 0.8673 -- iter: 4000/8000\n",
            "Training Step: 10386  | total loss: \u001b[1m\u001b[32m0.50101\u001b[0m\u001b[0m | time: 2.592s\n",
            "| SGD | epoch: 947 | loss: 0.50101 - acc: 0.8671 -- iter: 5000/8000\n",
            "Training Step: 10387  | total loss: \u001b[1m\u001b[32m0.50421\u001b[0m\u001b[0m | time: 3.105s\n",
            "| SGD | epoch: 947 | loss: 0.50421 - acc: 0.8652 -- iter: 6000/8000\n",
            "Training Step: 10388  | total loss: \u001b[1m\u001b[32m0.50581\u001b[0m\u001b[0m | time: 3.618s\n",
            "| SGD | epoch: 947 | loss: 0.50581 - acc: 0.8656 -- iter: 7000/8000\n",
            "Training Step: 10389  | total loss: \u001b[1m\u001b[32m0.50718\u001b[0m\u001b[0m | time: 4.133s\n",
            "| SGD | epoch: 947 | loss: 0.50718 - acc: 0.8656 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10390  | total loss: \u001b[1m\u001b[32m0.50632\u001b[0m\u001b[0m | time: 0.510s\n",
            "| SGD | epoch: 948 | loss: 0.50632 - acc: 0.8671 -- iter: 1000/8000\n",
            "Training Step: 10391  | total loss: \u001b[1m\u001b[32m0.50474\u001b[0m\u001b[0m | time: 1.029s\n",
            "| SGD | epoch: 948 | loss: 0.50474 - acc: 0.8668 -- iter: 2000/8000\n",
            "Training Step: 10392  | total loss: \u001b[1m\u001b[32m0.50285\u001b[0m\u001b[0m | time: 1.538s\n",
            "| SGD | epoch: 948 | loss: 0.50285 - acc: 0.8662 -- iter: 3000/8000\n",
            "Training Step: 10393  | total loss: \u001b[1m\u001b[32m0.50534\u001b[0m\u001b[0m | time: 2.050s\n",
            "| SGD | epoch: 948 | loss: 0.50534 - acc: 0.8653 -- iter: 4000/8000\n",
            "Training Step: 10394  | total loss: \u001b[1m\u001b[32m0.50478\u001b[0m\u001b[0m | time: 2.559s\n",
            "| SGD | epoch: 948 | loss: 0.50478 - acc: 0.8658 -- iter: 5000/8000\n",
            "Training Step: 10395  | total loss: \u001b[1m\u001b[32m0.50157\u001b[0m\u001b[0m | time: 3.072s\n",
            "| SGD | epoch: 948 | loss: 0.50157 - acc: 0.8668 -- iter: 6000/8000\n",
            "Training Step: 10396  | total loss: \u001b[1m\u001b[32m0.50427\u001b[0m\u001b[0m | time: 3.576s\n",
            "| SGD | epoch: 948 | loss: 0.50427 - acc: 0.8649 -- iter: 7000/8000\n",
            "Training Step: 10397  | total loss: \u001b[1m\u001b[32m0.50241\u001b[0m\u001b[0m | time: 4.088s\n",
            "| SGD | epoch: 948 | loss: 0.50241 - acc: 0.8660 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10398  | total loss: \u001b[1m\u001b[32m0.50078\u001b[0m\u001b[0m | time: 0.507s\n",
            "| SGD | epoch: 949 | loss: 0.50078 - acc: 0.8677 -- iter: 1000/8000\n",
            "Training Step: 10399  | total loss: \u001b[1m\u001b[32m0.50589\u001b[0m\u001b[0m | time: 1.023s\n",
            "| SGD | epoch: 949 | loss: 0.50589 - acc: 0.8653 -- iter: 2000/8000\n",
            "Training Step: 10400  | total loss: \u001b[1m\u001b[32m0.50512\u001b[0m\u001b[0m | time: 1.529s\n",
            "| SGD | epoch: 949 | loss: 0.50512 - acc: 0.8666 -- iter: 3000/8000\n",
            "Training Step: 10401  | total loss: \u001b[1m\u001b[32m0.50715\u001b[0m\u001b[0m | time: 2.047s\n",
            "| SGD | epoch: 949 | loss: 0.50715 - acc: 0.8651 -- iter: 4000/8000\n",
            "Training Step: 10402  | total loss: \u001b[1m\u001b[32m0.50300\u001b[0m\u001b[0m | time: 2.635s\n",
            "| SGD | epoch: 949 | loss: 0.50300 - acc: 0.8674 -- iter: 5000/8000\n",
            "Training Step: 10403  | total loss: \u001b[1m\u001b[32m0.50025\u001b[0m\u001b[0m | time: 3.149s\n",
            "| SGD | epoch: 949 | loss: 0.50025 - acc: 0.8681 -- iter: 6000/8000\n",
            "Training Step: 10404  | total loss: \u001b[1m\u001b[32m0.49747\u001b[0m\u001b[0m | time: 3.661s\n",
            "| SGD | epoch: 949 | loss: 0.49747 - acc: 0.8703 -- iter: 7000/8000\n",
            "Training Step: 10405  | total loss: \u001b[1m\u001b[32m0.49684\u001b[0m\u001b[0m | time: 4.176s\n",
            "| SGD | epoch: 949 | loss: 0.49684 - acc: 0.8706 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10406  | total loss: \u001b[1m\u001b[32m0.49876\u001b[0m\u001b[0m | time: 0.510s\n",
            "| SGD | epoch: 950 | loss: 0.49876 - acc: 0.8709 -- iter: 1000/8000\n",
            "Training Step: 10407  | total loss: \u001b[1m\u001b[32m0.50062\u001b[0m\u001b[0m | time: 1.035s\n",
            "| SGD | epoch: 950 | loss: 0.50062 - acc: 0.8700 -- iter: 2000/8000\n",
            "Training Step: 10408  | total loss: \u001b[1m\u001b[32m0.50905\u001b[0m\u001b[0m | time: 1.550s\n",
            "| SGD | epoch: 950 | loss: 0.50905 - acc: 0.8668 -- iter: 3000/8000\n",
            "Training Step: 10409  | total loss: \u001b[1m\u001b[32m0.50655\u001b[0m\u001b[0m | time: 2.072s\n",
            "| SGD | epoch: 950 | loss: 0.50655 - acc: 0.8673 -- iter: 4000/8000\n",
            "Training Step: 10410  | total loss: \u001b[1m\u001b[32m0.50692\u001b[0m\u001b[0m | time: 2.601s\n",
            "| SGD | epoch: 950 | loss: 0.50692 - acc: 0.8677 -- iter: 5000/8000\n",
            "Training Step: 10411  | total loss: \u001b[1m\u001b[32m0.50482\u001b[0m\u001b[0m | time: 3.124s\n",
            "| SGD | epoch: 950 | loss: 0.50482 - acc: 0.8680 -- iter: 6000/8000\n",
            "Training Step: 10412  | total loss: \u001b[1m\u001b[32m0.50287\u001b[0m\u001b[0m | time: 3.642s\n",
            "| SGD | epoch: 950 | loss: 0.50287 - acc: 0.8694 -- iter: 7000/8000\n",
            "Training Step: 10413  | total loss: \u001b[1m\u001b[32m0.50519\u001b[0m\u001b[0m | time: 4.164s\n",
            "| SGD | epoch: 950 | loss: 0.50519 - acc: 0.8682 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10414  | total loss: \u001b[1m\u001b[32m0.50390\u001b[0m\u001b[0m | time: 0.522s\n",
            "| SGD | epoch: 951 | loss: 0.50390 - acc: 0.8681 -- iter: 1000/8000\n",
            "Training Step: 10415  | total loss: \u001b[1m\u001b[32m0.50417\u001b[0m\u001b[0m | time: 1.039s\n",
            "| SGD | epoch: 951 | loss: 0.50417 - acc: 0.8686 -- iter: 2000/8000\n",
            "Training Step: 10416  | total loss: \u001b[1m\u001b[32m0.50362\u001b[0m\u001b[0m | time: 1.542s\n",
            "| SGD | epoch: 951 | loss: 0.50362 - acc: 0.8688 -- iter: 3000/8000\n",
            "Training Step: 10417  | total loss: \u001b[1m\u001b[32m0.50461\u001b[0m\u001b[0m | time: 2.062s\n",
            "| SGD | epoch: 951 | loss: 0.50461 - acc: 0.8682 -- iter: 4000/8000\n",
            "Training Step: 10418  | total loss: \u001b[1m\u001b[32m0.50541\u001b[0m\u001b[0m | time: 2.576s\n",
            "| SGD | epoch: 951 | loss: 0.50541 - acc: 0.8668 -- iter: 5000/8000\n",
            "Training Step: 10419  | total loss: \u001b[1m\u001b[32m0.50528\u001b[0m\u001b[0m | time: 3.093s\n",
            "| SGD | epoch: 951 | loss: 0.50528 - acc: 0.8672 -- iter: 6000/8000\n",
            "Training Step: 10420  | total loss: \u001b[1m\u001b[32m0.50472\u001b[0m\u001b[0m | time: 3.617s\n",
            "| SGD | epoch: 951 | loss: 0.50472 - acc: 0.8671 -- iter: 7000/8000\n",
            "Training Step: 10421  | total loss: \u001b[1m\u001b[32m0.50244\u001b[0m\u001b[0m | time: 4.126s\n",
            "| SGD | epoch: 951 | loss: 0.50244 - acc: 0.8688 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10422  | total loss: \u001b[1m\u001b[32m0.49909\u001b[0m\u001b[0m | time: 0.522s\n",
            "| SGD | epoch: 952 | loss: 0.49909 - acc: 0.8705 -- iter: 1000/8000\n",
            "Training Step: 10423  | total loss: \u001b[1m\u001b[32m0.50257\u001b[0m\u001b[0m | time: 1.045s\n",
            "| SGD | epoch: 952 | loss: 0.50257 - acc: 0.8687 -- iter: 2000/8000\n",
            "Training Step: 10424  | total loss: \u001b[1m\u001b[32m0.50439\u001b[0m\u001b[0m | time: 1.558s\n",
            "| SGD | epoch: 952 | loss: 0.50439 - acc: 0.8667 -- iter: 3000/8000\n",
            "Training Step: 10425  | total loss: \u001b[1m\u001b[32m0.50592\u001b[0m\u001b[0m | time: 2.075s\n",
            "| SGD | epoch: 952 | loss: 0.50592 - acc: 0.8658 -- iter: 4000/8000\n",
            "Training Step: 10426  | total loss: \u001b[1m\u001b[32m0.50488\u001b[0m\u001b[0m | time: 2.594s\n",
            "| SGD | epoch: 952 | loss: 0.50488 - acc: 0.8665 -- iter: 5000/8000\n",
            "Training Step: 10427  | total loss: \u001b[1m\u001b[32m0.50606\u001b[0m\u001b[0m | time: 3.115s\n",
            "| SGD | epoch: 952 | loss: 0.50606 - acc: 0.8661 -- iter: 6000/8000\n",
            "Training Step: 10428  | total loss: \u001b[1m\u001b[32m0.50592\u001b[0m\u001b[0m | time: 3.635s\n",
            "| SGD | epoch: 952 | loss: 0.50592 - acc: 0.8674 -- iter: 7000/8000\n",
            "Training Step: 10429  | total loss: \u001b[1m\u001b[32m0.50632\u001b[0m\u001b[0m | time: 4.162s\n",
            "| SGD | epoch: 952 | loss: 0.50632 - acc: 0.8675 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10430  | total loss: \u001b[1m\u001b[32m0.50633\u001b[0m\u001b[0m | time: 0.527s\n",
            "| SGD | epoch: 953 | loss: 0.50633 - acc: 0.8683 -- iter: 1000/8000\n",
            "Training Step: 10431  | total loss: \u001b[1m\u001b[32m0.50752\u001b[0m\u001b[0m | time: 1.046s\n",
            "| SGD | epoch: 953 | loss: 0.50752 - acc: 0.8669 -- iter: 2000/8000\n",
            "Training Step: 10432  | total loss: \u001b[1m\u001b[32m0.50521\u001b[0m\u001b[0m | time: 1.569s\n",
            "| SGD | epoch: 953 | loss: 0.50521 - acc: 0.8677 -- iter: 3000/8000\n",
            "Training Step: 10433  | total loss: \u001b[1m\u001b[32m0.50539\u001b[0m\u001b[0m | time: 2.093s\n",
            "| SGD | epoch: 953 | loss: 0.50539 - acc: 0.8675 -- iter: 4000/8000\n",
            "Training Step: 10434  | total loss: \u001b[1m\u001b[32m0.50475\u001b[0m\u001b[0m | time: 2.618s\n",
            "| SGD | epoch: 953 | loss: 0.50475 - acc: 0.8677 -- iter: 5000/8000\n",
            "Training Step: 10435  | total loss: \u001b[1m\u001b[32m0.50394\u001b[0m\u001b[0m | time: 3.142s\n",
            "| SGD | epoch: 953 | loss: 0.50394 - acc: 0.8667 -- iter: 6000/8000\n",
            "Training Step: 10436  | total loss: \u001b[1m\u001b[32m0.50371\u001b[0m\u001b[0m | time: 3.664s\n",
            "| SGD | epoch: 953 | loss: 0.50371 - acc: 0.8681 -- iter: 7000/8000\n",
            "Training Step: 10437  | total loss: \u001b[1m\u001b[32m0.50486\u001b[0m\u001b[0m | time: 4.196s\n",
            "| SGD | epoch: 953 | loss: 0.50486 - acc: 0.8675 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10438  | total loss: \u001b[1m\u001b[32m0.50725\u001b[0m\u001b[0m | time: 0.522s\n",
            "| SGD | epoch: 954 | loss: 0.50725 - acc: 0.8659 -- iter: 1000/8000\n",
            "Training Step: 10439  | total loss: \u001b[1m\u001b[32m0.50761\u001b[0m\u001b[0m | time: 1.043s\n",
            "| SGD | epoch: 954 | loss: 0.50761 - acc: 0.8657 -- iter: 2000/8000\n",
            "Training Step: 10440  | total loss: \u001b[1m\u001b[32m0.50641\u001b[0m\u001b[0m | time: 1.569s\n",
            "| SGD | epoch: 954 | loss: 0.50641 - acc: 0.8666 -- iter: 3000/8000\n",
            "Training Step: 10441  | total loss: \u001b[1m\u001b[32m0.50179\u001b[0m\u001b[0m | time: 2.096s\n",
            "| SGD | epoch: 954 | loss: 0.50179 - acc: 0.8665 -- iter: 4000/8000\n",
            "Training Step: 10442  | total loss: \u001b[1m\u001b[32m0.50010\u001b[0m\u001b[0m | time: 2.615s\n",
            "| SGD | epoch: 954 | loss: 0.50010 - acc: 0.8674 -- iter: 5000/8000\n",
            "Training Step: 10443  | total loss: \u001b[1m\u001b[32m0.49819\u001b[0m\u001b[0m | time: 3.142s\n",
            "| SGD | epoch: 954 | loss: 0.49819 - acc: 0.8689 -- iter: 6000/8000\n",
            "Training Step: 10444  | total loss: \u001b[1m\u001b[32m0.49778\u001b[0m\u001b[0m | time: 3.650s\n",
            "| SGD | epoch: 954 | loss: 0.49778 - acc: 0.8686 -- iter: 7000/8000\n",
            "Training Step: 10445  | total loss: \u001b[1m\u001b[32m0.50108\u001b[0m\u001b[0m | time: 4.173s\n",
            "| SGD | epoch: 954 | loss: 0.50108 - acc: 0.8676 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10446  | total loss: \u001b[1m\u001b[32m0.49902\u001b[0m\u001b[0m | time: 0.512s\n",
            "| SGD | epoch: 955 | loss: 0.49902 - acc: 0.8684 -- iter: 1000/8000\n",
            "Training Step: 10447  | total loss: \u001b[1m\u001b[32m0.49996\u001b[0m\u001b[0m | time: 1.037s\n",
            "| SGD | epoch: 955 | loss: 0.49996 - acc: 0.8686 -- iter: 2000/8000\n",
            "Training Step: 10448  | total loss: \u001b[1m\u001b[32m0.49939\u001b[0m\u001b[0m | time: 1.550s\n",
            "| SGD | epoch: 955 | loss: 0.49939 - acc: 0.8695 -- iter: 3000/8000\n",
            "Training Step: 10449  | total loss: \u001b[1m\u001b[32m0.49964\u001b[0m\u001b[0m | time: 2.066s\n",
            "| SGD | epoch: 955 | loss: 0.49964 - acc: 0.8691 -- iter: 4000/8000\n",
            "Training Step: 10450  | total loss: \u001b[1m\u001b[32m0.50079\u001b[0m\u001b[0m | time: 2.582s\n",
            "| SGD | epoch: 955 | loss: 0.50079 - acc: 0.8690 -- iter: 5000/8000\n",
            "Training Step: 10451  | total loss: \u001b[1m\u001b[32m0.50166\u001b[0m\u001b[0m | time: 3.107s\n",
            "| SGD | epoch: 955 | loss: 0.50166 - acc: 0.8693 -- iter: 6000/8000\n",
            "Training Step: 10452  | total loss: \u001b[1m\u001b[32m0.50149\u001b[0m\u001b[0m | time: 3.623s\n",
            "| SGD | epoch: 955 | loss: 0.50149 - acc: 0.8707 -- iter: 7000/8000\n",
            "Training Step: 10453  | total loss: \u001b[1m\u001b[32m0.50267\u001b[0m\u001b[0m | time: 4.143s\n",
            "| SGD | epoch: 955 | loss: 0.50267 - acc: 0.8691 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10454  | total loss: \u001b[1m\u001b[32m0.50150\u001b[0m\u001b[0m | time: 0.517s\n",
            "| SGD | epoch: 956 | loss: 0.50150 - acc: 0.8685 -- iter: 1000/8000\n",
            "Training Step: 10455  | total loss: \u001b[1m\u001b[32m0.50599\u001b[0m\u001b[0m | time: 1.035s\n",
            "| SGD | epoch: 956 | loss: 0.50599 - acc: 0.8678 -- iter: 2000/8000\n",
            "Training Step: 10456  | total loss: \u001b[1m\u001b[32m0.50583\u001b[0m\u001b[0m | time: 1.553s\n",
            "| SGD | epoch: 956 | loss: 0.50583 - acc: 0.8667 -- iter: 3000/8000\n",
            "Training Step: 10457  | total loss: \u001b[1m\u001b[32m0.50338\u001b[0m\u001b[0m | time: 2.069s\n",
            "| SGD | epoch: 956 | loss: 0.50338 - acc: 0.8676 -- iter: 4000/8000\n",
            "Training Step: 10458  | total loss: \u001b[1m\u001b[32m0.50397\u001b[0m\u001b[0m | time: 2.592s\n",
            "| SGD | epoch: 956 | loss: 0.50397 - acc: 0.8677 -- iter: 5000/8000\n",
            "Training Step: 10459  | total loss: \u001b[1m\u001b[32m0.50356\u001b[0m\u001b[0m | time: 3.116s\n",
            "| SGD | epoch: 956 | loss: 0.50356 - acc: 0.8680 -- iter: 6000/8000\n",
            "Training Step: 10460  | total loss: \u001b[1m\u001b[32m0.50313\u001b[0m\u001b[0m | time: 3.647s\n",
            "| SGD | epoch: 956 | loss: 0.50313 - acc: 0.8692 -- iter: 7000/8000\n",
            "Training Step: 10461  | total loss: \u001b[1m\u001b[32m0.50106\u001b[0m\u001b[0m | time: 4.166s\n",
            "| SGD | epoch: 956 | loss: 0.50106 - acc: 0.8694 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10462  | total loss: \u001b[1m\u001b[32m0.49700\u001b[0m\u001b[0m | time: 0.527s\n",
            "| SGD | epoch: 957 | loss: 0.49700 - acc: 0.8702 -- iter: 1000/8000\n",
            "Training Step: 10463  | total loss: \u001b[1m\u001b[32m0.49814\u001b[0m\u001b[0m | time: 1.059s\n",
            "| SGD | epoch: 957 | loss: 0.49814 - acc: 0.8704 -- iter: 2000/8000\n",
            "Training Step: 10464  | total loss: \u001b[1m\u001b[32m0.49602\u001b[0m\u001b[0m | time: 1.576s\n",
            "| SGD | epoch: 957 | loss: 0.49602 - acc: 0.8714 -- iter: 3000/8000\n",
            "Training Step: 10465  | total loss: \u001b[1m\u001b[32m0.49932\u001b[0m\u001b[0m | time: 2.095s\n",
            "| SGD | epoch: 957 | loss: 0.49932 - acc: 0.8708 -- iter: 4000/8000\n",
            "Training Step: 10466  | total loss: \u001b[1m\u001b[32m0.50069\u001b[0m\u001b[0m | time: 2.618s\n",
            "| SGD | epoch: 957 | loss: 0.50069 - acc: 0.8700 -- iter: 5000/8000\n",
            "Training Step: 10467  | total loss: \u001b[1m\u001b[32m0.50031\u001b[0m\u001b[0m | time: 3.146s\n",
            "| SGD | epoch: 957 | loss: 0.50031 - acc: 0.8697 -- iter: 6000/8000\n",
            "Training Step: 10468  | total loss: \u001b[1m\u001b[32m0.49705\u001b[0m\u001b[0m | time: 3.659s\n",
            "| SGD | epoch: 957 | loss: 0.49705 - acc: 0.8710 -- iter: 7000/8000\n",
            "Training Step: 10469  | total loss: \u001b[1m\u001b[32m0.50029\u001b[0m\u001b[0m | time: 4.178s\n",
            "| SGD | epoch: 957 | loss: 0.50029 - acc: 0.8711 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10470  | total loss: \u001b[1m\u001b[32m0.50262\u001b[0m\u001b[0m | time: 0.521s\n",
            "| SGD | epoch: 958 | loss: 0.50262 - acc: 0.8723 -- iter: 1000/8000\n",
            "Training Step: 10471  | total loss: \u001b[1m\u001b[32m0.50450\u001b[0m\u001b[0m | time: 1.044s\n",
            "| SGD | epoch: 958 | loss: 0.50450 - acc: 0.8696 -- iter: 2000/8000\n",
            "Training Step: 10472  | total loss: \u001b[1m\u001b[32m0.50058\u001b[0m\u001b[0m | time: 1.567s\n",
            "| SGD | epoch: 958 | loss: 0.50058 - acc: 0.8707 -- iter: 3000/8000\n",
            "Training Step: 10473  | total loss: \u001b[1m\u001b[32m0.50253\u001b[0m\u001b[0m | time: 2.087s\n",
            "| SGD | epoch: 958 | loss: 0.50253 - acc: 0.8697 -- iter: 4000/8000\n",
            "Training Step: 10474  | total loss: \u001b[1m\u001b[32m0.50298\u001b[0m\u001b[0m | time: 2.609s\n",
            "| SGD | epoch: 958 | loss: 0.50298 - acc: 0.8695 -- iter: 5000/8000\n",
            "Training Step: 10475  | total loss: \u001b[1m\u001b[32m0.49814\u001b[0m\u001b[0m | time: 3.128s\n",
            "| SGD | epoch: 958 | loss: 0.49814 - acc: 0.8693 -- iter: 6000/8000\n",
            "Training Step: 10476  | total loss: \u001b[1m\u001b[32m0.50138\u001b[0m\u001b[0m | time: 3.644s\n",
            "| SGD | epoch: 958 | loss: 0.50138 - acc: 0.8680 -- iter: 7000/8000\n",
            "Training Step: 10477  | total loss: \u001b[1m\u001b[32m0.50212\u001b[0m\u001b[0m | time: 4.159s\n",
            "| SGD | epoch: 958 | loss: 0.50212 - acc: 0.8683 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10478  | total loss: \u001b[1m\u001b[32m0.50076\u001b[0m\u001b[0m | time: 0.515s\n",
            "| SGD | epoch: 959 | loss: 0.50076 - acc: 0.8694 -- iter: 1000/8000\n",
            "Training Step: 10479  | total loss: \u001b[1m\u001b[32m0.49914\u001b[0m\u001b[0m | time: 1.038s\n",
            "| SGD | epoch: 959 | loss: 0.49914 - acc: 0.8714 -- iter: 2000/8000\n",
            "Training Step: 10480  | total loss: \u001b[1m\u001b[32m0.49716\u001b[0m\u001b[0m | time: 1.545s\n",
            "| SGD | epoch: 959 | loss: 0.49716 - acc: 0.8715 -- iter: 3000/8000\n",
            "Training Step: 10481  | total loss: \u001b[1m\u001b[32m0.49792\u001b[0m\u001b[0m | time: 2.071s\n",
            "| SGD | epoch: 959 | loss: 0.49792 - acc: 0.8715 -- iter: 4000/8000\n",
            "Training Step: 10482  | total loss: \u001b[1m\u001b[32m0.49782\u001b[0m\u001b[0m | time: 2.585s\n",
            "| SGD | epoch: 959 | loss: 0.49782 - acc: 0.8716 -- iter: 5000/8000\n",
            "Training Step: 10483  | total loss: \u001b[1m\u001b[32m0.49511\u001b[0m\u001b[0m | time: 3.098s\n",
            "| SGD | epoch: 959 | loss: 0.49511 - acc: 0.8732 -- iter: 6000/8000\n",
            "Training Step: 10484  | total loss: \u001b[1m\u001b[32m0.49689\u001b[0m\u001b[0m | time: 3.613s\n",
            "| SGD | epoch: 959 | loss: 0.49689 - acc: 0.8717 -- iter: 7000/8000\n",
            "Training Step: 10485  | total loss: \u001b[1m\u001b[32m0.49615\u001b[0m\u001b[0m | time: 4.136s\n",
            "| SGD | epoch: 959 | loss: 0.49615 - acc: 0.8732 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10486  | total loss: \u001b[1m\u001b[32m0.49610\u001b[0m\u001b[0m | time: 0.525s\n",
            "| SGD | epoch: 960 | loss: 0.49610 - acc: 0.8735 -- iter: 1000/8000\n",
            "Training Step: 10487  | total loss: \u001b[1m\u001b[32m0.49826\u001b[0m\u001b[0m | time: 1.044s\n",
            "| SGD | epoch: 960 | loss: 0.49826 - acc: 0.8721 -- iter: 2000/8000\n",
            "Training Step: 10488  | total loss: \u001b[1m\u001b[32m0.49884\u001b[0m\u001b[0m | time: 1.565s\n",
            "| SGD | epoch: 960 | loss: 0.49884 - acc: 0.8720 -- iter: 3000/8000\n",
            "Training Step: 10489  | total loss: \u001b[1m\u001b[32m0.49898\u001b[0m\u001b[0m | time: 2.091s\n",
            "| SGD | epoch: 960 | loss: 0.49898 - acc: 0.8710 -- iter: 4000/8000\n",
            "Training Step: 10490  | total loss: \u001b[1m\u001b[32m0.49905\u001b[0m\u001b[0m | time: 2.615s\n",
            "| SGD | epoch: 960 | loss: 0.49905 - acc: 0.8699 -- iter: 5000/8000\n",
            "Training Step: 10491  | total loss: \u001b[1m\u001b[32m0.49896\u001b[0m\u001b[0m | time: 3.135s\n",
            "| SGD | epoch: 960 | loss: 0.49896 - acc: 0.8693 -- iter: 6000/8000\n",
            "Training Step: 10492  | total loss: \u001b[1m\u001b[32m0.49705\u001b[0m\u001b[0m | time: 3.657s\n",
            "| SGD | epoch: 960 | loss: 0.49705 - acc: 0.8703 -- iter: 7000/8000\n",
            "Training Step: 10493  | total loss: \u001b[1m\u001b[32m0.49656\u001b[0m\u001b[0m | time: 4.189s\n",
            "| SGD | epoch: 960 | loss: 0.49656 - acc: 0.8724 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10494  | total loss: \u001b[1m\u001b[32m0.49592\u001b[0m\u001b[0m | time: 0.512s\n",
            "| SGD | epoch: 961 | loss: 0.49592 - acc: 0.8720 -- iter: 1000/8000\n",
            "Training Step: 10495  | total loss: \u001b[1m\u001b[32m0.49651\u001b[0m\u001b[0m | time: 1.030s\n",
            "| SGD | epoch: 961 | loss: 0.49651 - acc: 0.8713 -- iter: 2000/8000\n",
            "Training Step: 10496  | total loss: \u001b[1m\u001b[32m0.49658\u001b[0m\u001b[0m | time: 1.549s\n",
            "| SGD | epoch: 961 | loss: 0.49658 - acc: 0.8710 -- iter: 3000/8000\n",
            "Training Step: 10497  | total loss: \u001b[1m\u001b[32m0.49539\u001b[0m\u001b[0m | time: 2.066s\n",
            "| SGD | epoch: 961 | loss: 0.49539 - acc: 0.8713 -- iter: 4000/8000\n",
            "Training Step: 10498  | total loss: \u001b[1m\u001b[32m0.49875\u001b[0m\u001b[0m | time: 2.584s\n",
            "| SGD | epoch: 961 | loss: 0.49875 - acc: 0.8706 -- iter: 5000/8000\n",
            "Training Step: 10499  | total loss: \u001b[1m\u001b[32m0.49971\u001b[0m\u001b[0m | time: 3.099s\n",
            "| SGD | epoch: 961 | loss: 0.49971 - acc: 0.8709 -- iter: 6000/8000\n",
            "Training Step: 10500  | total loss: \u001b[1m\u001b[32m0.50430\u001b[0m\u001b[0m | time: 3.624s\n",
            "| SGD | epoch: 961 | loss: 0.50430 - acc: 0.8692 -- iter: 7000/8000\n",
            "Training Step: 10501  | total loss: \u001b[1m\u001b[32m0.50306\u001b[0m\u001b[0m | time: 4.140s\n",
            "| SGD | epoch: 961 | loss: 0.50306 - acc: 0.8696 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10502  | total loss: \u001b[1m\u001b[32m0.50158\u001b[0m\u001b[0m | time: 0.516s\n",
            "| SGD | epoch: 962 | loss: 0.50158 - acc: 0.8693 -- iter: 1000/8000\n",
            "Training Step: 10503  | total loss: \u001b[1m\u001b[32m0.50209\u001b[0m\u001b[0m | time: 1.032s\n",
            "| SGD | epoch: 962 | loss: 0.50209 - acc: 0.8692 -- iter: 2000/8000\n",
            "Training Step: 10504  | total loss: \u001b[1m\u001b[32m0.50108\u001b[0m\u001b[0m | time: 1.547s\n",
            "| SGD | epoch: 962 | loss: 0.50108 - acc: 0.8688 -- iter: 3000/8000\n",
            "Training Step: 10505  | total loss: \u001b[1m\u001b[32m0.49932\u001b[0m\u001b[0m | time: 2.067s\n",
            "| SGD | epoch: 962 | loss: 0.49932 - acc: 0.8688 -- iter: 4000/8000\n",
            "Training Step: 10506  | total loss: \u001b[1m\u001b[32m0.49802\u001b[0m\u001b[0m | time: 2.582s\n",
            "| SGD | epoch: 962 | loss: 0.49802 - acc: 0.8678 -- iter: 5000/8000\n",
            "Training Step: 10507  | total loss: \u001b[1m\u001b[32m0.49274\u001b[0m\u001b[0m | time: 3.097s\n",
            "| SGD | epoch: 962 | loss: 0.49274 - acc: 0.8716 -- iter: 6000/8000\n",
            "Training Step: 10508  | total loss: \u001b[1m\u001b[32m0.49435\u001b[0m\u001b[0m | time: 3.620s\n",
            "| SGD | epoch: 962 | loss: 0.49435 - acc: 0.8702 -- iter: 7000/8000\n",
            "Training Step: 10509  | total loss: \u001b[1m\u001b[32m0.49272\u001b[0m\u001b[0m | time: 4.129s\n",
            "| SGD | epoch: 962 | loss: 0.49272 - acc: 0.8716 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10510  | total loss: \u001b[1m\u001b[32m0.49528\u001b[0m\u001b[0m | time: 0.521s\n",
            "| SGD | epoch: 963 | loss: 0.49528 - acc: 0.8709 -- iter: 1000/8000\n",
            "Training Step: 10511  | total loss: \u001b[1m\u001b[32m0.49572\u001b[0m\u001b[0m | time: 1.040s\n",
            "| SGD | epoch: 963 | loss: 0.49572 - acc: 0.8711 -- iter: 2000/8000\n",
            "Training Step: 10512  | total loss: \u001b[1m\u001b[32m0.49527\u001b[0m\u001b[0m | time: 1.579s\n",
            "| SGD | epoch: 963 | loss: 0.49527 - acc: 0.8713 -- iter: 3000/8000\n",
            "Training Step: 10513  | total loss: \u001b[1m\u001b[32m0.49692\u001b[0m\u001b[0m | time: 2.094s\n",
            "| SGD | epoch: 963 | loss: 0.49692 - acc: 0.8702 -- iter: 4000/8000\n",
            "Training Step: 10514  | total loss: \u001b[1m\u001b[32m0.49500\u001b[0m\u001b[0m | time: 2.622s\n",
            "| SGD | epoch: 963 | loss: 0.49500 - acc: 0.8707 -- iter: 5000/8000\n",
            "Training Step: 10515  | total loss: \u001b[1m\u001b[32m0.49310\u001b[0m\u001b[0m | time: 3.145s\n",
            "| SGD | epoch: 963 | loss: 0.49310 - acc: 0.8716 -- iter: 6000/8000\n",
            "Training Step: 10516  | total loss: \u001b[1m\u001b[32m0.49492\u001b[0m\u001b[0m | time: 3.672s\n",
            "| SGD | epoch: 963 | loss: 0.49492 - acc: 0.8701 -- iter: 7000/8000\n",
            "Training Step: 10517  | total loss: \u001b[1m\u001b[32m0.49388\u001b[0m\u001b[0m | time: 4.196s\n",
            "| SGD | epoch: 963 | loss: 0.49388 - acc: 0.8702 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10518  | total loss: \u001b[1m\u001b[32m0.49191\u001b[0m\u001b[0m | time: 0.526s\n",
            "| SGD | epoch: 964 | loss: 0.49191 - acc: 0.8708 -- iter: 1000/8000\n",
            "Training Step: 10519  | total loss: \u001b[1m\u001b[32m0.49144\u001b[0m\u001b[0m | time: 1.049s\n",
            "| SGD | epoch: 964 | loss: 0.49144 - acc: 0.8723 -- iter: 2000/8000\n",
            "Training Step: 10520  | total loss: \u001b[1m\u001b[32m0.49120\u001b[0m\u001b[0m | time: 1.581s\n",
            "| SGD | epoch: 964 | loss: 0.49120 - acc: 0.8717 -- iter: 3000/8000\n",
            "Training Step: 10521  | total loss: \u001b[1m\u001b[32m0.49143\u001b[0m\u001b[0m | time: 2.094s\n",
            "| SGD | epoch: 964 | loss: 0.49143 - acc: 0.8723 -- iter: 4000/8000\n",
            "Training Step: 10522  | total loss: \u001b[1m\u001b[32m0.49333\u001b[0m\u001b[0m | time: 2.609s\n",
            "| SGD | epoch: 964 | loss: 0.49333 - acc: 0.8727 -- iter: 5000/8000\n",
            "Training Step: 10523  | total loss: \u001b[1m\u001b[32m0.49466\u001b[0m\u001b[0m | time: 3.127s\n",
            "| SGD | epoch: 964 | loss: 0.49466 - acc: 0.8721 -- iter: 6000/8000\n",
            "Training Step: 10524  | total loss: \u001b[1m\u001b[32m0.49610\u001b[0m\u001b[0m | time: 3.651s\n",
            "| SGD | epoch: 964 | loss: 0.49610 - acc: 0.8713 -- iter: 7000/8000\n",
            "Training Step: 10525  | total loss: \u001b[1m\u001b[32m0.49538\u001b[0m\u001b[0m | time: 4.176s\n",
            "| SGD | epoch: 964 | loss: 0.49538 - acc: 0.8703 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10526  | total loss: \u001b[1m\u001b[32m0.49816\u001b[0m\u001b[0m | time: 0.523s\n",
            "| SGD | epoch: 965 | loss: 0.49816 - acc: 0.8695 -- iter: 1000/8000\n",
            "Training Step: 10527  | total loss: \u001b[1m\u001b[32m0.49433\u001b[0m\u001b[0m | time: 1.038s\n",
            "| SGD | epoch: 965 | loss: 0.49433 - acc: 0.8716 -- iter: 2000/8000\n",
            "Training Step: 10528  | total loss: \u001b[1m\u001b[32m0.49291\u001b[0m\u001b[0m | time: 1.553s\n",
            "| SGD | epoch: 965 | loss: 0.49291 - acc: 0.8709 -- iter: 3000/8000\n",
            "Training Step: 10529  | total loss: \u001b[1m\u001b[32m0.48953\u001b[0m\u001b[0m | time: 2.070s\n",
            "| SGD | epoch: 965 | loss: 0.48953 - acc: 0.8731 -- iter: 4000/8000\n",
            "Training Step: 10530  | total loss: \u001b[1m\u001b[32m0.49218\u001b[0m\u001b[0m | time: 2.588s\n",
            "| SGD | epoch: 965 | loss: 0.49218 - acc: 0.8717 -- iter: 5000/8000\n",
            "Training Step: 10531  | total loss: \u001b[1m\u001b[32m0.49304\u001b[0m\u001b[0m | time: 3.108s\n",
            "| SGD | epoch: 965 | loss: 0.49304 - acc: 0.8736 -- iter: 6000/8000\n",
            "Training Step: 10532  | total loss: \u001b[1m\u001b[32m0.49390\u001b[0m\u001b[0m | time: 3.617s\n",
            "| SGD | epoch: 965 | loss: 0.49390 - acc: 0.8735 -- iter: 7000/8000\n",
            "Training Step: 10533  | total loss: \u001b[1m\u001b[32m0.49255\u001b[0m\u001b[0m | time: 4.136s\n",
            "| SGD | epoch: 965 | loss: 0.49255 - acc: 0.8737 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10534  | total loss: \u001b[1m\u001b[32m0.49707\u001b[0m\u001b[0m | time: 0.511s\n",
            "| SGD | epoch: 966 | loss: 0.49707 - acc: 0.8705 -- iter: 1000/8000\n",
            "Training Step: 10535  | total loss: \u001b[1m\u001b[32m0.49821\u001b[0m\u001b[0m | time: 1.021s\n",
            "| SGD | epoch: 966 | loss: 0.49821 - acc: 0.8700 -- iter: 2000/8000\n",
            "Training Step: 10536  | total loss: \u001b[1m\u001b[32m0.49946\u001b[0m\u001b[0m | time: 1.549s\n",
            "| SGD | epoch: 966 | loss: 0.49946 - acc: 0.8707 -- iter: 3000/8000\n",
            "Training Step: 10537  | total loss: \u001b[1m\u001b[32m0.49725\u001b[0m\u001b[0m | time: 2.067s\n",
            "| SGD | epoch: 966 | loss: 0.49725 - acc: 0.8713 -- iter: 4000/8000\n",
            "Training Step: 10538  | total loss: \u001b[1m\u001b[32m0.49582\u001b[0m\u001b[0m | time: 2.592s\n",
            "| SGD | epoch: 966 | loss: 0.49582 - acc: 0.8719 -- iter: 5000/8000\n",
            "Training Step: 10539  | total loss: \u001b[1m\u001b[32m0.49796\u001b[0m\u001b[0m | time: 3.108s\n",
            "| SGD | epoch: 966 | loss: 0.49796 - acc: 0.8711 -- iter: 6000/8000\n",
            "Training Step: 10540  | total loss: \u001b[1m\u001b[32m0.49741\u001b[0m\u001b[0m | time: 3.632s\n",
            "| SGD | epoch: 966 | loss: 0.49741 - acc: 0.8709 -- iter: 7000/8000\n",
            "Training Step: 10541  | total loss: \u001b[1m\u001b[32m0.49689\u001b[0m\u001b[0m | time: 4.156s\n",
            "| SGD | epoch: 966 | loss: 0.49689 - acc: 0.8704 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10542  | total loss: \u001b[1m\u001b[32m0.49695\u001b[0m\u001b[0m | time: 0.526s\n",
            "| SGD | epoch: 967 | loss: 0.49695 - acc: 0.8690 -- iter: 1000/8000\n",
            "Training Step: 10543  | total loss: \u001b[1m\u001b[32m0.49562\u001b[0m\u001b[0m | time: 1.048s\n",
            "| SGD | epoch: 967 | loss: 0.49562 - acc: 0.8696 -- iter: 2000/8000\n",
            "Training Step: 10544  | total loss: \u001b[1m\u001b[32m0.49607\u001b[0m\u001b[0m | time: 1.571s\n",
            "| SGD | epoch: 967 | loss: 0.49607 - acc: 0.8688 -- iter: 3000/8000\n",
            "Training Step: 10545  | total loss: \u001b[1m\u001b[32m0.49510\u001b[0m\u001b[0m | time: 2.099s\n",
            "| SGD | epoch: 967 | loss: 0.49510 - acc: 0.8698 -- iter: 4000/8000\n",
            "Training Step: 10546  | total loss: \u001b[1m\u001b[32m0.49677\u001b[0m\u001b[0m | time: 2.610s\n",
            "| SGD | epoch: 967 | loss: 0.49677 - acc: 0.8691 -- iter: 5000/8000\n",
            "Training Step: 10547  | total loss: \u001b[1m\u001b[32m0.49294\u001b[0m\u001b[0m | time: 3.119s\n",
            "| SGD | epoch: 967 | loss: 0.49294 - acc: 0.8715 -- iter: 6000/8000\n",
            "Training Step: 10548  | total loss: \u001b[1m\u001b[32m0.49180\u001b[0m\u001b[0m | time: 3.634s\n",
            "| SGD | epoch: 967 | loss: 0.49180 - acc: 0.8721 -- iter: 7000/8000\n",
            "Training Step: 10549  | total loss: \u001b[1m\u001b[32m0.49297\u001b[0m\u001b[0m | time: 4.156s\n",
            "| SGD | epoch: 967 | loss: 0.49297 - acc: 0.8712 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10550  | total loss: \u001b[1m\u001b[32m0.49394\u001b[0m\u001b[0m | time: 0.514s\n",
            "| SGD | epoch: 968 | loss: 0.49394 - acc: 0.8712 -- iter: 1000/8000\n",
            "Training Step: 10551  | total loss: \u001b[1m\u001b[32m0.49491\u001b[0m\u001b[0m | time: 1.034s\n",
            "| SGD | epoch: 968 | loss: 0.49491 - acc: 0.8707 -- iter: 2000/8000\n",
            "Training Step: 10552  | total loss: \u001b[1m\u001b[32m0.49722\u001b[0m\u001b[0m | time: 1.552s\n",
            "| SGD | epoch: 968 | loss: 0.49722 - acc: 0.8707 -- iter: 3000/8000\n",
            "Training Step: 10553  | total loss: \u001b[1m\u001b[32m0.49582\u001b[0m\u001b[0m | time: 2.065s\n",
            "| SGD | epoch: 968 | loss: 0.49582 - acc: 0.8700 -- iter: 4000/8000\n",
            "Training Step: 10554  | total loss: \u001b[1m\u001b[32m0.49934\u001b[0m\u001b[0m | time: 2.582s\n",
            "| SGD | epoch: 968 | loss: 0.49934 - acc: 0.8686 -- iter: 5000/8000\n",
            "Training Step: 10555  | total loss: \u001b[1m\u001b[32m0.49551\u001b[0m\u001b[0m | time: 3.102s\n",
            "| SGD | epoch: 968 | loss: 0.49551 - acc: 0.8715 -- iter: 6000/8000\n",
            "Training Step: 10556  | total loss: \u001b[1m\u001b[32m0.49311\u001b[0m\u001b[0m | time: 3.633s\n",
            "| SGD | epoch: 968 | loss: 0.49311 - acc: 0.8718 -- iter: 7000/8000\n",
            "Training Step: 10557  | total loss: \u001b[1m\u001b[32m0.49505\u001b[0m\u001b[0m | time: 4.143s\n",
            "| SGD | epoch: 968 | loss: 0.49505 - acc: 0.8719 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10558  | total loss: \u001b[1m\u001b[32m0.49257\u001b[0m\u001b[0m | time: 0.586s\n",
            "| SGD | epoch: 969 | loss: 0.49257 - acc: 0.8734 -- iter: 1000/8000\n",
            "Training Step: 10559  | total loss: \u001b[1m\u001b[32m0.49332\u001b[0m\u001b[0m | time: 1.095s\n",
            "| SGD | epoch: 969 | loss: 0.49332 - acc: 0.8720 -- iter: 2000/8000\n",
            "Training Step: 10560  | total loss: \u001b[1m\u001b[32m0.49326\u001b[0m\u001b[0m | time: 1.604s\n",
            "| SGD | epoch: 969 | loss: 0.49326 - acc: 0.8720 -- iter: 3000/8000\n",
            "Training Step: 10561  | total loss: \u001b[1m\u001b[32m0.49518\u001b[0m\u001b[0m | time: 2.108s\n",
            "| SGD | epoch: 969 | loss: 0.49518 - acc: 0.8710 -- iter: 4000/8000\n",
            "Training Step: 10562  | total loss: \u001b[1m\u001b[32m0.49501\u001b[0m\u001b[0m | time: 2.626s\n",
            "| SGD | epoch: 969 | loss: 0.49501 - acc: 0.8698 -- iter: 5000/8000\n",
            "Training Step: 10563  | total loss: \u001b[1m\u001b[32m0.49669\u001b[0m\u001b[0m | time: 3.147s\n",
            "| SGD | epoch: 969 | loss: 0.49669 - acc: 0.8693 -- iter: 6000/8000\n",
            "Training Step: 10564  | total loss: \u001b[1m\u001b[32m0.49656\u001b[0m\u001b[0m | time: 3.670s\n",
            "| SGD | epoch: 969 | loss: 0.49656 - acc: 0.8696 -- iter: 7000/8000\n",
            "Training Step: 10565  | total loss: \u001b[1m\u001b[32m0.49576\u001b[0m\u001b[0m | time: 4.197s\n",
            "| SGD | epoch: 969 | loss: 0.49576 - acc: 0.8695 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10566  | total loss: \u001b[1m\u001b[32m0.49520\u001b[0m\u001b[0m | time: 0.539s\n",
            "| SGD | epoch: 970 | loss: 0.49520 - acc: 0.8692 -- iter: 1000/8000\n",
            "Training Step: 10567  | total loss: \u001b[1m\u001b[32m0.49597\u001b[0m\u001b[0m | time: 1.185s\n",
            "| SGD | epoch: 970 | loss: 0.49597 - acc: 0.8689 -- iter: 2000/8000\n",
            "Training Step: 10568  | total loss: \u001b[1m\u001b[32m0.49413\u001b[0m\u001b[0m | time: 1.831s\n",
            "| SGD | epoch: 970 | loss: 0.49413 - acc: 0.8692 -- iter: 3000/8000\n",
            "Training Step: 10569  | total loss: \u001b[1m\u001b[32m0.49240\u001b[0m\u001b[0m | time: 2.475s\n",
            "| SGD | epoch: 970 | loss: 0.49240 - acc: 0.8701 -- iter: 4000/8000\n",
            "Training Step: 10570  | total loss: \u001b[1m\u001b[32m0.49439\u001b[0m\u001b[0m | time: 3.120s\n",
            "| SGD | epoch: 970 | loss: 0.49439 - acc: 0.8703 -- iter: 5000/8000\n",
            "Training Step: 10571  | total loss: \u001b[1m\u001b[32m0.49168\u001b[0m\u001b[0m | time: 3.761s\n",
            "| SGD | epoch: 970 | loss: 0.49168 - acc: 0.8724 -- iter: 6000/8000\n",
            "Training Step: 10572  | total loss: \u001b[1m\u001b[32m0.48856\u001b[0m\u001b[0m | time: 4.393s\n",
            "| SGD | epoch: 970 | loss: 0.48856 - acc: 0.8742 -- iter: 7000/8000\n",
            "Training Step: 10573  | total loss: \u001b[1m\u001b[32m0.48638\u001b[0m\u001b[0m | time: 5.028s\n",
            "| SGD | epoch: 970 | loss: 0.48638 - acc: 0.8751 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10574  | total loss: \u001b[1m\u001b[32m0.48829\u001b[0m\u001b[0m | time: 0.636s\n",
            "| SGD | epoch: 971 | loss: 0.48829 - acc: 0.8746 -- iter: 1000/8000\n",
            "Training Step: 10575  | total loss: \u001b[1m\u001b[32m0.48581\u001b[0m\u001b[0m | time: 1.271s\n",
            "| SGD | epoch: 971 | loss: 0.48581 - acc: 0.8764 -- iter: 2000/8000\n",
            "Training Step: 10576  | total loss: \u001b[1m\u001b[32m0.48806\u001b[0m\u001b[0m | time: 1.907s\n",
            "| SGD | epoch: 971 | loss: 0.48806 - acc: 0.8755 -- iter: 3000/8000\n",
            "Training Step: 10577  | total loss: \u001b[1m\u001b[32m0.48998\u001b[0m\u001b[0m | time: 2.546s\n",
            "| SGD | epoch: 971 | loss: 0.48998 - acc: 0.8750 -- iter: 4000/8000\n",
            "Training Step: 10578  | total loss: \u001b[1m\u001b[32m0.49204\u001b[0m\u001b[0m | time: 3.161s\n",
            "| SGD | epoch: 971 | loss: 0.49204 - acc: 0.8746 -- iter: 5000/8000\n",
            "Training Step: 10579  | total loss: \u001b[1m\u001b[32m0.49395\u001b[0m\u001b[0m | time: 3.776s\n",
            "| SGD | epoch: 971 | loss: 0.49395 - acc: 0.8724 -- iter: 6000/8000\n",
            "Training Step: 10580  | total loss: \u001b[1m\u001b[32m0.49515\u001b[0m\u001b[0m | time: 4.396s\n",
            "| SGD | epoch: 971 | loss: 0.49515 - acc: 0.8722 -- iter: 7000/8000\n",
            "Training Step: 10581  | total loss: \u001b[1m\u001b[32m0.49510\u001b[0m\u001b[0m | time: 5.009s\n",
            "| SGD | epoch: 971 | loss: 0.49510 - acc: 0.8727 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10582  | total loss: \u001b[1m\u001b[32m0.49620\u001b[0m\u001b[0m | time: 0.619s\n",
            "| SGD | epoch: 972 | loss: 0.49620 - acc: 0.8721 -- iter: 1000/8000\n",
            "Training Step: 10583  | total loss: \u001b[1m\u001b[32m0.49326\u001b[0m\u001b[0m | time: 1.251s\n",
            "| SGD | epoch: 972 | loss: 0.49326 - acc: 0.8722 -- iter: 2000/8000\n",
            "Training Step: 10584  | total loss: \u001b[1m\u001b[32m0.49293\u001b[0m\u001b[0m | time: 1.888s\n",
            "| SGD | epoch: 972 | loss: 0.49293 - acc: 0.8722 -- iter: 3000/8000\n",
            "Training Step: 10585  | total loss: \u001b[1m\u001b[32m0.49413\u001b[0m\u001b[0m | time: 2.521s\n",
            "| SGD | epoch: 972 | loss: 0.49413 - acc: 0.8718 -- iter: 4000/8000\n",
            "Training Step: 10586  | total loss: \u001b[1m\u001b[32m0.49421\u001b[0m\u001b[0m | time: 3.159s\n",
            "| SGD | epoch: 972 | loss: 0.49421 - acc: 0.8732 -- iter: 5000/8000\n",
            "Training Step: 10587  | total loss: \u001b[1m\u001b[32m0.49372\u001b[0m\u001b[0m | time: 3.802s\n",
            "| SGD | epoch: 972 | loss: 0.49372 - acc: 0.8736 -- iter: 6000/8000\n",
            "Training Step: 10588  | total loss: \u001b[1m\u001b[32m0.49541\u001b[0m\u001b[0m | time: 4.325s\n",
            "| SGD | epoch: 972 | loss: 0.49541 - acc: 0.8725 -- iter: 7000/8000\n",
            "Training Step: 10589  | total loss: \u001b[1m\u001b[32m0.49310\u001b[0m\u001b[0m | time: 4.850s\n",
            "| SGD | epoch: 972 | loss: 0.49310 - acc: 0.8719 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10590  | total loss: \u001b[1m\u001b[32m0.49688\u001b[0m\u001b[0m | time: 0.523s\n",
            "| SGD | epoch: 973 | loss: 0.49688 - acc: 0.8704 -- iter: 1000/8000\n",
            "Training Step: 10591  | total loss: \u001b[1m\u001b[32m0.49315\u001b[0m\u001b[0m | time: 1.040s\n",
            "| SGD | epoch: 973 | loss: 0.49315 - acc: 0.8721 -- iter: 2000/8000\n",
            "Training Step: 10592  | total loss: \u001b[1m\u001b[32m0.49622\u001b[0m\u001b[0m | time: 1.553s\n",
            "| SGD | epoch: 973 | loss: 0.49622 - acc: 0.8719 -- iter: 3000/8000\n",
            "Training Step: 10593  | total loss: \u001b[1m\u001b[32m0.49301\u001b[0m\u001b[0m | time: 2.079s\n",
            "| SGD | epoch: 973 | loss: 0.49301 - acc: 0.8725 -- iter: 4000/8000\n",
            "Training Step: 10594  | total loss: \u001b[1m\u001b[32m0.49182\u001b[0m\u001b[0m | time: 2.595s\n",
            "| SGD | epoch: 973 | loss: 0.49182 - acc: 0.8730 -- iter: 5000/8000\n",
            "Training Step: 10595  | total loss: \u001b[1m\u001b[32m0.49480\u001b[0m\u001b[0m | time: 3.140s\n",
            "| SGD | epoch: 973 | loss: 0.49480 - acc: 0.8707 -- iter: 6000/8000\n",
            "Training Step: 10596  | total loss: \u001b[1m\u001b[32m0.49652\u001b[0m\u001b[0m | time: 3.686s\n",
            "| SGD | epoch: 973 | loss: 0.49652 - acc: 0.8706 -- iter: 7000/8000\n",
            "Training Step: 10597  | total loss: \u001b[1m\u001b[32m0.49591\u001b[0m\u001b[0m | time: 4.231s\n",
            "| SGD | epoch: 973 | loss: 0.49591 - acc: 0.8706 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10598  | total loss: \u001b[1m\u001b[32m0.49254\u001b[0m\u001b[0m | time: 0.549s\n",
            "| SGD | epoch: 974 | loss: 0.49254 - acc: 0.8722 -- iter: 1000/8000\n",
            "Training Step: 10599  | total loss: \u001b[1m\u001b[32m0.49334\u001b[0m\u001b[0m | time: 1.094s\n",
            "| SGD | epoch: 974 | loss: 0.49334 - acc: 0.8720 -- iter: 2000/8000\n",
            "Training Step: 10600  | total loss: \u001b[1m\u001b[32m0.49260\u001b[0m\u001b[0m | time: 1.649s\n",
            "| SGD | epoch: 974 | loss: 0.49260 - acc: 0.8726 -- iter: 3000/8000\n",
            "Training Step: 10601  | total loss: \u001b[1m\u001b[32m0.49239\u001b[0m\u001b[0m | time: 2.193s\n",
            "| SGD | epoch: 974 | loss: 0.49239 - acc: 0.8717 -- iter: 4000/8000\n",
            "Training Step: 10602  | total loss: \u001b[1m\u001b[32m0.49278\u001b[0m\u001b[0m | time: 2.749s\n",
            "| SGD | epoch: 974 | loss: 0.49278 - acc: 0.8699 -- iter: 5000/8000\n",
            "Training Step: 10603  | total loss: \u001b[1m\u001b[32m0.49022\u001b[0m\u001b[0m | time: 3.310s\n",
            "| SGD | epoch: 974 | loss: 0.49022 - acc: 0.8711 -- iter: 6000/8000\n",
            "Training Step: 10604  | total loss: \u001b[1m\u001b[32m0.49187\u001b[0m\u001b[0m | time: 3.883s\n",
            "| SGD | epoch: 974 | loss: 0.49187 - acc: 0.8698 -- iter: 7000/8000\n",
            "Training Step: 10605  | total loss: \u001b[1m\u001b[32m0.49177\u001b[0m\u001b[0m | time: 4.447s\n",
            "| SGD | epoch: 974 | loss: 0.49177 - acc: 0.8711 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10606  | total loss: \u001b[1m\u001b[32m0.48999\u001b[0m\u001b[0m | time: 0.573s\n",
            "| SGD | epoch: 975 | loss: 0.48999 - acc: 0.8733 -- iter: 1000/8000\n",
            "Training Step: 10607  | total loss: \u001b[1m\u001b[32m0.49259\u001b[0m\u001b[0m | time: 1.137s\n",
            "| SGD | epoch: 975 | loss: 0.49259 - acc: 0.8737 -- iter: 2000/8000\n",
            "Training Step: 10608  | total loss: \u001b[1m\u001b[32m0.49222\u001b[0m\u001b[0m | time: 1.708s\n",
            "| SGD | epoch: 975 | loss: 0.49222 - acc: 0.8741 -- iter: 3000/8000\n",
            "Training Step: 10609  | total loss: \u001b[1m\u001b[32m0.49091\u001b[0m\u001b[0m | time: 2.279s\n",
            "| SGD | epoch: 975 | loss: 0.49091 - acc: 0.8744 -- iter: 4000/8000\n",
            "Training Step: 10610  | total loss: \u001b[1m\u001b[32m0.49152\u001b[0m\u001b[0m | time: 2.848s\n",
            "| SGD | epoch: 975 | loss: 0.49152 - acc: 0.8750 -- iter: 5000/8000\n",
            "Training Step: 10611  | total loss: \u001b[1m\u001b[32m0.49282\u001b[0m\u001b[0m | time: 3.434s\n",
            "| SGD | epoch: 975 | loss: 0.49282 - acc: 0.8738 -- iter: 6000/8000\n",
            "Training Step: 10612  | total loss: \u001b[1m\u001b[32m0.48993\u001b[0m\u001b[0m | time: 4.009s\n",
            "| SGD | epoch: 975 | loss: 0.48993 - acc: 0.8747 -- iter: 7000/8000\n",
            "Training Step: 10613  | total loss: \u001b[1m\u001b[32m0.48916\u001b[0m\u001b[0m | time: 4.588s\n",
            "| SGD | epoch: 975 | loss: 0.48916 - acc: 0.8740 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10614  | total loss: \u001b[1m\u001b[32m0.48868\u001b[0m\u001b[0m | time: 0.573s\n",
            "| SGD | epoch: 976 | loss: 0.48868 - acc: 0.8725 -- iter: 1000/8000\n",
            "Training Step: 10615  | total loss: \u001b[1m\u001b[32m0.49120\u001b[0m\u001b[0m | time: 1.156s\n",
            "| SGD | epoch: 976 | loss: 0.49120 - acc: 0.8727 -- iter: 2000/8000\n",
            "Training Step: 10616  | total loss: \u001b[1m\u001b[32m0.49089\u001b[0m\u001b[0m | time: 1.728s\n",
            "| SGD | epoch: 976 | loss: 0.49089 - acc: 0.8729 -- iter: 3000/8000\n",
            "Training Step: 10617  | total loss: \u001b[1m\u001b[32m0.48894\u001b[0m\u001b[0m | time: 2.301s\n",
            "| SGD | epoch: 976 | loss: 0.48894 - acc: 0.8740 -- iter: 4000/8000\n",
            "Training Step: 10618  | total loss: \u001b[1m\u001b[32m0.48786\u001b[0m\u001b[0m | time: 2.865s\n",
            "| SGD | epoch: 976 | loss: 0.48786 - acc: 0.8728 -- iter: 5000/8000\n",
            "Training Step: 10619  | total loss: \u001b[1m\u001b[32m0.48821\u001b[0m\u001b[0m | time: 3.425s\n",
            "| SGD | epoch: 976 | loss: 0.48821 - acc: 0.8718 -- iter: 6000/8000\n",
            "Training Step: 10620  | total loss: \u001b[1m\u001b[32m0.48668\u001b[0m\u001b[0m | time: 4.004s\n",
            "| SGD | epoch: 976 | loss: 0.48668 - acc: 0.8742 -- iter: 7000/8000\n",
            "Training Step: 10621  | total loss: \u001b[1m\u001b[32m0.49150\u001b[0m\u001b[0m | time: 4.569s\n",
            "| SGD | epoch: 976 | loss: 0.49150 - acc: 0.8725 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10622  | total loss: \u001b[1m\u001b[32m0.49185\u001b[0m\u001b[0m | time: 0.575s\n",
            "| SGD | epoch: 977 | loss: 0.49185 - acc: 0.8718 -- iter: 1000/8000\n",
            "Training Step: 10623  | total loss: \u001b[1m\u001b[32m0.49176\u001b[0m\u001b[0m | time: 1.146s\n",
            "| SGD | epoch: 977 | loss: 0.49176 - acc: 0.8717 -- iter: 2000/8000\n",
            "Training Step: 10624  | total loss: \u001b[1m\u001b[32m0.49162\u001b[0m\u001b[0m | time: 1.716s\n",
            "| SGD | epoch: 977 | loss: 0.49162 - acc: 0.8720 -- iter: 3000/8000\n",
            "Training Step: 10625  | total loss: \u001b[1m\u001b[32m0.49138\u001b[0m\u001b[0m | time: 2.299s\n",
            "| SGD | epoch: 977 | loss: 0.49138 - acc: 0.8719 -- iter: 4000/8000\n",
            "Training Step: 10626  | total loss: \u001b[1m\u001b[32m0.49048\u001b[0m\u001b[0m | time: 2.863s\n",
            "| SGD | epoch: 977 | loss: 0.49048 - acc: 0.8728 -- iter: 5000/8000\n",
            "Training Step: 10627  | total loss: \u001b[1m\u001b[32m0.49017\u001b[0m\u001b[0m | time: 3.426s\n",
            "| SGD | epoch: 977 | loss: 0.49017 - acc: 0.8731 -- iter: 6000/8000\n",
            "Training Step: 10628  | total loss: \u001b[1m\u001b[32m0.49399\u001b[0m\u001b[0m | time: 3.996s\n",
            "| SGD | epoch: 977 | loss: 0.49399 - acc: 0.8721 -- iter: 7000/8000\n",
            "Training Step: 10629  | total loss: \u001b[1m\u001b[32m0.49251\u001b[0m\u001b[0m | time: 4.571s\n",
            "| SGD | epoch: 977 | loss: 0.49251 - acc: 0.8720 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10630  | total loss: \u001b[1m\u001b[32m0.49147\u001b[0m\u001b[0m | time: 0.570s\n",
            "| SGD | epoch: 978 | loss: 0.49147 - acc: 0.8735 -- iter: 1000/8000\n",
            "Training Step: 10631  | total loss: \u001b[1m\u001b[32m0.48955\u001b[0m\u001b[0m | time: 1.141s\n",
            "| SGD | epoch: 978 | loss: 0.48955 - acc: 0.8727 -- iter: 2000/8000\n",
            "Training Step: 10632  | total loss: \u001b[1m\u001b[32m0.48671\u001b[0m\u001b[0m | time: 1.721s\n",
            "| SGD | epoch: 978 | loss: 0.48671 - acc: 0.8731 -- iter: 3000/8000\n",
            "Training Step: 10633  | total loss: \u001b[1m\u001b[32m0.48804\u001b[0m\u001b[0m | time: 2.285s\n",
            "| SGD | epoch: 978 | loss: 0.48804 - acc: 0.8732 -- iter: 4000/8000\n",
            "Training Step: 10634  | total loss: \u001b[1m\u001b[32m0.48767\u001b[0m\u001b[0m | time: 2.862s\n",
            "| SGD | epoch: 978 | loss: 0.48767 - acc: 0.8726 -- iter: 5000/8000\n",
            "Training Step: 10635  | total loss: \u001b[1m\u001b[32m0.48798\u001b[0m\u001b[0m | time: 3.441s\n",
            "| SGD | epoch: 978 | loss: 0.48798 - acc: 0.8732 -- iter: 6000/8000\n",
            "Training Step: 10636  | total loss: \u001b[1m\u001b[32m0.48981\u001b[0m\u001b[0m | time: 4.026s\n",
            "| SGD | epoch: 978 | loss: 0.48981 - acc: 0.8726 -- iter: 7000/8000\n",
            "Training Step: 10637  | total loss: \u001b[1m\u001b[32m0.49017\u001b[0m\u001b[0m | time: 4.598s\n",
            "| SGD | epoch: 978 | loss: 0.49017 - acc: 0.8727 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10638  | total loss: \u001b[1m\u001b[32m0.48919\u001b[0m\u001b[0m | time: 0.565s\n",
            "| SGD | epoch: 979 | loss: 0.48919 - acc: 0.8739 -- iter: 1000/8000\n",
            "Training Step: 10639  | total loss: \u001b[1m\u001b[32m0.48721\u001b[0m\u001b[0m | time: 1.144s\n",
            "| SGD | epoch: 979 | loss: 0.48721 - acc: 0.8744 -- iter: 2000/8000\n",
            "Training Step: 10640  | total loss: \u001b[1m\u001b[32m0.48921\u001b[0m\u001b[0m | time: 1.718s\n",
            "| SGD | epoch: 979 | loss: 0.48921 - acc: 0.8743 -- iter: 3000/8000\n",
            "Training Step: 10641  | total loss: \u001b[1m\u001b[32m0.49109\u001b[0m\u001b[0m | time: 2.282s\n",
            "| SGD | epoch: 979 | loss: 0.49109 - acc: 0.8729 -- iter: 4000/8000\n",
            "Training Step: 10642  | total loss: \u001b[1m\u001b[32m0.49375\u001b[0m\u001b[0m | time: 2.855s\n",
            "| SGD | epoch: 979 | loss: 0.49375 - acc: 0.8720 -- iter: 5000/8000\n",
            "Training Step: 10643  | total loss: \u001b[1m\u001b[32m0.48960\u001b[0m\u001b[0m | time: 3.419s\n",
            "| SGD | epoch: 979 | loss: 0.48960 - acc: 0.8737 -- iter: 6000/8000\n",
            "Training Step: 10644  | total loss: \u001b[1m\u001b[32m0.49049\u001b[0m\u001b[0m | time: 3.998s\n",
            "| SGD | epoch: 979 | loss: 0.49049 - acc: 0.8728 -- iter: 7000/8000\n",
            "Training Step: 10645  | total loss: \u001b[1m\u001b[32m0.48773\u001b[0m\u001b[0m | time: 4.565s\n",
            "| SGD | epoch: 979 | loss: 0.48773 - acc: 0.8741 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10646  | total loss: \u001b[1m\u001b[32m0.48698\u001b[0m\u001b[0m | time: 0.575s\n",
            "| SGD | epoch: 980 | loss: 0.48698 - acc: 0.8751 -- iter: 1000/8000\n",
            "Training Step: 10647  | total loss: \u001b[1m\u001b[32m0.48739\u001b[0m\u001b[0m | time: 1.143s\n",
            "| SGD | epoch: 980 | loss: 0.48739 - acc: 0.8750 -- iter: 2000/8000\n",
            "Training Step: 10648  | total loss: \u001b[1m\u001b[32m0.48891\u001b[0m\u001b[0m | time: 1.718s\n",
            "| SGD | epoch: 980 | loss: 0.48891 - acc: 0.8744 -- iter: 3000/8000\n",
            "Training Step: 10649  | total loss: \u001b[1m\u001b[32m0.49167\u001b[0m\u001b[0m | time: 2.289s\n",
            "| SGD | epoch: 980 | loss: 0.49167 - acc: 0.8730 -- iter: 4000/8000\n",
            "Training Step: 10650  | total loss: \u001b[1m\u001b[32m0.49434\u001b[0m\u001b[0m | time: 2.859s\n",
            "| SGD | epoch: 980 | loss: 0.49434 - acc: 0.8714 -- iter: 5000/8000\n",
            "Training Step: 10651  | total loss: \u001b[1m\u001b[32m0.49442\u001b[0m\u001b[0m | time: 3.434s\n",
            "| SGD | epoch: 980 | loss: 0.49442 - acc: 0.8709 -- iter: 6000/8000\n",
            "Training Step: 10652  | total loss: \u001b[1m\u001b[32m0.49105\u001b[0m\u001b[0m | time: 3.992s\n",
            "| SGD | epoch: 980 | loss: 0.49105 - acc: 0.8713 -- iter: 7000/8000\n",
            "Training Step: 10653  | total loss: \u001b[1m\u001b[32m0.49190\u001b[0m\u001b[0m | time: 4.570s\n",
            "| SGD | epoch: 980 | loss: 0.49190 - acc: 0.8699 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10654  | total loss: \u001b[1m\u001b[32m0.49412\u001b[0m\u001b[0m | time: 0.570s\n",
            "| SGD | epoch: 981 | loss: 0.49412 - acc: 0.8699 -- iter: 1000/8000\n",
            "Training Step: 10655  | total loss: \u001b[1m\u001b[32m0.49586\u001b[0m\u001b[0m | time: 1.138s\n",
            "| SGD | epoch: 981 | loss: 0.49586 - acc: 0.8699 -- iter: 2000/8000\n",
            "Training Step: 10656  | total loss: \u001b[1m\u001b[32m0.49448\u001b[0m\u001b[0m | time: 1.712s\n",
            "| SGD | epoch: 981 | loss: 0.49448 - acc: 0.8703 -- iter: 3000/8000\n",
            "Training Step: 10657  | total loss: \u001b[1m\u001b[32m0.49048\u001b[0m\u001b[0m | time: 2.283s\n",
            "| SGD | epoch: 981 | loss: 0.49048 - acc: 0.8728 -- iter: 4000/8000\n",
            "Training Step: 10658  | total loss: \u001b[1m\u001b[32m0.48893\u001b[0m\u001b[0m | time: 2.863s\n",
            "| SGD | epoch: 981 | loss: 0.48893 - acc: 0.8725 -- iter: 5000/8000\n",
            "Training Step: 10659  | total loss: \u001b[1m\u001b[32m0.48656\u001b[0m\u001b[0m | time: 3.436s\n",
            "| SGD | epoch: 981 | loss: 0.48656 - acc: 0.8730 -- iter: 6000/8000\n",
            "Training Step: 10660  | total loss: \u001b[1m\u001b[32m0.48503\u001b[0m\u001b[0m | time: 4.020s\n",
            "| SGD | epoch: 981 | loss: 0.48503 - acc: 0.8731 -- iter: 7000/8000\n",
            "Training Step: 10661  | total loss: \u001b[1m\u001b[32m0.48559\u001b[0m\u001b[0m | time: 4.587s\n",
            "| SGD | epoch: 981 | loss: 0.48559 - acc: 0.8741 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10662  | total loss: \u001b[1m\u001b[32m0.48402\u001b[0m\u001b[0m | time: 0.571s\n",
            "| SGD | epoch: 982 | loss: 0.48402 - acc: 0.8752 -- iter: 1000/8000\n",
            "Training Step: 10663  | total loss: \u001b[1m\u001b[32m0.48307\u001b[0m\u001b[0m | time: 1.145s\n",
            "| SGD | epoch: 982 | loss: 0.48307 - acc: 0.8748 -- iter: 2000/8000\n",
            "Training Step: 10664  | total loss: \u001b[1m\u001b[32m0.48424\u001b[0m\u001b[0m | time: 1.715s\n",
            "| SGD | epoch: 982 | loss: 0.48424 - acc: 0.8749 -- iter: 3000/8000\n",
            "Training Step: 10665  | total loss: \u001b[1m\u001b[32m0.48596\u001b[0m\u001b[0m | time: 2.288s\n",
            "| SGD | epoch: 982 | loss: 0.48596 - acc: 0.8730 -- iter: 4000/8000\n",
            "Training Step: 10666  | total loss: \u001b[1m\u001b[32m0.48837\u001b[0m\u001b[0m | time: 2.858s\n",
            "| SGD | epoch: 982 | loss: 0.48837 - acc: 0.8726 -- iter: 5000/8000\n",
            "Training Step: 10667  | total loss: \u001b[1m\u001b[32m0.48629\u001b[0m\u001b[0m | time: 3.432s\n",
            "| SGD | epoch: 982 | loss: 0.48629 - acc: 0.8745 -- iter: 6000/8000\n",
            "Training Step: 10668  | total loss: \u001b[1m\u001b[32m0.48379\u001b[0m\u001b[0m | time: 4.001s\n",
            "| SGD | epoch: 982 | loss: 0.48379 - acc: 0.8763 -- iter: 7000/8000\n",
            "Training Step: 10669  | total loss: \u001b[1m\u001b[32m0.48586\u001b[0m\u001b[0m | time: 4.570s\n",
            "| SGD | epoch: 982 | loss: 0.48586 - acc: 0.8755 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10670  | total loss: \u001b[1m\u001b[32m0.48409\u001b[0m\u001b[0m | time: 0.572s\n",
            "| SGD | epoch: 983 | loss: 0.48409 - acc: 0.8759 -- iter: 1000/8000\n",
            "Training Step: 10671  | total loss: \u001b[1m\u001b[32m0.48615\u001b[0m\u001b[0m | time: 1.144s\n",
            "| SGD | epoch: 983 | loss: 0.48615 - acc: 0.8767 -- iter: 2000/8000\n",
            "Training Step: 10672  | total loss: \u001b[1m\u001b[32m0.48646\u001b[0m\u001b[0m | time: 1.718s\n",
            "| SGD | epoch: 983 | loss: 0.48646 - acc: 0.8758 -- iter: 3000/8000\n",
            "Training Step: 10673  | total loss: \u001b[1m\u001b[32m0.48618\u001b[0m\u001b[0m | time: 2.283s\n",
            "| SGD | epoch: 983 | loss: 0.48618 - acc: 0.8778 -- iter: 4000/8000\n",
            "Training Step: 10674  | total loss: \u001b[1m\u001b[32m0.48960\u001b[0m\u001b[0m | time: 2.857s\n",
            "| SGD | epoch: 983 | loss: 0.48960 - acc: 0.8766 -- iter: 5000/8000\n",
            "Training Step: 10675  | total loss: \u001b[1m\u001b[32m0.48508\u001b[0m\u001b[0m | time: 3.421s\n",
            "| SGD | epoch: 983 | loss: 0.48508 - acc: 0.8778 -- iter: 6000/8000\n",
            "Training Step: 10676  | total loss: \u001b[1m\u001b[32m0.48310\u001b[0m\u001b[0m | time: 3.995s\n",
            "| SGD | epoch: 983 | loss: 0.48310 - acc: 0.8779 -- iter: 7000/8000\n",
            "Training Step: 10677  | total loss: \u001b[1m\u001b[32m0.48152\u001b[0m\u001b[0m | time: 4.567s\n",
            "| SGD | epoch: 983 | loss: 0.48152 - acc: 0.8779 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10678  | total loss: \u001b[1m\u001b[32m0.48287\u001b[0m\u001b[0m | time: 0.572s\n",
            "| SGD | epoch: 984 | loss: 0.48287 - acc: 0.8769 -- iter: 1000/8000\n",
            "Training Step: 10679  | total loss: \u001b[1m\u001b[32m0.48197\u001b[0m\u001b[0m | time: 1.146s\n",
            "| SGD | epoch: 984 | loss: 0.48197 - acc: 0.8766 -- iter: 2000/8000\n",
            "Training Step: 10680  | total loss: \u001b[1m\u001b[32m0.48648\u001b[0m\u001b[0m | time: 1.718s\n",
            "| SGD | epoch: 984 | loss: 0.48648 - acc: 0.8739 -- iter: 3000/8000\n",
            "Training Step: 10681  | total loss: \u001b[1m\u001b[32m0.48344\u001b[0m\u001b[0m | time: 2.288s\n",
            "| SGD | epoch: 984 | loss: 0.48344 - acc: 0.8756 -- iter: 4000/8000\n",
            "Training Step: 10682  | total loss: \u001b[1m\u001b[32m0.48325\u001b[0m\u001b[0m | time: 2.862s\n",
            "| SGD | epoch: 984 | loss: 0.48325 - acc: 0.8751 -- iter: 5000/8000\n",
            "Training Step: 10683  | total loss: \u001b[1m\u001b[32m0.48346\u001b[0m\u001b[0m | time: 3.430s\n",
            "| SGD | epoch: 984 | loss: 0.48346 - acc: 0.8746 -- iter: 6000/8000\n",
            "Training Step: 10684  | total loss: \u001b[1m\u001b[32m0.48294\u001b[0m\u001b[0m | time: 4.014s\n",
            "| SGD | epoch: 984 | loss: 0.48294 - acc: 0.8740 -- iter: 7000/8000\n",
            "Training Step: 10685  | total loss: \u001b[1m\u001b[32m0.48229\u001b[0m\u001b[0m | time: 4.591s\n",
            "| SGD | epoch: 984 | loss: 0.48229 - acc: 0.8751 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10686  | total loss: \u001b[1m\u001b[32m0.47997\u001b[0m\u001b[0m | time: 0.570s\n",
            "| SGD | epoch: 985 | loss: 0.47997 - acc: 0.8774 -- iter: 1000/8000\n",
            "Training Step: 10687  | total loss: \u001b[1m\u001b[32m0.47906\u001b[0m\u001b[0m | time: 1.147s\n",
            "| SGD | epoch: 985 | loss: 0.47906 - acc: 0.8790 -- iter: 2000/8000\n",
            "Training Step: 10688  | total loss: \u001b[1m\u001b[32m0.48311\u001b[0m\u001b[0m | time: 1.721s\n",
            "| SGD | epoch: 985 | loss: 0.48311 - acc: 0.8773 -- iter: 3000/8000\n",
            "Training Step: 10689  | total loss: \u001b[1m\u001b[32m0.48367\u001b[0m\u001b[0m | time: 2.296s\n",
            "| SGD | epoch: 985 | loss: 0.48367 - acc: 0.8769 -- iter: 4000/8000\n",
            "Training Step: 10690  | total loss: \u001b[1m\u001b[32m0.48664\u001b[0m\u001b[0m | time: 2.874s\n",
            "| SGD | epoch: 985 | loss: 0.48664 - acc: 0.8767 -- iter: 5000/8000\n",
            "Training Step: 10691  | total loss: \u001b[1m\u001b[32m0.48451\u001b[0m\u001b[0m | time: 3.435s\n",
            "| SGD | epoch: 985 | loss: 0.48451 - acc: 0.8772 -- iter: 6000/8000\n",
            "Training Step: 10692  | total loss: \u001b[1m\u001b[32m0.48601\u001b[0m\u001b[0m | time: 4.001s\n",
            "| SGD | epoch: 985 | loss: 0.48601 - acc: 0.8772 -- iter: 7000/8000\n",
            "Training Step: 10693  | total loss: \u001b[1m\u001b[32m0.48426\u001b[0m\u001b[0m | time: 4.571s\n",
            "| SGD | epoch: 985 | loss: 0.48426 - acc: 0.8775 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10694  | total loss: \u001b[1m\u001b[32m0.48174\u001b[0m\u001b[0m | time: 0.574s\n",
            "| SGD | epoch: 986 | loss: 0.48174 - acc: 0.8782 -- iter: 1000/8000\n",
            "Training Step: 10695  | total loss: \u001b[1m\u001b[32m0.47919\u001b[0m\u001b[0m | time: 1.143s\n",
            "| SGD | epoch: 986 | loss: 0.47919 - acc: 0.8788 -- iter: 2000/8000\n",
            "Training Step: 10696  | total loss: \u001b[1m\u001b[32m0.48264\u001b[0m\u001b[0m | time: 1.715s\n",
            "| SGD | epoch: 986 | loss: 0.48264 - acc: 0.8770 -- iter: 3000/8000\n",
            "Training Step: 10697  | total loss: \u001b[1m\u001b[32m0.48322\u001b[0m\u001b[0m | time: 2.287s\n",
            "| SGD | epoch: 986 | loss: 0.48322 - acc: 0.8760 -- iter: 4000/8000\n",
            "Training Step: 10698  | total loss: \u001b[1m\u001b[32m0.48274\u001b[0m\u001b[0m | time: 2.857s\n",
            "| SGD | epoch: 986 | loss: 0.48274 - acc: 0.8764 -- iter: 5000/8000\n",
            "Training Step: 10699  | total loss: \u001b[1m\u001b[32m0.48419\u001b[0m\u001b[0m | time: 3.436s\n",
            "| SGD | epoch: 986 | loss: 0.48419 - acc: 0.8746 -- iter: 6000/8000\n",
            "Training Step: 10700  | total loss: \u001b[1m\u001b[32m0.48424\u001b[0m\u001b[0m | time: 4.000s\n",
            "| SGD | epoch: 986 | loss: 0.48424 - acc: 0.8752 -- iter: 7000/8000\n",
            "Training Step: 10701  | total loss: \u001b[1m\u001b[32m0.48399\u001b[0m\u001b[0m | time: 4.566s\n",
            "| SGD | epoch: 986 | loss: 0.48399 - acc: 0.8755 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10702  | total loss: \u001b[1m\u001b[32m0.48113\u001b[0m\u001b[0m | time: 0.572s\n",
            "| SGD | epoch: 987 | loss: 0.48113 - acc: 0.8759 -- iter: 1000/8000\n",
            "Training Step: 10703  | total loss: \u001b[1m\u001b[32m0.48227\u001b[0m\u001b[0m | time: 1.144s\n",
            "| SGD | epoch: 987 | loss: 0.48227 - acc: 0.8757 -- iter: 2000/8000\n",
            "Training Step: 10704  | total loss: \u001b[1m\u001b[32m0.48320\u001b[0m\u001b[0m | time: 1.712s\n",
            "| SGD | epoch: 987 | loss: 0.48320 - acc: 0.8757 -- iter: 3000/8000\n",
            "Training Step: 10705  | total loss: \u001b[1m\u001b[32m0.48619\u001b[0m\u001b[0m | time: 2.284s\n",
            "| SGD | epoch: 987 | loss: 0.48619 - acc: 0.8741 -- iter: 4000/8000\n",
            "Training Step: 10706  | total loss: \u001b[1m\u001b[32m0.48507\u001b[0m\u001b[0m | time: 2.856s\n",
            "| SGD | epoch: 987 | loss: 0.48507 - acc: 0.8753 -- iter: 5000/8000\n",
            "Training Step: 10707  | total loss: \u001b[1m\u001b[32m0.48240\u001b[0m\u001b[0m | time: 3.441s\n",
            "| SGD | epoch: 987 | loss: 0.48240 - acc: 0.8768 -- iter: 6000/8000\n",
            "Training Step: 10708  | total loss: \u001b[1m\u001b[32m0.48414\u001b[0m\u001b[0m | time: 4.011s\n",
            "| SGD | epoch: 987 | loss: 0.48414 - acc: 0.8753 -- iter: 7000/8000\n",
            "Training Step: 10709  | total loss: \u001b[1m\u001b[32m0.48181\u001b[0m\u001b[0m | time: 4.585s\n",
            "| SGD | epoch: 987 | loss: 0.48181 - acc: 0.8773 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10710  | total loss: \u001b[1m\u001b[32m0.48547\u001b[0m\u001b[0m | time: 0.575s\n",
            "| SGD | epoch: 988 | loss: 0.48547 - acc: 0.8764 -- iter: 1000/8000\n",
            "Training Step: 10711  | total loss: \u001b[1m\u001b[32m0.48413\u001b[0m\u001b[0m | time: 1.156s\n",
            "| SGD | epoch: 988 | loss: 0.48413 - acc: 0.8763 -- iter: 2000/8000\n",
            "Training Step: 10712  | total loss: \u001b[1m\u001b[32m0.48318\u001b[0m\u001b[0m | time: 1.725s\n",
            "| SGD | epoch: 988 | loss: 0.48318 - acc: 0.8763 -- iter: 3000/8000\n",
            "Training Step: 10713  | total loss: \u001b[1m\u001b[32m0.48103\u001b[0m\u001b[0m | time: 2.297s\n",
            "| SGD | epoch: 988 | loss: 0.48103 - acc: 0.8771 -- iter: 4000/8000\n",
            "Training Step: 10714  | total loss: \u001b[1m\u001b[32m0.48403\u001b[0m\u001b[0m | time: 2.864s\n",
            "| SGD | epoch: 988 | loss: 0.48403 - acc: 0.8761 -- iter: 5000/8000\n",
            "Training Step: 10715  | total loss: \u001b[1m\u001b[32m0.48570\u001b[0m\u001b[0m | time: 3.430s\n",
            "| SGD | epoch: 988 | loss: 0.48570 - acc: 0.8758 -- iter: 6000/8000\n",
            "Training Step: 10716  | total loss: \u001b[1m\u001b[32m0.48679\u001b[0m\u001b[0m | time: 3.999s\n",
            "| SGD | epoch: 988 | loss: 0.48679 - acc: 0.8757 -- iter: 7000/8000\n",
            "Training Step: 10717  | total loss: \u001b[1m\u001b[32m0.48577\u001b[0m\u001b[0m | time: 4.568s\n",
            "| SGD | epoch: 988 | loss: 0.48577 - acc: 0.8764 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10718  | total loss: \u001b[1m\u001b[32m0.48359\u001b[0m\u001b[0m | time: 0.572s\n",
            "| SGD | epoch: 989 | loss: 0.48359 - acc: 0.8769 -- iter: 1000/8000\n",
            "Training Step: 10719  | total loss: \u001b[1m\u001b[32m0.48649\u001b[0m\u001b[0m | time: 1.142s\n",
            "| SGD | epoch: 989 | loss: 0.48649 - acc: 0.8759 -- iter: 2000/8000\n",
            "Training Step: 10720  | total loss: \u001b[1m\u001b[32m0.48443\u001b[0m\u001b[0m | time: 1.712s\n",
            "| SGD | epoch: 989 | loss: 0.48443 - acc: 0.8760 -- iter: 3000/8000\n",
            "Training Step: 10721  | total loss: \u001b[1m\u001b[32m0.48490\u001b[0m\u001b[0m | time: 2.289s\n",
            "| SGD | epoch: 989 | loss: 0.48490 - acc: 0.8756 -- iter: 4000/8000\n",
            "Training Step: 10722  | total loss: \u001b[1m\u001b[32m0.48407\u001b[0m\u001b[0m | time: 2.854s\n",
            "| SGD | epoch: 989 | loss: 0.48407 - acc: 0.8760 -- iter: 5000/8000\n",
            "Training Step: 10723  | total loss: \u001b[1m\u001b[32m0.48671\u001b[0m\u001b[0m | time: 3.427s\n",
            "| SGD | epoch: 989 | loss: 0.48671 - acc: 0.8748 -- iter: 6000/8000\n",
            "Training Step: 10724  | total loss: \u001b[1m\u001b[32m0.48577\u001b[0m\u001b[0m | time: 3.994s\n",
            "| SGD | epoch: 989 | loss: 0.48577 - acc: 0.8743 -- iter: 7000/8000\n",
            "Training Step: 10725  | total loss: \u001b[1m\u001b[32m0.48570\u001b[0m\u001b[0m | time: 4.566s\n",
            "| SGD | epoch: 989 | loss: 0.48570 - acc: 0.8748 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10726  | total loss: \u001b[1m\u001b[32m0.48912\u001b[0m\u001b[0m | time: 0.572s\n",
            "| SGD | epoch: 990 | loss: 0.48912 - acc: 0.8733 -- iter: 1000/8000\n",
            "Training Step: 10727  | total loss: \u001b[1m\u001b[32m0.48560\u001b[0m\u001b[0m | time: 1.139s\n",
            "| SGD | epoch: 990 | loss: 0.48560 - acc: 0.8758 -- iter: 2000/8000\n",
            "Training Step: 10728  | total loss: \u001b[1m\u001b[32m0.48409\u001b[0m\u001b[0m | time: 1.712s\n",
            "| SGD | epoch: 990 | loss: 0.48409 - acc: 0.8755 -- iter: 3000/8000\n",
            "Training Step: 10729  | total loss: \u001b[1m\u001b[32m0.48698\u001b[0m\u001b[0m | time: 2.287s\n",
            "| SGD | epoch: 990 | loss: 0.48698 - acc: 0.8743 -- iter: 4000/8000\n",
            "Training Step: 10730  | total loss: \u001b[1m\u001b[32m0.48687\u001b[0m\u001b[0m | time: 2.864s\n",
            "| SGD | epoch: 990 | loss: 0.48687 - acc: 0.8741 -- iter: 5000/8000\n",
            "Training Step: 10731  | total loss: \u001b[1m\u001b[32m0.48730\u001b[0m\u001b[0m | time: 3.430s\n",
            "| SGD | epoch: 990 | loss: 0.48730 - acc: 0.8744 -- iter: 6000/8000\n",
            "Training Step: 10732  | total loss: \u001b[1m\u001b[32m0.48394\u001b[0m\u001b[0m | time: 4.012s\n",
            "| SGD | epoch: 990 | loss: 0.48394 - acc: 0.8750 -- iter: 7000/8000\n",
            "Training Step: 10733  | total loss: \u001b[1m\u001b[32m0.48547\u001b[0m\u001b[0m | time: 4.593s\n",
            "| SGD | epoch: 990 | loss: 0.48547 - acc: 0.8746 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10734  | total loss: \u001b[1m\u001b[32m0.48563\u001b[0m\u001b[0m | time: 0.574s\n",
            "| SGD | epoch: 991 | loss: 0.48563 - acc: 0.8744 -- iter: 1000/8000\n",
            "Training Step: 10735  | total loss: \u001b[1m\u001b[32m0.48404\u001b[0m\u001b[0m | time: 1.150s\n",
            "| SGD | epoch: 991 | loss: 0.48404 - acc: 0.8749 -- iter: 2000/8000\n",
            "Training Step: 10736  | total loss: \u001b[1m\u001b[32m0.48582\u001b[0m\u001b[0m | time: 1.724s\n",
            "| SGD | epoch: 991 | loss: 0.48582 - acc: 0.8733 -- iter: 3000/8000\n",
            "Training Step: 10737  | total loss: \u001b[1m\u001b[32m0.48505\u001b[0m\u001b[0m | time: 2.306s\n",
            "| SGD | epoch: 991 | loss: 0.48505 - acc: 0.8748 -- iter: 4000/8000\n",
            "Training Step: 10738  | total loss: \u001b[1m\u001b[32m0.48513\u001b[0m\u001b[0m | time: 2.875s\n",
            "| SGD | epoch: 991 | loss: 0.48513 - acc: 0.8746 -- iter: 5000/8000\n",
            "Training Step: 10739  | total loss: \u001b[1m\u001b[32m0.48669\u001b[0m\u001b[0m | time: 3.453s\n",
            "| SGD | epoch: 991 | loss: 0.48669 - acc: 0.8740 -- iter: 6000/8000\n",
            "Training Step: 10740  | total loss: \u001b[1m\u001b[32m0.48727\u001b[0m\u001b[0m | time: 4.014s\n",
            "| SGD | epoch: 991 | loss: 0.48727 - acc: 0.8732 -- iter: 7000/8000\n",
            "Training Step: 10741  | total loss: \u001b[1m\u001b[32m0.48886\u001b[0m\u001b[0m | time: 4.583s\n",
            "| SGD | epoch: 991 | loss: 0.48886 - acc: 0.8737 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10742  | total loss: \u001b[1m\u001b[32m0.48513\u001b[0m\u001b[0m | time: 0.574s\n",
            "| SGD | epoch: 992 | loss: 0.48513 - acc: 0.8748 -- iter: 1000/8000\n",
            "Training Step: 10743  | total loss: \u001b[1m\u001b[32m0.48305\u001b[0m\u001b[0m | time: 1.146s\n",
            "| SGD | epoch: 992 | loss: 0.48305 - acc: 0.8755 -- iter: 2000/8000\n",
            "Training Step: 10744  | total loss: \u001b[1m\u001b[32m0.48429\u001b[0m\u001b[0m | time: 1.716s\n",
            "| SGD | epoch: 992 | loss: 0.48429 - acc: 0.8730 -- iter: 3000/8000\n",
            "Training Step: 10745  | total loss: \u001b[1m\u001b[32m0.48317\u001b[0m\u001b[0m | time: 2.294s\n",
            "| SGD | epoch: 992 | loss: 0.48317 - acc: 0.8733 -- iter: 4000/8000\n",
            "Training Step: 10746  | total loss: \u001b[1m\u001b[32m0.48240\u001b[0m\u001b[0m | time: 2.860s\n",
            "| SGD | epoch: 992 | loss: 0.48240 - acc: 0.8744 -- iter: 5000/8000\n",
            "Training Step: 10747  | total loss: \u001b[1m\u001b[32m0.48256\u001b[0m\u001b[0m | time: 3.432s\n",
            "| SGD | epoch: 992 | loss: 0.48256 - acc: 0.8735 -- iter: 6000/8000\n",
            "Training Step: 10748  | total loss: \u001b[1m\u001b[32m0.48341\u001b[0m\u001b[0m | time: 4.005s\n",
            "| SGD | epoch: 992 | loss: 0.48341 - acc: 0.8739 -- iter: 7000/8000\n",
            "Training Step: 10749  | total loss: \u001b[1m\u001b[32m0.48415\u001b[0m\u001b[0m | time: 4.578s\n",
            "| SGD | epoch: 992 | loss: 0.48415 - acc: 0.8746 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10750  | total loss: \u001b[1m\u001b[32m0.48529\u001b[0m\u001b[0m | time: 0.571s\n",
            "| SGD | epoch: 993 | loss: 0.48529 - acc: 0.8746 -- iter: 1000/8000\n",
            "Training Step: 10751  | total loss: \u001b[1m\u001b[32m0.48532\u001b[0m\u001b[0m | time: 1.140s\n",
            "| SGD | epoch: 993 | loss: 0.48532 - acc: 0.8758 -- iter: 2000/8000\n",
            "Training Step: 10752  | total loss: \u001b[1m\u001b[32m0.48507\u001b[0m\u001b[0m | time: 1.713s\n",
            "| SGD | epoch: 993 | loss: 0.48507 - acc: 0.8754 -- iter: 3000/8000\n",
            "Training Step: 10753  | total loss: \u001b[1m\u001b[32m0.48483\u001b[0m\u001b[0m | time: 2.282s\n",
            "| SGD | epoch: 993 | loss: 0.48483 - acc: 0.8776 -- iter: 4000/8000\n",
            "Training Step: 10754  | total loss: \u001b[1m\u001b[32m0.48449\u001b[0m\u001b[0m | time: 2.855s\n",
            "| SGD | epoch: 993 | loss: 0.48449 - acc: 0.8777 -- iter: 5000/8000\n",
            "Training Step: 10755  | total loss: \u001b[1m\u001b[32m0.48095\u001b[0m\u001b[0m | time: 3.429s\n",
            "| SGD | epoch: 993 | loss: 0.48095 - acc: 0.8795 -- iter: 6000/8000\n",
            "Training Step: 10756  | total loss: \u001b[1m\u001b[32m0.48033\u001b[0m\u001b[0m | time: 4.017s\n",
            "| SGD | epoch: 993 | loss: 0.48033 - acc: 0.8787 -- iter: 7000/8000\n",
            "Training Step: 10757  | total loss: \u001b[1m\u001b[32m0.47992\u001b[0m\u001b[0m | time: 4.585s\n",
            "| SGD | epoch: 993 | loss: 0.47992 - acc: 0.8788 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10758  | total loss: \u001b[1m\u001b[32m0.47988\u001b[0m\u001b[0m | time: 0.587s\n",
            "| SGD | epoch: 994 | loss: 0.47988 - acc: 0.8783 -- iter: 1000/8000\n",
            "Training Step: 10759  | total loss: \u001b[1m\u001b[32m0.48280\u001b[0m\u001b[0m | time: 1.160s\n",
            "| SGD | epoch: 994 | loss: 0.48280 - acc: 0.8765 -- iter: 2000/8000\n",
            "Training Step: 10760  | total loss: \u001b[1m\u001b[32m0.48407\u001b[0m\u001b[0m | time: 1.732s\n",
            "| SGD | epoch: 994 | loss: 0.48407 - acc: 0.8763 -- iter: 3000/8000\n",
            "Training Step: 10761  | total loss: \u001b[1m\u001b[32m0.48412\u001b[0m\u001b[0m | time: 2.306s\n",
            "| SGD | epoch: 994 | loss: 0.48412 - acc: 0.8760 -- iter: 4000/8000\n",
            "Training Step: 10762  | total loss: \u001b[1m\u001b[32m0.48152\u001b[0m\u001b[0m | time: 2.876s\n",
            "| SGD | epoch: 994 | loss: 0.48152 - acc: 0.8767 -- iter: 5000/8000\n",
            "Training Step: 10763  | total loss: \u001b[1m\u001b[32m0.47975\u001b[0m\u001b[0m | time: 3.440s\n",
            "| SGD | epoch: 994 | loss: 0.47975 - acc: 0.8777 -- iter: 6000/8000\n",
            "Training Step: 10764  | total loss: \u001b[1m\u001b[32m0.48147\u001b[0m\u001b[0m | time: 4.016s\n",
            "| SGD | epoch: 994 | loss: 0.48147 - acc: 0.8773 -- iter: 7000/8000\n",
            "Training Step: 10765  | total loss: \u001b[1m\u001b[32m0.48489\u001b[0m\u001b[0m | time: 4.583s\n",
            "| SGD | epoch: 994 | loss: 0.48489 - acc: 0.8754 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10766  | total loss: \u001b[1m\u001b[32m0.48719\u001b[0m\u001b[0m | time: 0.574s\n",
            "| SGD | epoch: 995 | loss: 0.48719 - acc: 0.8751 -- iter: 1000/8000\n",
            "Training Step: 10767  | total loss: \u001b[1m\u001b[32m0.48925\u001b[0m\u001b[0m | time: 1.146s\n",
            "| SGD | epoch: 995 | loss: 0.48925 - acc: 0.8736 -- iter: 2000/8000\n",
            "Training Step: 10768  | total loss: \u001b[1m\u001b[32m0.48727\u001b[0m\u001b[0m | time: 1.724s\n",
            "| SGD | epoch: 995 | loss: 0.48727 - acc: 0.8745 -- iter: 3000/8000\n",
            "Training Step: 10769  | total loss: \u001b[1m\u001b[32m0.48499\u001b[0m\u001b[0m | time: 2.292s\n",
            "| SGD | epoch: 995 | loss: 0.48499 - acc: 0.8747 -- iter: 4000/8000\n",
            "Training Step: 10770  | total loss: \u001b[1m\u001b[32m0.48678\u001b[0m\u001b[0m | time: 2.867s\n",
            "| SGD | epoch: 995 | loss: 0.48678 - acc: 0.8745 -- iter: 5000/8000\n",
            "Training Step: 10771  | total loss: \u001b[1m\u001b[32m0.48874\u001b[0m\u001b[0m | time: 3.437s\n",
            "| SGD | epoch: 995 | loss: 0.48874 - acc: 0.8752 -- iter: 6000/8000\n",
            "Training Step: 10772  | total loss: \u001b[1m\u001b[32m0.48648\u001b[0m\u001b[0m | time: 4.012s\n",
            "| SGD | epoch: 995 | loss: 0.48648 - acc: 0.8760 -- iter: 7000/8000\n",
            "Training Step: 10773  | total loss: \u001b[1m\u001b[32m0.48688\u001b[0m\u001b[0m | time: 4.584s\n",
            "| SGD | epoch: 995 | loss: 0.48688 - acc: 0.8767 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10774  | total loss: \u001b[1m\u001b[32m0.48448\u001b[0m\u001b[0m | time: 0.567s\n",
            "| SGD | epoch: 996 | loss: 0.48448 - acc: 0.8769 -- iter: 1000/8000\n",
            "Training Step: 10775  | total loss: \u001b[1m\u001b[32m0.48253\u001b[0m\u001b[0m | time: 1.139s\n",
            "| SGD | epoch: 996 | loss: 0.48253 - acc: 0.8768 -- iter: 2000/8000\n",
            "Training Step: 10776  | total loss: \u001b[1m\u001b[32m0.48072\u001b[0m\u001b[0m | time: 1.713s\n",
            "| SGD | epoch: 996 | loss: 0.48072 - acc: 0.8766 -- iter: 3000/8000\n",
            "Training Step: 10777  | total loss: \u001b[1m\u001b[32m0.47956\u001b[0m\u001b[0m | time: 2.284s\n",
            "| SGD | epoch: 996 | loss: 0.47956 - acc: 0.8774 -- iter: 4000/8000\n",
            "Training Step: 10778  | total loss: \u001b[1m\u001b[32m0.47865\u001b[0m\u001b[0m | time: 2.860s\n",
            "| SGD | epoch: 996 | loss: 0.47865 - acc: 0.8764 -- iter: 5000/8000\n",
            "Training Step: 10779  | total loss: \u001b[1m\u001b[32m0.47893\u001b[0m\u001b[0m | time: 3.446s\n",
            "| SGD | epoch: 996 | loss: 0.47893 - acc: 0.8770 -- iter: 6000/8000\n",
            "Training Step: 10780  | total loss: \u001b[1m\u001b[32m0.48085\u001b[0m\u001b[0m | time: 4.019s\n",
            "| SGD | epoch: 996 | loss: 0.48085 - acc: 0.8769 -- iter: 7000/8000\n",
            "Training Step: 10781  | total loss: \u001b[1m\u001b[32m0.47984\u001b[0m\u001b[0m | time: 4.595s\n",
            "| SGD | epoch: 996 | loss: 0.47984 - acc: 0.8768 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10782  | total loss: \u001b[1m\u001b[32m0.48090\u001b[0m\u001b[0m | time: 0.582s\n",
            "| SGD | epoch: 997 | loss: 0.48090 - acc: 0.8762 -- iter: 1000/8000\n",
            "Training Step: 10783  | total loss: \u001b[1m\u001b[32m0.47749\u001b[0m\u001b[0m | time: 1.159s\n",
            "| SGD | epoch: 997 | loss: 0.47749 - acc: 0.8758 -- iter: 2000/8000\n",
            "Training Step: 10784  | total loss: \u001b[1m\u001b[32m0.47744\u001b[0m\u001b[0m | time: 1.734s\n",
            "| SGD | epoch: 997 | loss: 0.47744 - acc: 0.8751 -- iter: 3000/8000\n",
            "Training Step: 10785  | total loss: \u001b[1m\u001b[32m0.47803\u001b[0m\u001b[0m | time: 2.306s\n",
            "| SGD | epoch: 997 | loss: 0.47803 - acc: 0.8751 -- iter: 4000/8000\n",
            "Training Step: 10786  | total loss: \u001b[1m\u001b[32m0.47649\u001b[0m\u001b[0m | time: 2.874s\n",
            "| SGD | epoch: 997 | loss: 0.47649 - acc: 0.8759 -- iter: 5000/8000\n",
            "Training Step: 10787  | total loss: \u001b[1m\u001b[32m0.47953\u001b[0m\u001b[0m | time: 3.444s\n",
            "| SGD | epoch: 997 | loss: 0.47953 - acc: 0.8741 -- iter: 6000/8000\n",
            "Training Step: 10788  | total loss: \u001b[1m\u001b[32m0.47922\u001b[0m\u001b[0m | time: 4.012s\n",
            "| SGD | epoch: 997 | loss: 0.47922 - acc: 0.8742 -- iter: 7000/8000\n",
            "Training Step: 10789  | total loss: \u001b[1m\u001b[32m0.48069\u001b[0m\u001b[0m | time: 4.587s\n",
            "| SGD | epoch: 997 | loss: 0.48069 - acc: 0.8740 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10790  | total loss: \u001b[1m\u001b[32m0.48035\u001b[0m\u001b[0m | time: 0.575s\n",
            "| SGD | epoch: 998 | loss: 0.48035 - acc: 0.8744 -- iter: 1000/8000\n",
            "Training Step: 10791  | total loss: \u001b[1m\u001b[32m0.48236\u001b[0m\u001b[0m | time: 1.141s\n",
            "| SGD | epoch: 998 | loss: 0.48236 - acc: 0.8744 -- iter: 2000/8000\n",
            "Training Step: 10792  | total loss: \u001b[1m\u001b[32m0.47839\u001b[0m\u001b[0m | time: 1.704s\n",
            "| SGD | epoch: 998 | loss: 0.47839 - acc: 0.8762 -- iter: 3000/8000\n",
            "Training Step: 10793  | total loss: \u001b[1m\u001b[32m0.47746\u001b[0m\u001b[0m | time: 2.276s\n",
            "| SGD | epoch: 998 | loss: 0.47746 - acc: 0.8772 -- iter: 4000/8000\n",
            "Training Step: 10794  | total loss: \u001b[1m\u001b[32m0.47663\u001b[0m\u001b[0m | time: 2.843s\n",
            "| SGD | epoch: 998 | loss: 0.47663 - acc: 0.8774 -- iter: 5000/8000\n",
            "Training Step: 10795  | total loss: \u001b[1m\u001b[32m0.47845\u001b[0m\u001b[0m | time: 3.412s\n",
            "| SGD | epoch: 998 | loss: 0.47845 - acc: 0.8766 -- iter: 6000/8000\n",
            "Training Step: 10796  | total loss: \u001b[1m\u001b[32m0.47801\u001b[0m\u001b[0m | time: 3.975s\n",
            "| SGD | epoch: 998 | loss: 0.47801 - acc: 0.8759 -- iter: 7000/8000\n",
            "Training Step: 10797  | total loss: \u001b[1m\u001b[32m0.47981\u001b[0m\u001b[0m | time: 4.550s\n",
            "| SGD | epoch: 998 | loss: 0.47981 - acc: 0.8757 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10798  | total loss: \u001b[1m\u001b[32m0.47876\u001b[0m\u001b[0m | time: 0.571s\n",
            "| SGD | epoch: 999 | loss: 0.47876 - acc: 0.8763 -- iter: 1000/8000\n",
            "Training Step: 10799  | total loss: \u001b[1m\u001b[32m0.47932\u001b[0m\u001b[0m | time: 1.142s\n",
            "| SGD | epoch: 999 | loss: 0.47932 - acc: 0.8756 -- iter: 2000/8000\n",
            "Training Step: 10800  | total loss: \u001b[1m\u001b[32m0.48065\u001b[0m\u001b[0m | time: 1.707s\n",
            "| SGD | epoch: 999 | loss: 0.48065 - acc: 0.8754 -- iter: 3000/8000\n",
            "Training Step: 10801  | total loss: \u001b[1m\u001b[32m0.48039\u001b[0m\u001b[0m | time: 2.271s\n",
            "| SGD | epoch: 999 | loss: 0.48039 - acc: 0.8747 -- iter: 4000/8000\n",
            "Training Step: 10802  | total loss: \u001b[1m\u001b[32m0.47852\u001b[0m\u001b[0m | time: 2.846s\n",
            "| SGD | epoch: 999 | loss: 0.47852 - acc: 0.8755 -- iter: 5000/8000\n",
            "Training Step: 10803  | total loss: \u001b[1m\u001b[32m0.47611\u001b[0m\u001b[0m | time: 3.492s\n",
            "| SGD | epoch: 999 | loss: 0.47611 - acc: 0.8772 -- iter: 6000/8000\n",
            "Training Step: 10804  | total loss: \u001b[1m\u001b[32m0.47630\u001b[0m\u001b[0m | time: 4.117s\n",
            "| SGD | epoch: 999 | loss: 0.47630 - acc: 0.8784 -- iter: 7000/8000\n",
            "Training Step: 10805  | total loss: \u001b[1m\u001b[32m0.47763\u001b[0m\u001b[0m | time: 4.742s\n",
            "| SGD | epoch: 999 | loss: 0.47763 - acc: 0.8779 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10806  | total loss: \u001b[1m\u001b[32m0.47910\u001b[0m\u001b[0m | time: 0.627s\n",
            "| SGD | epoch: 1000 | loss: 0.47910 - acc: 0.8775 -- iter: 1000/8000\n",
            "Training Step: 10807  | total loss: \u001b[1m\u001b[32m0.48237\u001b[0m\u001b[0m | time: 1.252s\n",
            "| SGD | epoch: 1000 | loss: 0.48237 - acc: 0.8775 -- iter: 2000/8000\n",
            "Training Step: 10808  | total loss: \u001b[1m\u001b[32m0.47710\u001b[0m\u001b[0m | time: 1.874s\n",
            "| SGD | epoch: 1000 | loss: 0.47710 - acc: 0.8786 -- iter: 3000/8000\n",
            "Training Step: 10809  | total loss: \u001b[1m\u001b[32m0.47805\u001b[0m\u001b[0m | time: 2.516s\n",
            "| SGD | epoch: 1000 | loss: 0.47805 - acc: 0.8788 -- iter: 4000/8000\n",
            "Training Step: 10810  | total loss: \u001b[1m\u001b[32m0.47759\u001b[0m\u001b[0m | time: 3.132s\n",
            "| SGD | epoch: 1000 | loss: 0.47759 - acc: 0.8795 -- iter: 5000/8000\n",
            "Training Step: 10811  | total loss: \u001b[1m\u001b[32m0.47668\u001b[0m\u001b[0m | time: 3.752s\n",
            "| SGD | epoch: 1000 | loss: 0.47668 - acc: 0.8791 -- iter: 6000/8000\n",
            "Training Step: 10812  | total loss: \u001b[1m\u001b[32m0.47523\u001b[0m\u001b[0m | time: 4.369s\n",
            "| SGD | epoch: 1000 | loss: 0.47523 - acc: 0.8797 -- iter: 7000/8000\n",
            "Training Step: 10813  | total loss: \u001b[1m\u001b[32m0.47607\u001b[0m\u001b[0m | time: 4.983s\n",
            "| SGD | epoch: 1000 | loss: 0.47607 - acc: 0.8803 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10814  | total loss: \u001b[1m\u001b[32m0.47668\u001b[0m\u001b[0m | time: 0.617s\n",
            "| SGD | epoch: 1001 | loss: 0.47668 - acc: 0.8796 -- iter: 1000/8000\n",
            "Training Step: 10815  | total loss: \u001b[1m\u001b[32m0.47753\u001b[0m\u001b[0m | time: 1.234s\n",
            "| SGD | epoch: 1001 | loss: 0.47753 - acc: 0.8799 -- iter: 2000/8000\n",
            "Training Step: 10816  | total loss: \u001b[1m\u001b[32m0.47620\u001b[0m\u001b[0m | time: 1.856s\n",
            "| SGD | epoch: 1001 | loss: 0.47620 - acc: 0.8803 -- iter: 3000/8000\n",
            "Training Step: 10817  | total loss: \u001b[1m\u001b[32m0.47407\u001b[0m\u001b[0m | time: 2.473s\n",
            "| SGD | epoch: 1001 | loss: 0.47407 - acc: 0.8797 -- iter: 4000/8000\n",
            "Training Step: 10818  | total loss: \u001b[1m\u001b[32m0.47607\u001b[0m\u001b[0m | time: 3.089s\n",
            "| SGD | epoch: 1001 | loss: 0.47607 - acc: 0.8793 -- iter: 5000/8000\n",
            "Training Step: 10819  | total loss: \u001b[1m\u001b[32m0.47519\u001b[0m\u001b[0m | time: 3.703s\n",
            "| SGD | epoch: 1001 | loss: 0.47519 - acc: 0.8808 -- iter: 6000/8000\n",
            "Training Step: 10820  | total loss: \u001b[1m\u001b[32m0.47932\u001b[0m\u001b[0m | time: 4.318s\n",
            "| SGD | epoch: 1001 | loss: 0.47932 - acc: 0.8777 -- iter: 7000/8000\n",
            "Training Step: 10821  | total loss: \u001b[1m\u001b[32m0.48152\u001b[0m\u001b[0m | time: 4.936s\n",
            "| SGD | epoch: 1001 | loss: 0.48152 - acc: 0.8769 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10822  | total loss: \u001b[1m\u001b[32m0.48152\u001b[0m\u001b[0m | time: 0.618s\n",
            "| SGD | epoch: 1002 | loss: 0.48152 - acc: 0.8769 -- iter: 1000/8000\n",
            "Training Step: 10823  | total loss: \u001b[1m\u001b[32m0.48015\u001b[0m\u001b[0m | time: 1.233s\n",
            "| SGD | epoch: 1002 | loss: 0.48015 - acc: 0.8765 -- iter: 2000/8000\n",
            "Training Step: 10824  | total loss: \u001b[1m\u001b[32m0.48281\u001b[0m\u001b[0m | time: 1.859s\n",
            "| SGD | epoch: 1002 | loss: 0.48281 - acc: 0.8758 -- iter: 3000/8000\n",
            "Training Step: 10825  | total loss: \u001b[1m\u001b[32m0.48034\u001b[0m\u001b[0m | time: 2.482s\n",
            "| SGD | epoch: 1002 | loss: 0.48034 - acc: 0.8762 -- iter: 4000/8000\n",
            "Training Step: 10826  | total loss: \u001b[1m\u001b[32m0.47707\u001b[0m\u001b[0m | time: 3.108s\n",
            "| SGD | epoch: 1002 | loss: 0.47707 - acc: 0.8769 -- iter: 5000/8000\n",
            "Training Step: 10827  | total loss: \u001b[1m\u001b[32m0.47925\u001b[0m\u001b[0m | time: 3.734s\n",
            "| SGD | epoch: 1002 | loss: 0.47925 - acc: 0.8770 -- iter: 6000/8000\n",
            "Training Step: 10828  | total loss: \u001b[1m\u001b[32m0.48012\u001b[0m\u001b[0m | time: 4.362s\n",
            "| SGD | epoch: 1002 | loss: 0.48012 - acc: 0.8769 -- iter: 7000/8000\n",
            "Training Step: 10829  | total loss: \u001b[1m\u001b[32m0.47921\u001b[0m\u001b[0m | time: 4.990s\n",
            "| SGD | epoch: 1002 | loss: 0.47921 - acc: 0.8781 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10830  | total loss: \u001b[1m\u001b[32m0.47804\u001b[0m\u001b[0m | time: 0.626s\n",
            "| SGD | epoch: 1003 | loss: 0.47804 - acc: 0.8794 -- iter: 1000/8000\n",
            "Training Step: 10831  | total loss: \u001b[1m\u001b[32m0.47803\u001b[0m\u001b[0m | time: 1.249s\n",
            "| SGD | epoch: 1003 | loss: 0.47803 - acc: 0.8802 -- iter: 2000/8000\n",
            "Training Step: 10832  | total loss: \u001b[1m\u001b[32m0.47766\u001b[0m\u001b[0m | time: 1.870s\n",
            "| SGD | epoch: 1003 | loss: 0.47766 - acc: 0.8790 -- iter: 3000/8000\n",
            "Training Step: 10833  | total loss: \u001b[1m\u001b[32m0.47460\u001b[0m\u001b[0m | time: 2.485s\n",
            "| SGD | epoch: 1003 | loss: 0.47460 - acc: 0.8804 -- iter: 4000/8000\n",
            "Training Step: 10834  | total loss: \u001b[1m\u001b[32m0.47470\u001b[0m\u001b[0m | time: 3.105s\n",
            "| SGD | epoch: 1003 | loss: 0.47470 - acc: 0.8795 -- iter: 5000/8000\n",
            "Training Step: 10835  | total loss: \u001b[1m\u001b[32m0.47735\u001b[0m\u001b[0m | time: 3.722s\n",
            "| SGD | epoch: 1003 | loss: 0.47735 - acc: 0.8794 -- iter: 6000/8000\n",
            "Training Step: 10836  | total loss: \u001b[1m\u001b[32m0.48215\u001b[0m\u001b[0m | time: 4.342s\n",
            "| SGD | epoch: 1003 | loss: 0.48215 - acc: 0.8771 -- iter: 7000/8000\n",
            "Training Step: 10837  | total loss: \u001b[1m\u001b[32m0.47869\u001b[0m\u001b[0m | time: 5.076s\n",
            "| SGD | epoch: 1003 | loss: 0.47869 - acc: 0.8778 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10838  | total loss: \u001b[1m\u001b[32m0.47757\u001b[0m\u001b[0m | time: 0.624s\n",
            "| SGD | epoch: 1004 | loss: 0.47757 - acc: 0.8778 -- iter: 1000/8000\n",
            "Training Step: 10839  | total loss: \u001b[1m\u001b[32m0.47485\u001b[0m\u001b[0m | time: 1.244s\n",
            "| SGD | epoch: 1004 | loss: 0.47485 - acc: 0.8797 -- iter: 2000/8000\n",
            "Training Step: 10840  | total loss: \u001b[1m\u001b[32m0.47390\u001b[0m\u001b[0m | time: 1.866s\n",
            "| SGD | epoch: 1004 | loss: 0.47390 - acc: 0.8809 -- iter: 3000/8000\n",
            "Training Step: 10841  | total loss: \u001b[1m\u001b[32m0.47630\u001b[0m\u001b[0m | time: 2.484s\n",
            "| SGD | epoch: 1004 | loss: 0.47630 - acc: 0.8792 -- iter: 4000/8000\n",
            "Training Step: 10842  | total loss: \u001b[1m\u001b[32m0.47639\u001b[0m\u001b[0m | time: 3.108s\n",
            "| SGD | epoch: 1004 | loss: 0.47639 - acc: 0.8791 -- iter: 5000/8000\n",
            "Training Step: 10843  | total loss: \u001b[1m\u001b[32m0.47715\u001b[0m\u001b[0m | time: 3.731s\n",
            "| SGD | epoch: 1004 | loss: 0.47715 - acc: 0.8784 -- iter: 6000/8000\n",
            "Training Step: 10844  | total loss: \u001b[1m\u001b[32m0.47930\u001b[0m\u001b[0m | time: 4.351s\n",
            "| SGD | epoch: 1004 | loss: 0.47930 - acc: 0.8779 -- iter: 7000/8000\n",
            "Training Step: 10845  | total loss: \u001b[1m\u001b[32m0.48146\u001b[0m\u001b[0m | time: 4.973s\n",
            "| SGD | epoch: 1004 | loss: 0.48146 - acc: 0.8779 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10846  | total loss: \u001b[1m\u001b[32m0.47741\u001b[0m\u001b[0m | time: 0.623s\n",
            "| SGD | epoch: 1005 | loss: 0.47741 - acc: 0.8788 -- iter: 1000/8000\n",
            "Training Step: 10847  | total loss: \u001b[1m\u001b[32m0.47584\u001b[0m\u001b[0m | time: 1.285s\n",
            "| SGD | epoch: 1005 | loss: 0.47584 - acc: 0.8794 -- iter: 2000/8000\n",
            "Training Step: 10848  | total loss: \u001b[1m\u001b[32m0.47449\u001b[0m\u001b[0m | time: 1.910s\n",
            "| SGD | epoch: 1005 | loss: 0.47449 - acc: 0.8795 -- iter: 3000/8000\n",
            "Training Step: 10849  | total loss: \u001b[1m\u001b[32m0.47555\u001b[0m\u001b[0m | time: 2.538s\n",
            "| SGD | epoch: 1005 | loss: 0.47555 - acc: 0.8781 -- iter: 4000/8000\n",
            "Training Step: 10850  | total loss: \u001b[1m\u001b[32m0.47394\u001b[0m\u001b[0m | time: 3.165s\n",
            "| SGD | epoch: 1005 | loss: 0.47394 - acc: 0.8800 -- iter: 5000/8000\n",
            "Training Step: 10851  | total loss: \u001b[1m\u001b[32m0.47677\u001b[0m\u001b[0m | time: 3.789s\n",
            "| SGD | epoch: 1005 | loss: 0.47677 - acc: 0.8794 -- iter: 6000/8000\n",
            "Training Step: 10852  | total loss: \u001b[1m\u001b[32m0.47910\u001b[0m\u001b[0m | time: 4.412s\n",
            "| SGD | epoch: 1005 | loss: 0.47910 - acc: 0.8790 -- iter: 7000/8000\n",
            "Training Step: 10853  | total loss: \u001b[1m\u001b[32m0.47893\u001b[0m\u001b[0m | time: 5.030s\n",
            "| SGD | epoch: 1005 | loss: 0.47893 - acc: 0.8800 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10854  | total loss: \u001b[1m\u001b[32m0.47487\u001b[0m\u001b[0m | time: 0.620s\n",
            "| SGD | epoch: 1006 | loss: 0.47487 - acc: 0.8797 -- iter: 1000/8000\n",
            "Training Step: 10855  | total loss: \u001b[1m\u001b[32m0.47579\u001b[0m\u001b[0m | time: 1.236s\n",
            "| SGD | epoch: 1006 | loss: 0.47579 - acc: 0.8805 -- iter: 2000/8000\n",
            "Training Step: 10856  | total loss: \u001b[1m\u001b[32m0.47593\u001b[0m\u001b[0m | time: 1.852s\n",
            "| SGD | epoch: 1006 | loss: 0.47593 - acc: 0.8802 -- iter: 3000/8000\n",
            "Training Step: 10857  | total loss: \u001b[1m\u001b[32m0.47625\u001b[0m\u001b[0m | time: 2.469s\n",
            "| SGD | epoch: 1006 | loss: 0.47625 - acc: 0.8797 -- iter: 4000/8000\n",
            "Training Step: 10858  | total loss: \u001b[1m\u001b[32m0.47656\u001b[0m\u001b[0m | time: 3.084s\n",
            "| SGD | epoch: 1006 | loss: 0.47656 - acc: 0.8796 -- iter: 5000/8000\n",
            "Training Step: 10859  | total loss: \u001b[1m\u001b[32m0.47343\u001b[0m\u001b[0m | time: 3.702s\n",
            "| SGD | epoch: 1006 | loss: 0.47343 - acc: 0.8801 -- iter: 6000/8000\n",
            "Training Step: 10860  | total loss: \u001b[1m\u001b[32m0.47263\u001b[0m\u001b[0m | time: 4.319s\n",
            "| SGD | epoch: 1006 | loss: 0.47263 - acc: 0.8820 -- iter: 7000/8000\n",
            "Training Step: 10861  | total loss: \u001b[1m\u001b[32m0.47193\u001b[0m\u001b[0m | time: 4.934s\n",
            "| SGD | epoch: 1006 | loss: 0.47193 - acc: 0.8827 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10862  | total loss: \u001b[1m\u001b[32m0.47065\u001b[0m\u001b[0m | time: 0.617s\n",
            "| SGD | epoch: 1007 | loss: 0.47065 - acc: 0.8835 -- iter: 1000/8000\n",
            "Training Step: 10863  | total loss: \u001b[1m\u001b[32m0.47148\u001b[0m\u001b[0m | time: 1.231s\n",
            "| SGD | epoch: 1007 | loss: 0.47148 - acc: 0.8831 -- iter: 2000/8000\n",
            "Training Step: 10864  | total loss: \u001b[1m\u001b[32m0.47671\u001b[0m\u001b[0m | time: 1.858s\n",
            "| SGD | epoch: 1007 | loss: 0.47671 - acc: 0.8812 -- iter: 3000/8000\n",
            "Training Step: 10865  | total loss: \u001b[1m\u001b[32m0.47829\u001b[0m\u001b[0m | time: 2.472s\n",
            "| SGD | epoch: 1007 | loss: 0.47829 - acc: 0.8799 -- iter: 4000/8000\n",
            "Training Step: 10866  | total loss: \u001b[1m\u001b[32m0.47908\u001b[0m\u001b[0m | time: 3.087s\n",
            "| SGD | epoch: 1007 | loss: 0.47908 - acc: 0.8794 -- iter: 5000/8000\n",
            "Training Step: 10867  | total loss: \u001b[1m\u001b[32m0.47654\u001b[0m\u001b[0m | time: 3.704s\n",
            "| SGD | epoch: 1007 | loss: 0.47654 - acc: 0.8805 -- iter: 6000/8000\n",
            "Training Step: 10868  | total loss: \u001b[1m\u001b[32m0.47355\u001b[0m\u001b[0m | time: 4.330s\n",
            "| SGD | epoch: 1007 | loss: 0.47355 - acc: 0.8815 -- iter: 7000/8000\n",
            "Training Step: 10869  | total loss: \u001b[1m\u001b[32m0.47457\u001b[0m\u001b[0m | time: 4.953s\n",
            "| SGD | epoch: 1007 | loss: 0.47457 - acc: 0.8812 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10870  | total loss: \u001b[1m\u001b[32m0.47630\u001b[0m\u001b[0m | time: 0.627s\n",
            "| SGD | epoch: 1008 | loss: 0.47630 - acc: 0.8795 -- iter: 1000/8000\n",
            "Training Step: 10871  | total loss: \u001b[1m\u001b[32m0.47696\u001b[0m\u001b[0m | time: 1.269s\n",
            "| SGD | epoch: 1008 | loss: 0.47696 - acc: 0.8787 -- iter: 2000/8000\n",
            "Training Step: 10872  | total loss: \u001b[1m\u001b[32m0.47914\u001b[0m\u001b[0m | time: 1.917s\n",
            "| SGD | epoch: 1008 | loss: 0.47914 - acc: 0.8785 -- iter: 3000/8000\n",
            "Training Step: 10873  | total loss: \u001b[1m\u001b[32m0.47714\u001b[0m\u001b[0m | time: 2.559s\n",
            "| SGD | epoch: 1008 | loss: 0.47714 - acc: 0.8792 -- iter: 4000/8000\n",
            "Training Step: 10874  | total loss: \u001b[1m\u001b[32m0.47342\u001b[0m\u001b[0m | time: 3.193s\n",
            "| SGD | epoch: 1008 | loss: 0.47342 - acc: 0.8800 -- iter: 5000/8000\n",
            "Training Step: 10875  | total loss: \u001b[1m\u001b[32m0.47046\u001b[0m\u001b[0m | time: 3.826s\n",
            "| SGD | epoch: 1008 | loss: 0.47046 - acc: 0.8812 -- iter: 6000/8000\n",
            "Training Step: 10876  | total loss: \u001b[1m\u001b[32m0.47176\u001b[0m\u001b[0m | time: 4.460s\n",
            "| SGD | epoch: 1008 | loss: 0.47176 - acc: 0.8800 -- iter: 7000/8000\n",
            "Training Step: 10877  | total loss: \u001b[1m\u001b[32m0.47144\u001b[0m\u001b[0m | time: 5.092s\n",
            "| SGD | epoch: 1008 | loss: 0.47144 - acc: 0.8800 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10878  | total loss: \u001b[1m\u001b[32m0.47127\u001b[0m\u001b[0m | time: 0.632s\n",
            "| SGD | epoch: 1009 | loss: 0.47127 - acc: 0.8805 -- iter: 1000/8000\n",
            "Training Step: 10879  | total loss: \u001b[1m\u001b[32m0.46980\u001b[0m\u001b[0m | time: 1.267s\n",
            "| SGD | epoch: 1009 | loss: 0.46980 - acc: 0.8800 -- iter: 2000/8000\n",
            "Training Step: 10880  | total loss: \u001b[1m\u001b[32m0.47212\u001b[0m\u001b[0m | time: 1.905s\n",
            "| SGD | epoch: 1009 | loss: 0.47212 - acc: 0.8801 -- iter: 3000/8000\n",
            "Training Step: 10881  | total loss: \u001b[1m\u001b[32m0.47162\u001b[0m\u001b[0m | time: 2.519s\n",
            "| SGD | epoch: 1009 | loss: 0.47162 - acc: 0.8802 -- iter: 4000/8000\n",
            "Training Step: 10882  | total loss: \u001b[1m\u001b[32m0.47333\u001b[0m\u001b[0m | time: 3.131s\n",
            "| SGD | epoch: 1009 | loss: 0.47333 - acc: 0.8799 -- iter: 5000/8000\n",
            "Training Step: 10883  | total loss: \u001b[1m\u001b[32m0.47455\u001b[0m\u001b[0m | time: 3.749s\n",
            "| SGD | epoch: 1009 | loss: 0.47455 - acc: 0.8788 -- iter: 6000/8000\n",
            "Training Step: 10884  | total loss: \u001b[1m\u001b[32m0.47474\u001b[0m\u001b[0m | time: 4.382s\n",
            "| SGD | epoch: 1009 | loss: 0.47474 - acc: 0.8800 -- iter: 7000/8000\n",
            "Training Step: 10885  | total loss: \u001b[1m\u001b[32m0.47479\u001b[0m\u001b[0m | time: 5.017s\n",
            "| SGD | epoch: 1009 | loss: 0.47479 - acc: 0.8792 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10886  | total loss: \u001b[1m\u001b[32m0.47580\u001b[0m\u001b[0m | time: 0.638s\n",
            "| SGD | epoch: 1010 | loss: 0.47580 - acc: 0.8779 -- iter: 1000/8000\n",
            "Training Step: 10887  | total loss: \u001b[1m\u001b[32m0.47415\u001b[0m\u001b[0m | time: 1.271s\n",
            "| SGD | epoch: 1010 | loss: 0.47415 - acc: 0.8773 -- iter: 2000/8000\n",
            "Training Step: 10888  | total loss: \u001b[1m\u001b[32m0.47866\u001b[0m\u001b[0m | time: 1.905s\n",
            "| SGD | epoch: 1010 | loss: 0.47866 - acc: 0.8762 -- iter: 3000/8000\n",
            "Training Step: 10889  | total loss: \u001b[1m\u001b[32m0.47845\u001b[0m\u001b[0m | time: 2.546s\n",
            "| SGD | epoch: 1010 | loss: 0.47845 - acc: 0.8775 -- iter: 4000/8000\n",
            "Training Step: 10890  | total loss: \u001b[1m\u001b[32m0.47677\u001b[0m\u001b[0m | time: 3.187s\n",
            "| SGD | epoch: 1010 | loss: 0.47677 - acc: 0.8773 -- iter: 5000/8000\n",
            "Training Step: 10891  | total loss: \u001b[1m\u001b[32m0.47660\u001b[0m\u001b[0m | time: 3.828s\n",
            "| SGD | epoch: 1010 | loss: 0.47660 - acc: 0.8771 -- iter: 6000/8000\n",
            "Training Step: 10892  | total loss: \u001b[1m\u001b[32m0.47699\u001b[0m\u001b[0m | time: 4.476s\n",
            "| SGD | epoch: 1010 | loss: 0.47699 - acc: 0.8766 -- iter: 7000/8000\n",
            "Training Step: 10893  | total loss: \u001b[1m\u001b[32m0.47624\u001b[0m\u001b[0m | time: 5.119s\n",
            "| SGD | epoch: 1010 | loss: 0.47624 - acc: 0.8770 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10894  | total loss: \u001b[1m\u001b[32m0.47438\u001b[0m\u001b[0m | time: 0.640s\n",
            "| SGD | epoch: 1011 | loss: 0.47438 - acc: 0.8786 -- iter: 1000/8000\n",
            "Training Step: 10895  | total loss: \u001b[1m\u001b[32m0.47413\u001b[0m\u001b[0m | time: 1.276s\n",
            "| SGD | epoch: 1011 | loss: 0.47413 - acc: 0.8797 -- iter: 2000/8000\n",
            "Training Step: 10896  | total loss: \u001b[1m\u001b[32m0.47419\u001b[0m\u001b[0m | time: 1.906s\n",
            "| SGD | epoch: 1011 | loss: 0.47419 - acc: 0.8787 -- iter: 3000/8000\n",
            "Training Step: 10897  | total loss: \u001b[1m\u001b[32m0.47446\u001b[0m\u001b[0m | time: 2.540s\n",
            "| SGD | epoch: 1011 | loss: 0.47446 - acc: 0.8788 -- iter: 4000/8000\n",
            "Training Step: 10898  | total loss: \u001b[1m\u001b[32m0.47427\u001b[0m\u001b[0m | time: 3.170s\n",
            "| SGD | epoch: 1011 | loss: 0.47427 - acc: 0.8800 -- iter: 5000/8000\n",
            "Training Step: 10899  | total loss: \u001b[1m\u001b[32m0.47384\u001b[0m\u001b[0m | time: 3.805s\n",
            "| SGD | epoch: 1011 | loss: 0.47384 - acc: 0.8795 -- iter: 6000/8000\n",
            "Training Step: 10900  | total loss: \u001b[1m\u001b[32m0.47071\u001b[0m\u001b[0m | time: 4.421s\n",
            "| SGD | epoch: 1011 | loss: 0.47071 - acc: 0.8806 -- iter: 7000/8000\n",
            "Training Step: 10901  | total loss: \u001b[1m\u001b[32m0.47147\u001b[0m\u001b[0m | time: 5.037s\n",
            "| SGD | epoch: 1011 | loss: 0.47147 - acc: 0.8800 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10902  | total loss: \u001b[1m\u001b[32m0.47206\u001b[0m\u001b[0m | time: 0.624s\n",
            "| SGD | epoch: 1012 | loss: 0.47206 - acc: 0.8794 -- iter: 1000/8000\n",
            "Training Step: 10903  | total loss: \u001b[1m\u001b[32m0.47058\u001b[0m\u001b[0m | time: 1.241s\n",
            "| SGD | epoch: 1012 | loss: 0.47058 - acc: 0.8817 -- iter: 2000/8000\n",
            "Training Step: 10904  | total loss: \u001b[1m\u001b[32m0.47315\u001b[0m\u001b[0m | time: 1.858s\n",
            "| SGD | epoch: 1012 | loss: 0.47315 - acc: 0.8815 -- iter: 3000/8000\n",
            "Training Step: 10905  | total loss: \u001b[1m\u001b[32m0.47168\u001b[0m\u001b[0m | time: 2.478s\n",
            "| SGD | epoch: 1012 | loss: 0.47168 - acc: 0.8826 -- iter: 4000/8000\n",
            "Training Step: 10906  | total loss: \u001b[1m\u001b[32m0.47313\u001b[0m\u001b[0m | time: 3.093s\n",
            "| SGD | epoch: 1012 | loss: 0.47313 - acc: 0.8823 -- iter: 5000/8000\n",
            "Training Step: 10907  | total loss: \u001b[1m\u001b[32m0.47275\u001b[0m\u001b[0m | time: 3.707s\n",
            "| SGD | epoch: 1012 | loss: 0.47275 - acc: 0.8811 -- iter: 6000/8000\n",
            "Training Step: 10908  | total loss: \u001b[1m\u001b[32m0.47312\u001b[0m\u001b[0m | time: 4.325s\n",
            "| SGD | epoch: 1012 | loss: 0.47312 - acc: 0.8802 -- iter: 7000/8000\n",
            "Training Step: 10909  | total loss: \u001b[1m\u001b[32m0.47437\u001b[0m\u001b[0m | time: 4.945s\n",
            "| SGD | epoch: 1012 | loss: 0.47437 - acc: 0.8794 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10910  | total loss: \u001b[1m\u001b[32m0.47381\u001b[0m\u001b[0m | time: 0.626s\n",
            "| SGD | epoch: 1013 | loss: 0.47381 - acc: 0.8799 -- iter: 1000/8000\n",
            "Training Step: 10911  | total loss: \u001b[1m\u001b[32m0.47217\u001b[0m\u001b[0m | time: 1.254s\n",
            "| SGD | epoch: 1013 | loss: 0.47217 - acc: 0.8819 -- iter: 2000/8000\n",
            "Training Step: 10912  | total loss: \u001b[1m\u001b[32m0.47316\u001b[0m\u001b[0m | time: 1.883s\n",
            "| SGD | epoch: 1013 | loss: 0.47316 - acc: 0.8812 -- iter: 3000/8000\n",
            "Training Step: 10913  | total loss: \u001b[1m\u001b[32m0.47343\u001b[0m\u001b[0m | time: 2.511s\n",
            "| SGD | epoch: 1013 | loss: 0.47343 - acc: 0.8799 -- iter: 4000/8000\n",
            "Training Step: 10914  | total loss: \u001b[1m\u001b[32m0.47430\u001b[0m\u001b[0m | time: 3.140s\n",
            "| SGD | epoch: 1013 | loss: 0.47430 - acc: 0.8795 -- iter: 5000/8000\n",
            "Training Step: 10915  | total loss: \u001b[1m\u001b[32m0.47448\u001b[0m\u001b[0m | time: 3.768s\n",
            "| SGD | epoch: 1013 | loss: 0.47448 - acc: 0.8794 -- iter: 6000/8000\n",
            "Training Step: 10916  | total loss: \u001b[1m\u001b[32m0.47465\u001b[0m\u001b[0m | time: 4.400s\n",
            "| SGD | epoch: 1013 | loss: 0.47465 - acc: 0.8791 -- iter: 7000/8000\n",
            "Training Step: 10917  | total loss: \u001b[1m\u001b[32m0.47123\u001b[0m\u001b[0m | time: 5.021s\n",
            "| SGD | epoch: 1013 | loss: 0.47123 - acc: 0.8797 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10918  | total loss: \u001b[1m\u001b[32m0.47159\u001b[0m\u001b[0m | time: 0.620s\n",
            "| SGD | epoch: 1014 | loss: 0.47159 - acc: 0.8805 -- iter: 1000/8000\n",
            "Training Step: 10919  | total loss: \u001b[1m\u001b[32m0.47129\u001b[0m\u001b[0m | time: 1.239s\n",
            "| SGD | epoch: 1014 | loss: 0.47129 - acc: 0.8805 -- iter: 2000/8000\n",
            "Training Step: 10920  | total loss: \u001b[1m\u001b[32m0.47051\u001b[0m\u001b[0m | time: 1.858s\n",
            "| SGD | epoch: 1014 | loss: 0.47051 - acc: 0.8798 -- iter: 3000/8000\n",
            "Training Step: 10921  | total loss: \u001b[1m\u001b[32m0.47269\u001b[0m\u001b[0m | time: 2.474s\n",
            "| SGD | epoch: 1014 | loss: 0.47269 - acc: 0.8789 -- iter: 4000/8000\n",
            "Training Step: 10922  | total loss: \u001b[1m\u001b[32m0.47138\u001b[0m\u001b[0m | time: 3.095s\n",
            "| SGD | epoch: 1014 | loss: 0.47138 - acc: 0.8798 -- iter: 5000/8000\n",
            "Training Step: 10923  | total loss: \u001b[1m\u001b[32m0.46803\u001b[0m\u001b[0m | time: 3.712s\n",
            "| SGD | epoch: 1014 | loss: 0.46803 - acc: 0.8814 -- iter: 6000/8000\n",
            "Training Step: 10924  | total loss: \u001b[1m\u001b[32m0.47022\u001b[0m\u001b[0m | time: 4.330s\n",
            "| SGD | epoch: 1014 | loss: 0.47022 - acc: 0.8809 -- iter: 7000/8000\n",
            "Training Step: 10925  | total loss: \u001b[1m\u001b[32m0.47103\u001b[0m\u001b[0m | time: 4.947s\n",
            "| SGD | epoch: 1014 | loss: 0.47103 - acc: 0.8802 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10926  | total loss: \u001b[1m\u001b[32m0.47004\u001b[0m\u001b[0m | time: 0.621s\n",
            "| SGD | epoch: 1015 | loss: 0.47004 - acc: 0.8806 -- iter: 1000/8000\n",
            "Training Step: 10927  | total loss: \u001b[1m\u001b[32m0.46970\u001b[0m\u001b[0m | time: 1.247s\n",
            "| SGD | epoch: 1015 | loss: 0.46970 - acc: 0.8791 -- iter: 2000/8000\n",
            "Training Step: 10928  | total loss: \u001b[1m\u001b[32m0.47402\u001b[0m\u001b[0m | time: 1.865s\n",
            "| SGD | epoch: 1015 | loss: 0.47402 - acc: 0.8786 -- iter: 3000/8000\n",
            "Training Step: 10929  | total loss: \u001b[1m\u001b[32m0.47804\u001b[0m\u001b[0m | time: 2.482s\n",
            "| SGD | epoch: 1015 | loss: 0.47804 - acc: 0.8779 -- iter: 4000/8000\n",
            "Training Step: 10930  | total loss: \u001b[1m\u001b[32m0.47506\u001b[0m\u001b[0m | time: 3.102s\n",
            "| SGD | epoch: 1015 | loss: 0.47506 - acc: 0.8793 -- iter: 5000/8000\n",
            "Training Step: 10931  | total loss: \u001b[1m\u001b[32m0.47589\u001b[0m\u001b[0m | time: 3.721s\n",
            "| SGD | epoch: 1015 | loss: 0.47589 - acc: 0.8773 -- iter: 6000/8000\n",
            "Training Step: 10932  | total loss: \u001b[1m\u001b[32m0.47584\u001b[0m\u001b[0m | time: 4.346s\n",
            "| SGD | epoch: 1015 | loss: 0.47584 - acc: 0.8789 -- iter: 7000/8000\n",
            "Training Step: 10933  | total loss: \u001b[1m\u001b[32m0.47673\u001b[0m\u001b[0m | time: 4.973s\n",
            "| SGD | epoch: 1015 | loss: 0.47673 - acc: 0.8783 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10934  | total loss: \u001b[1m\u001b[32m0.47914\u001b[0m\u001b[0m | time: 0.626s\n",
            "| SGD | epoch: 1016 | loss: 0.47914 - acc: 0.8778 -- iter: 1000/8000\n",
            "Training Step: 10935  | total loss: \u001b[1m\u001b[32m0.47889\u001b[0m\u001b[0m | time: 1.254s\n",
            "| SGD | epoch: 1016 | loss: 0.47889 - acc: 0.8782 -- iter: 2000/8000\n",
            "Training Step: 10936  | total loss: \u001b[1m\u001b[32m0.47509\u001b[0m\u001b[0m | time: 1.884s\n",
            "| SGD | epoch: 1016 | loss: 0.47509 - acc: 0.8789 -- iter: 3000/8000\n",
            "Training Step: 10937  | total loss: \u001b[1m\u001b[32m0.47266\u001b[0m\u001b[0m | time: 2.509s\n",
            "| SGD | epoch: 1016 | loss: 0.47266 - acc: 0.8792 -- iter: 4000/8000\n",
            "Training Step: 10938  | total loss: \u001b[1m\u001b[32m0.47115\u001b[0m\u001b[0m | time: 3.132s\n",
            "| SGD | epoch: 1016 | loss: 0.47115 - acc: 0.8797 -- iter: 5000/8000\n",
            "Training Step: 10939  | total loss: \u001b[1m\u001b[32m0.47196\u001b[0m\u001b[0m | time: 3.756s\n",
            "| SGD | epoch: 1016 | loss: 0.47196 - acc: 0.8796 -- iter: 6000/8000\n",
            "Training Step: 10940  | total loss: \u001b[1m\u001b[32m0.47273\u001b[0m\u001b[0m | time: 4.376s\n",
            "| SGD | epoch: 1016 | loss: 0.47273 - acc: 0.8805 -- iter: 7000/8000\n",
            "Training Step: 10941  | total loss: \u001b[1m\u001b[32m0.47376\u001b[0m\u001b[0m | time: 5.003s\n",
            "| SGD | epoch: 1016 | loss: 0.47376 - acc: 0.8793 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10942  | total loss: \u001b[1m\u001b[32m0.47220\u001b[0m\u001b[0m | time: 0.618s\n",
            "| SGD | epoch: 1017 | loss: 0.47220 - acc: 0.8797 -- iter: 1000/8000\n",
            "Training Step: 10943  | total loss: \u001b[1m\u001b[32m0.47087\u001b[0m\u001b[0m | time: 1.238s\n",
            "| SGD | epoch: 1017 | loss: 0.47087 - acc: 0.8796 -- iter: 2000/8000\n",
            "Training Step: 10944  | total loss: \u001b[1m\u001b[32m0.47516\u001b[0m\u001b[0m | time: 1.863s\n",
            "| SGD | epoch: 1017 | loss: 0.47516 - acc: 0.8794 -- iter: 3000/8000\n",
            "Training Step: 10945  | total loss: \u001b[1m\u001b[32m0.47651\u001b[0m\u001b[0m | time: 2.481s\n",
            "| SGD | epoch: 1017 | loss: 0.47651 - acc: 0.8790 -- iter: 4000/8000\n",
            "Training Step: 10946  | total loss: \u001b[1m\u001b[32m0.47381\u001b[0m\u001b[0m | time: 3.101s\n",
            "| SGD | epoch: 1017 | loss: 0.47381 - acc: 0.8796 -- iter: 5000/8000\n",
            "Training Step: 10947  | total loss: \u001b[1m\u001b[32m0.47124\u001b[0m\u001b[0m | time: 3.719s\n",
            "| SGD | epoch: 1017 | loss: 0.47124 - acc: 0.8803 -- iter: 6000/8000\n",
            "Training Step: 10948  | total loss: \u001b[1m\u001b[32m0.47206\u001b[0m\u001b[0m | time: 4.337s\n",
            "| SGD | epoch: 1017 | loss: 0.47206 - acc: 0.8805 -- iter: 7000/8000\n",
            "Training Step: 10949  | total loss: \u001b[1m\u001b[32m0.46864\u001b[0m\u001b[0m | time: 4.956s\n",
            "| SGD | epoch: 1017 | loss: 0.46864 - acc: 0.8823 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10950  | total loss: \u001b[1m\u001b[32m0.46725\u001b[0m\u001b[0m | time: 0.628s\n",
            "| SGD | epoch: 1018 | loss: 0.46725 - acc: 0.8832 -- iter: 1000/8000\n",
            "Training Step: 10951  | total loss: \u001b[1m\u001b[32m0.46507\u001b[0m\u001b[0m | time: 1.252s\n",
            "| SGD | epoch: 1018 | loss: 0.46507 - acc: 0.8833 -- iter: 2000/8000\n",
            "Training Step: 10952  | total loss: \u001b[1m\u001b[32m0.46704\u001b[0m\u001b[0m | time: 1.880s\n",
            "| SGD | epoch: 1018 | loss: 0.46704 - acc: 0.8839 -- iter: 3000/8000\n",
            "Training Step: 10953  | total loss: \u001b[1m\u001b[32m0.46687\u001b[0m\u001b[0m | time: 2.509s\n",
            "| SGD | epoch: 1018 | loss: 0.46687 - acc: 0.8821 -- iter: 4000/8000\n",
            "Training Step: 10954  | total loss: \u001b[1m\u001b[32m0.46528\u001b[0m\u001b[0m | time: 3.138s\n",
            "| SGD | epoch: 1018 | loss: 0.46528 - acc: 0.8822 -- iter: 5000/8000\n",
            "Training Step: 10955  | total loss: \u001b[1m\u001b[32m0.46653\u001b[0m\u001b[0m | time: 3.770s\n",
            "| SGD | epoch: 1018 | loss: 0.46653 - acc: 0.8814 -- iter: 6000/8000\n",
            "Training Step: 10956  | total loss: \u001b[1m\u001b[32m0.46804\u001b[0m\u001b[0m | time: 4.394s\n",
            "| SGD | epoch: 1018 | loss: 0.46804 - acc: 0.8809 -- iter: 7000/8000\n",
            "Training Step: 10957  | total loss: \u001b[1m\u001b[32m0.47079\u001b[0m\u001b[0m | time: 5.022s\n",
            "| SGD | epoch: 1018 | loss: 0.47079 - acc: 0.8801 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10958  | total loss: \u001b[1m\u001b[32m0.47163\u001b[0m\u001b[0m | time: 0.627s\n",
            "| SGD | epoch: 1019 | loss: 0.47163 - acc: 0.8792 -- iter: 1000/8000\n",
            "Training Step: 10959  | total loss: \u001b[1m\u001b[32m0.46973\u001b[0m\u001b[0m | time: 1.260s\n",
            "| SGD | epoch: 1019 | loss: 0.46973 - acc: 0.8796 -- iter: 2000/8000\n",
            "Training Step: 10960  | total loss: \u001b[1m\u001b[32m0.47060\u001b[0m\u001b[0m | time: 1.889s\n",
            "| SGD | epoch: 1019 | loss: 0.47060 - acc: 0.8792 -- iter: 3000/8000\n",
            "Training Step: 10961  | total loss: \u001b[1m\u001b[32m0.46959\u001b[0m\u001b[0m | time: 2.514s\n",
            "| SGD | epoch: 1019 | loss: 0.46959 - acc: 0.8808 -- iter: 4000/8000\n",
            "Training Step: 10962  | total loss: \u001b[1m\u001b[32m0.47173\u001b[0m\u001b[0m | time: 3.134s\n",
            "| SGD | epoch: 1019 | loss: 0.47173 - acc: 0.8796 -- iter: 5000/8000\n",
            "Training Step: 10963  | total loss: \u001b[1m\u001b[32m0.47112\u001b[0m\u001b[0m | time: 3.751s\n",
            "| SGD | epoch: 1019 | loss: 0.47112 - acc: 0.8808 -- iter: 6000/8000\n",
            "Training Step: 10964  | total loss: \u001b[1m\u001b[32m0.47118\u001b[0m\u001b[0m | time: 4.367s\n",
            "| SGD | epoch: 1019 | loss: 0.47118 - acc: 0.8804 -- iter: 7000/8000\n",
            "Training Step: 10965  | total loss: \u001b[1m\u001b[32m0.47097\u001b[0m\u001b[0m | time: 4.985s\n",
            "| SGD | epoch: 1019 | loss: 0.47097 - acc: 0.8800 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10966  | total loss: \u001b[1m\u001b[32m0.46896\u001b[0m\u001b[0m | time: 0.731s\n",
            "| SGD | epoch: 1020 | loss: 0.46896 - acc: 0.8812 -- iter: 1000/8000\n",
            "Training Step: 10967  | total loss: \u001b[1m\u001b[32m0.47074\u001b[0m\u001b[0m | time: 1.344s\n",
            "| SGD | epoch: 1020 | loss: 0.47074 - acc: 0.8811 -- iter: 2000/8000\n",
            "Training Step: 10968  | total loss: \u001b[1m\u001b[32m0.46870\u001b[0m\u001b[0m | time: 1.958s\n",
            "| SGD | epoch: 1020 | loss: 0.46870 - acc: 0.8824 -- iter: 3000/8000\n",
            "Training Step: 10969  | total loss: \u001b[1m\u001b[32m0.46832\u001b[0m\u001b[0m | time: 2.591s\n",
            "| SGD | epoch: 1020 | loss: 0.46832 - acc: 0.8812 -- iter: 4000/8000\n",
            "Training Step: 10970  | total loss: \u001b[1m\u001b[32m0.46472\u001b[0m\u001b[0m | time: 3.224s\n",
            "| SGD | epoch: 1020 | loss: 0.46472 - acc: 0.8826 -- iter: 5000/8000\n",
            "Training Step: 10971  | total loss: \u001b[1m\u001b[32m0.46414\u001b[0m\u001b[0m | time: 3.860s\n",
            "| SGD | epoch: 1020 | loss: 0.46414 - acc: 0.8827 -- iter: 6000/8000\n",
            "Training Step: 10972  | total loss: \u001b[1m\u001b[32m0.46716\u001b[0m\u001b[0m | time: 4.493s\n",
            "| SGD | epoch: 1020 | loss: 0.46716 - acc: 0.8822 -- iter: 7000/8000\n",
            "Training Step: 10973  | total loss: \u001b[1m\u001b[32m0.46783\u001b[0m\u001b[0m | time: 5.125s\n",
            "| SGD | epoch: 1020 | loss: 0.46783 - acc: 0.8833 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10974  | total loss: \u001b[1m\u001b[32m0.46884\u001b[0m\u001b[0m | time: 0.631s\n",
            "| SGD | epoch: 1021 | loss: 0.46884 - acc: 0.8842 -- iter: 1000/8000\n",
            "Training Step: 10975  | total loss: \u001b[1m\u001b[32m0.46914\u001b[0m\u001b[0m | time: 1.269s\n",
            "| SGD | epoch: 1021 | loss: 0.46914 - acc: 0.8831 -- iter: 2000/8000\n",
            "Training Step: 10976  | total loss: \u001b[1m\u001b[32m0.47248\u001b[0m\u001b[0m | time: 1.915s\n",
            "| SGD | epoch: 1021 | loss: 0.47248 - acc: 0.8814 -- iter: 3000/8000\n",
            "Training Step: 10977  | total loss: \u001b[1m\u001b[32m0.47266\u001b[0m\u001b[0m | time: 2.563s\n",
            "| SGD | epoch: 1021 | loss: 0.47266 - acc: 0.8811 -- iter: 4000/8000\n",
            "Training Step: 10978  | total loss: \u001b[1m\u001b[32m0.47366\u001b[0m\u001b[0m | time: 3.191s\n",
            "| SGD | epoch: 1021 | loss: 0.47366 - acc: 0.8800 -- iter: 5000/8000\n",
            "Training Step: 10979  | total loss: \u001b[1m\u001b[32m0.46952\u001b[0m\u001b[0m | time: 3.820s\n",
            "| SGD | epoch: 1021 | loss: 0.46952 - acc: 0.8819 -- iter: 6000/8000\n",
            "Training Step: 10980  | total loss: \u001b[1m\u001b[32m0.47111\u001b[0m\u001b[0m | time: 4.448s\n",
            "| SGD | epoch: 1021 | loss: 0.47111 - acc: 0.8803 -- iter: 7000/8000\n",
            "Training Step: 10981  | total loss: \u001b[1m\u001b[32m0.46905\u001b[0m\u001b[0m | time: 5.075s\n",
            "| SGD | epoch: 1021 | loss: 0.46905 - acc: 0.8822 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10982  | total loss: \u001b[1m\u001b[32m0.46970\u001b[0m\u001b[0m | time: 0.618s\n",
            "| SGD | epoch: 1022 | loss: 0.46970 - acc: 0.8808 -- iter: 1000/8000\n",
            "Training Step: 10983  | total loss: \u001b[1m\u001b[32m0.46987\u001b[0m\u001b[0m | time: 1.233s\n",
            "| SGD | epoch: 1022 | loss: 0.46987 - acc: 0.8802 -- iter: 2000/8000\n",
            "Training Step: 10984  | total loss: \u001b[1m\u001b[32m0.47061\u001b[0m\u001b[0m | time: 1.846s\n",
            "| SGD | epoch: 1022 | loss: 0.47061 - acc: 0.8803 -- iter: 3000/8000\n",
            "Training Step: 10985  | total loss: \u001b[1m\u001b[32m0.46987\u001b[0m\u001b[0m | time: 2.462s\n",
            "| SGD | epoch: 1022 | loss: 0.46987 - acc: 0.8798 -- iter: 4000/8000\n",
            "Training Step: 10986  | total loss: \u001b[1m\u001b[32m0.46896\u001b[0m\u001b[0m | time: 3.075s\n",
            "| SGD | epoch: 1022 | loss: 0.46896 - acc: 0.8801 -- iter: 5000/8000\n",
            "Training Step: 10987  | total loss: \u001b[1m\u001b[32m0.46948\u001b[0m\u001b[0m | time: 3.688s\n",
            "| SGD | epoch: 1022 | loss: 0.46948 - acc: 0.8795 -- iter: 6000/8000\n",
            "Training Step: 10988  | total loss: \u001b[1m\u001b[32m0.47145\u001b[0m\u001b[0m | time: 4.302s\n",
            "| SGD | epoch: 1022 | loss: 0.47145 - acc: 0.8799 -- iter: 7000/8000\n",
            "Training Step: 10989  | total loss: \u001b[1m\u001b[32m0.47401\u001b[0m\u001b[0m | time: 4.915s\n",
            "| SGD | epoch: 1022 | loss: 0.47401 - acc: 0.8789 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10990  | total loss: \u001b[1m\u001b[32m0.47589\u001b[0m\u001b[0m | time: 0.615s\n",
            "| SGD | epoch: 1023 | loss: 0.47589 - acc: 0.8795 -- iter: 1000/8000\n",
            "Training Step: 10991  | total loss: \u001b[1m\u001b[32m0.47357\u001b[0m\u001b[0m | time: 1.226s\n",
            "| SGD | epoch: 1023 | loss: 0.47357 - acc: 0.8804 -- iter: 2000/8000\n",
            "Training Step: 10992  | total loss: \u001b[1m\u001b[32m0.47156\u001b[0m\u001b[0m | time: 1.840s\n",
            "| SGD | epoch: 1023 | loss: 0.47156 - acc: 0.8808 -- iter: 3000/8000\n",
            "Training Step: 10993  | total loss: \u001b[1m\u001b[32m0.46859\u001b[0m\u001b[0m | time: 2.458s\n",
            "| SGD | epoch: 1023 | loss: 0.46859 - acc: 0.8816 -- iter: 4000/8000\n",
            "Training Step: 10994  | total loss: \u001b[1m\u001b[32m0.46451\u001b[0m\u001b[0m | time: 3.073s\n",
            "| SGD | epoch: 1023 | loss: 0.46451 - acc: 0.8831 -- iter: 5000/8000\n",
            "Training Step: 10995  | total loss: \u001b[1m\u001b[32m0.46529\u001b[0m\u001b[0m | time: 3.688s\n",
            "| SGD | epoch: 1023 | loss: 0.46529 - acc: 0.8838 -- iter: 6000/8000\n",
            "Training Step: 10996  | total loss: \u001b[1m\u001b[32m0.46611\u001b[0m\u001b[0m | time: 4.301s\n",
            "| SGD | epoch: 1023 | loss: 0.46611 - acc: 0.8819 -- iter: 7000/8000\n",
            "Training Step: 10997  | total loss: \u001b[1m\u001b[32m0.46633\u001b[0m\u001b[0m | time: 4.923s\n",
            "| SGD | epoch: 1023 | loss: 0.46633 - acc: 0.8819 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 10998  | total loss: \u001b[1m\u001b[32m0.46839\u001b[0m\u001b[0m | time: 0.623s\n",
            "| SGD | epoch: 1024 | loss: 0.46839 - acc: 0.8809 -- iter: 1000/8000\n",
            "Training Step: 10999  | total loss: \u001b[1m\u001b[32m0.46826\u001b[0m\u001b[0m | time: 1.246s\n",
            "| SGD | epoch: 1024 | loss: 0.46826 - acc: 0.8815 -- iter: 2000/8000\n",
            "Training Step: 11000  | total loss: \u001b[1m\u001b[32m0.46598\u001b[0m\u001b[0m | time: 1.872s\n",
            "| SGD | epoch: 1024 | loss: 0.46598 - acc: 0.8817 -- iter: 3000/8000\n",
            "Training Step: 11001  | total loss: \u001b[1m\u001b[32m0.46395\u001b[0m\u001b[0m | time: 2.498s\n",
            "| SGD | epoch: 1024 | loss: 0.46395 - acc: 0.8828 -- iter: 4000/8000\n",
            "Training Step: 11002  | total loss: \u001b[1m\u001b[32m0.46531\u001b[0m\u001b[0m | time: 3.124s\n",
            "| SGD | epoch: 1024 | loss: 0.46531 - acc: 0.8821 -- iter: 5000/8000\n",
            "Training Step: 11003  | total loss: \u001b[1m\u001b[32m0.46417\u001b[0m\u001b[0m | time: 3.747s\n",
            "| SGD | epoch: 1024 | loss: 0.46417 - acc: 0.8823 -- iter: 6000/8000\n",
            "Training Step: 11004  | total loss: \u001b[1m\u001b[32m0.46677\u001b[0m\u001b[0m | time: 4.367s\n",
            "| SGD | epoch: 1024 | loss: 0.46677 - acc: 0.8811 -- iter: 7000/8000\n",
            "Training Step: 11005  | total loss: \u001b[1m\u001b[32m0.46575\u001b[0m\u001b[0m | time: 4.982s\n",
            "| SGD | epoch: 1024 | loss: 0.46575 - acc: 0.8814 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 11006  | total loss: \u001b[1m\u001b[32m0.46570\u001b[0m\u001b[0m | time: 0.618s\n",
            "| SGD | epoch: 1025 | loss: 0.46570 - acc: 0.8816 -- iter: 1000/8000\n",
            "Training Step: 11007  | total loss: \u001b[1m\u001b[32m0.46756\u001b[0m\u001b[0m | time: 1.235s\n",
            "| SGD | epoch: 1025 | loss: 0.46756 - acc: 0.8821 -- iter: 2000/8000\n",
            "Training Step: 11008  | total loss: \u001b[1m\u001b[32m0.46981\u001b[0m\u001b[0m | time: 1.854s\n",
            "| SGD | epoch: 1025 | loss: 0.46981 - acc: 0.8825 -- iter: 3000/8000\n",
            "Training Step: 11009  | total loss: \u001b[1m\u001b[32m0.46630\u001b[0m\u001b[0m | time: 2.470s\n",
            "| SGD | epoch: 1025 | loss: 0.46630 - acc: 0.8840 -- iter: 4000/8000\n",
            "Training Step: 11010  | total loss: \u001b[1m\u001b[32m0.46297\u001b[0m\u001b[0m | time: 3.084s\n",
            "| SGD | epoch: 1025 | loss: 0.46297 - acc: 0.8851 -- iter: 5000/8000\n",
            "Training Step: 11011  | total loss: \u001b[1m\u001b[32m0.46194\u001b[0m\u001b[0m | time: 3.700s\n",
            "| SGD | epoch: 1025 | loss: 0.46194 - acc: 0.8849 -- iter: 6000/8000\n",
            "Training Step: 11012  | total loss: \u001b[1m\u001b[32m0.46346\u001b[0m\u001b[0m | time: 4.313s\n",
            "| SGD | epoch: 1025 | loss: 0.46346 - acc: 0.8842 -- iter: 7000/8000\n",
            "Training Step: 11013  | total loss: \u001b[1m\u001b[32m0.46397\u001b[0m\u001b[0m | time: 4.944s\n",
            "| SGD | epoch: 1025 | loss: 0.46397 - acc: 0.8842 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 11014  | total loss: \u001b[1m\u001b[32m0.46045\u001b[0m\u001b[0m | time: 0.635s\n",
            "| SGD | epoch: 1026 | loss: 0.46045 - acc: 0.8841 -- iter: 1000/8000\n",
            "Training Step: 11015  | total loss: \u001b[1m\u001b[32m0.46342\u001b[0m\u001b[0m | time: 1.270s\n",
            "| SGD | epoch: 1026 | loss: 0.46342 - acc: 0.8828 -- iter: 2000/8000\n",
            "Training Step: 11016  | total loss: \u001b[1m\u001b[32m0.46476\u001b[0m\u001b[0m | time: 1.903s\n",
            "| SGD | epoch: 1026 | loss: 0.46476 - acc: 0.8825 -- iter: 3000/8000\n",
            "Training Step: 11017  | total loss: \u001b[1m\u001b[32m0.46159\u001b[0m\u001b[0m | time: 2.540s\n",
            "| SGD | epoch: 1026 | loss: 0.46159 - acc: 0.8838 -- iter: 4000/8000\n",
            "Training Step: 11018  | total loss: \u001b[1m\u001b[32m0.46230\u001b[0m\u001b[0m | time: 3.176s\n",
            "| SGD | epoch: 1026 | loss: 0.46230 - acc: 0.8830 -- iter: 5000/8000\n",
            "Training Step: 11019  | total loss: \u001b[1m\u001b[32m0.46266\u001b[0m\u001b[0m | time: 3.823s\n",
            "| SGD | epoch: 1026 | loss: 0.46266 - acc: 0.8829 -- iter: 6000/8000\n",
            "Training Step: 11020  | total loss: \u001b[1m\u001b[32m0.46695\u001b[0m\u001b[0m | time: 4.471s\n",
            "| SGD | epoch: 1026 | loss: 0.46695 - acc: 0.8814 -- iter: 7000/8000\n",
            "Training Step: 11021  | total loss: \u001b[1m\u001b[32m0.46672\u001b[0m\u001b[0m | time: 5.119s\n",
            "| SGD | epoch: 1026 | loss: 0.46672 - acc: 0.8824 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 11022  | total loss: \u001b[1m\u001b[32m0.46575\u001b[0m\u001b[0m | time: 0.649s\n",
            "| SGD | epoch: 1027 | loss: 0.46575 - acc: 0.8834 -- iter: 1000/8000\n",
            "Training Step: 11023  | total loss: \u001b[1m\u001b[32m0.46412\u001b[0m\u001b[0m | time: 1.295s\n",
            "| SGD | epoch: 1027 | loss: 0.46412 - acc: 0.8841 -- iter: 2000/8000\n",
            "Training Step: 11024  | total loss: \u001b[1m\u001b[32m0.46337\u001b[0m\u001b[0m | time: 1.938s\n",
            "| SGD | epoch: 1027 | loss: 0.46337 - acc: 0.8830 -- iter: 3000/8000\n",
            "Training Step: 11025  | total loss: \u001b[1m\u001b[32m0.46674\u001b[0m\u001b[0m | time: 2.576s\n",
            "| SGD | epoch: 1027 | loss: 0.46674 - acc: 0.8824 -- iter: 4000/8000\n",
            "Training Step: 11026  | total loss: \u001b[1m\u001b[32m0.46791\u001b[0m\u001b[0m | time: 3.221s\n",
            "| SGD | epoch: 1027 | loss: 0.46791 - acc: 0.8813 -- iter: 5000/8000\n",
            "Training Step: 11027  | total loss: \u001b[1m\u001b[32m0.47050\u001b[0m\u001b[0m | time: 3.853s\n",
            "| SGD | epoch: 1027 | loss: 0.47050 - acc: 0.8803 -- iter: 6000/8000\n",
            "Training Step: 11028  | total loss: \u001b[1m\u001b[32m0.47243\u001b[0m\u001b[0m | time: 4.487s\n",
            "| SGD | epoch: 1027 | loss: 0.47243 - acc: 0.8803 -- iter: 7000/8000\n",
            "Training Step: 11029  | total loss: \u001b[1m\u001b[32m0.46934\u001b[0m\u001b[0m | time: 5.124s\n",
            "| SGD | epoch: 1027 | loss: 0.46934 - acc: 0.8820 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 11030  | total loss: \u001b[1m\u001b[32m0.46848\u001b[0m\u001b[0m | time: 0.633s\n",
            "| SGD | epoch: 1028 | loss: 0.46848 - acc: 0.8825 -- iter: 1000/8000\n",
            "Training Step: 11031  | total loss: \u001b[1m\u001b[32m0.46519\u001b[0m\u001b[0m | time: 1.269s\n",
            "| SGD | epoch: 1028 | loss: 0.46519 - acc: 0.8849 -- iter: 2000/8000\n",
            "Training Step: 11032  | total loss: \u001b[1m\u001b[32m0.46531\u001b[0m\u001b[0m | time: 1.903s\n",
            "| SGD | epoch: 1028 | loss: 0.46531 - acc: 0.8836 -- iter: 3000/8000\n",
            "Training Step: 11033  | total loss: \u001b[1m\u001b[32m0.46638\u001b[0m\u001b[0m | time: 2.541s\n",
            "| SGD | epoch: 1028 | loss: 0.46638 - acc: 0.8834 -- iter: 4000/8000\n",
            "Training Step: 11034  | total loss: \u001b[1m\u001b[32m0.46569\u001b[0m\u001b[0m | time: 3.177s\n",
            "| SGD | epoch: 1028 | loss: 0.46569 - acc: 0.8837 -- iter: 5000/8000\n",
            "Training Step: 11035  | total loss: \u001b[1m\u001b[32m0.46630\u001b[0m\u001b[0m | time: 3.814s\n",
            "| SGD | epoch: 1028 | loss: 0.46630 - acc: 0.8833 -- iter: 6000/8000\n",
            "Training Step: 11036  | total loss: \u001b[1m\u001b[32m0.46857\u001b[0m\u001b[0m | time: 4.454s\n",
            "| SGD | epoch: 1028 | loss: 0.46857 - acc: 0.8830 -- iter: 7000/8000\n",
            "Training Step: 11037  | total loss: \u001b[1m\u001b[32m0.47106\u001b[0m\u001b[0m | time: 5.136s\n",
            "| SGD | epoch: 1028 | loss: 0.47106 - acc: 0.8828 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 11038  | total loss: \u001b[1m\u001b[32m0.46793\u001b[0m\u001b[0m | time: 0.616s\n",
            "| SGD | epoch: 1029 | loss: 0.46793 - acc: 0.8837 -- iter: 1000/8000\n",
            "Training Step: 11039  | total loss: \u001b[1m\u001b[32m0.46776\u001b[0m\u001b[0m | time: 1.232s\n",
            "| SGD | epoch: 1029 | loss: 0.46776 - acc: 0.8831 -- iter: 2000/8000\n",
            "Training Step: 11040  | total loss: \u001b[1m\u001b[32m0.46498\u001b[0m\u001b[0m | time: 1.871s\n",
            "| SGD | epoch: 1029 | loss: 0.46498 - acc: 0.8834 -- iter: 3000/8000\n",
            "Training Step: 11041  | total loss: \u001b[1m\u001b[32m0.46276\u001b[0m\u001b[0m | time: 2.517s\n",
            "| SGD | epoch: 1029 | loss: 0.46276 - acc: 0.8848 -- iter: 4000/8000\n",
            "Training Step: 11042  | total loss: \u001b[1m\u001b[32m0.46448\u001b[0m\u001b[0m | time: 3.160s\n",
            "| SGD | epoch: 1029 | loss: 0.46448 - acc: 0.8838 -- iter: 5000/8000\n",
            "Training Step: 11043  | total loss: \u001b[1m\u001b[32m0.46603\u001b[0m\u001b[0m | time: 3.807s\n",
            "| SGD | epoch: 1029 | loss: 0.46603 - acc: 0.8843 -- iter: 6000/8000\n",
            "Training Step: 11044  | total loss: \u001b[1m\u001b[32m0.46801\u001b[0m\u001b[0m | time: 4.456s\n",
            "| SGD | epoch: 1029 | loss: 0.46801 - acc: 0.8839 -- iter: 7000/8000\n",
            "Training Step: 11045  | total loss: \u001b[1m\u001b[32m0.46725\u001b[0m\u001b[0m | time: 5.103s\n",
            "| SGD | epoch: 1029 | loss: 0.46725 - acc: 0.8833 -- iter: 8000/8000\n",
            "--\n",
            "Training Step: 11046  | total loss: \u001b[1m\u001b[32m0.46586\u001b[0m\u001b[0m | time: 0.642s\n",
            "| SGD | epoch: 1030 | loss: 0.46586 - acc: 0.8834 -- iter: 1000/8000\n",
            "Training Step: 11047  | total loss: \u001b[1m\u001b[32m0.46549\u001b[0m\u001b[0m | time: 1.275s\n",
            "| SGD | epoch: 1030 | loss: 0.46549 - acc: 0.8834 -- iter: 2000/8000\n",
            "Training Step: 11048  | total loss: \u001b[1m\u001b[32m0.46709\u001b[0m\u001b[0m | time: 1.914s\n",
            "| SGD | epoch: 1030 | loss: 0.46709 - acc: 0.8827 -- iter: 3000/8000\n",
            "Training Step: 11049  | total loss: \u001b[1m\u001b[32m0.47137\u001b[0m\u001b[0m | time: 2.548s\n",
            "| SGD | epoch: 1030 | loss: 0.47137 - acc: 0.8795 -- iter: 4000/8000\n",
            "Training Step: 11050  | total loss: \u001b[1m\u001b[32m0.47478\u001b[0m\u001b[0m | time: 3.184s\n",
            "| SGD | epoch: 1030 | loss: 0.47478 - acc: 0.8789 -- iter: 5000/8000\n",
            "Training Step: 11051  | total loss: \u001b[1m\u001b[32m0.47035\u001b[0m\u001b[0m | time: 3.821s\n",
            "| SGD | epoch: 1030 | loss: 0.47035 - acc: 0.8801 -- iter: 6000/8000\n",
            "Training Step: 11052  | total loss: \u001b[1m\u001b[32m0.46858\u001b[0m\u001b[0m | time: 4.440s\n",
            "| SGD | epoch: 1030 | loss: 0.46858 - acc: 0.8815 -- iter: 7000/8000\n",
            "Training Step: 11053  | total loss: \u001b[1m\u001b[32m0.46709\u001b[0m\u001b[0m | time: 5.052s\n",
            "| SGD | epoch: 1030 | loss: 0.46709 - acc: 0.8822 -- iter: 8000/8000\n",
            "--\n",
            "Precisión con todo el dataset: 0.6045\n"
          ]
        }
      ],
      "source": [
        "model.fit(X_train, y_train, n_epoch=1000, batch_size=1000, show_metric=True)\n",
        "accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Precisión con todo el dataset: {accuracy[0]:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}